{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from textwrap import fill\n",
    "from math import sqrt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, classification_report, confusion_matrix, roc_auc_score,auc\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import hdbscan\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57c78d0",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581e7f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196294, 42)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>class_of_worker</th>\n",
       "      <th>detailed_industry_recode</th>\n",
       "      <th>detailed_occupation_recode</th>\n",
       "      <th>education</th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>enroll_in_edu_inst_last_wk</th>\n",
       "      <th>marital_stat</th>\n",
       "      <th>major_industry_code</th>\n",
       "      <th>major_occupation_code</th>\n",
       "      <th>race</th>\n",
       "      <th>hispanic_origin</th>\n",
       "      <th>sex</th>\n",
       "      <th>member_of_a_labor_union</th>\n",
       "      <th>reason_for_unemployment</th>\n",
       "      <th>full_or_part_time_employment_stat</th>\n",
       "      <th>capital_gains</th>\n",
       "      <th>capital_losses</th>\n",
       "      <th>dividends_from_stocks</th>\n",
       "      <th>tax_filer_stat</th>\n",
       "      <th>region_of_previous_residence</th>\n",
       "      <th>state_of_previous_residence</th>\n",
       "      <th>detailed_household_and_family_stat</th>\n",
       "      <th>detailed_household_summary_in_household</th>\n",
       "      <th>weight</th>\n",
       "      <th>migration_code-change_in_msa</th>\n",
       "      <th>migration_code-change_in_reg</th>\n",
       "      <th>migration_code-move_within_reg</th>\n",
       "      <th>live_in_this_house_1_year_ago</th>\n",
       "      <th>migration_prev_res_in_sunbelt</th>\n",
       "      <th>num_persons_worked_for_employer</th>\n",
       "      <th>family_members_under_18</th>\n",
       "      <th>country_of_birth_father</th>\n",
       "      <th>country_of_birth_mother</th>\n",
       "      <th>country_of_birth_self</th>\n",
       "      <th>citizenship</th>\n",
       "      <th>own_business_or_self_employed</th>\n",
       "      <th>fill_inc_questionnaire_for_veteran's_admin</th>\n",
       "      <th>veterans_benefits</th>\n",
       "      <th>weeks_worked_in_year</th>\n",
       "      <th>year</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>High school graduate</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>White</td>\n",
       "      <td>All other</td>\n",
       "      <td>Female</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in labor force</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nonfiler</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Other Rel 18+ ever marr not in subfamily</td>\n",
       "      <td>Other relative of householder</td>\n",
       "      <td>1700.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not in universe under 1 year old</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>Self-employed-not incorporated</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>Some college but no degree</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Construction</td>\n",
       "      <td>Precision production craft &amp; repair</td>\n",
       "      <td>White</td>\n",
       "      <td>All other</td>\n",
       "      <td>Male</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Children or Armed Forces</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Head of household</td>\n",
       "      <td>South</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>Householder</td>\n",
       "      <td>Householder</td>\n",
       "      <td>1053.55</td>\n",
       "      <td>MSA to MSA</td>\n",
       "      <td>Same county</td>\n",
       "      <td>Same county</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10th grade</td>\n",
       "      <td>0</td>\n",
       "      <td>High school</td>\n",
       "      <td>Never married</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Asian or Pacific Islander</td>\n",
       "      <td>All other</td>\n",
       "      <td>Female</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in labor force</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nonfiler</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Child 18+ never marr Not in a subfamily</td>\n",
       "      <td>Child 18 or older</td>\n",
       "      <td>991.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not in universe under 1 year old</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Foreign born- Not a citizen of U S</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Children</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Never married</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>White</td>\n",
       "      <td>All other</td>\n",
       "      <td>Female</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Children or Armed Forces</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nonfiler</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Child &lt;18 never marr not in subfamily</td>\n",
       "      <td>Child under 18 never married</td>\n",
       "      <td>1758.14</td>\n",
       "      <td>Nonmover</td>\n",
       "      <td>Nonmover</td>\n",
       "      <td>Nonmover</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>Both parents present</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Children</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Never married</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>White</td>\n",
       "      <td>All other</td>\n",
       "      <td>Female</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Children or Armed Forces</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nonfiler</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Child &lt;18 never marr not in subfamily</td>\n",
       "      <td>Child under 18 never married</td>\n",
       "      <td>1069.16</td>\n",
       "      <td>Nonmover</td>\n",
       "      <td>Nonmover</td>\n",
       "      <td>Nonmover</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>Both parents present</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age                 class_of_worker  detailed_industry_recode  detailed_occupation_recode                   education  wage_per_hour  \\\n",
       "0   73                 Not in universe                         0                           0        High school graduate              0   \n",
       "1   58  Self-employed-not incorporated                         4                          34  Some college but no degree              0   \n",
       "2   18                 Not in universe                         0                           0                  10th grade              0   \n",
       "3    9                 Not in universe                         0                           0                    Children              0   \n",
       "4   10                 Not in universe                         0                           0                    Children              0   \n",
       "\n",
       "  enroll_in_edu_inst_last_wk   marital_stat          major_industry_code                major_occupation_code                       race hispanic_origin  \\\n",
       "0            Not in universe        Widowed  Not in universe or children                      Not in universe                      White       All other   \n",
       "1            Not in universe       Divorced                 Construction  Precision production craft & repair                      White       All other   \n",
       "2                High school  Never married  Not in universe or children                      Not in universe  Asian or Pacific Islander       All other   \n",
       "3            Not in universe  Never married  Not in universe or children                      Not in universe                      White       All other   \n",
       "4            Not in universe  Never married  Not in universe or children                      Not in universe                      White       All other   \n",
       "\n",
       "      sex member_of_a_labor_union reason_for_unemployment full_or_part_time_employment_stat  capital_gains  capital_losses  dividends_from_stocks  \\\n",
       "0  Female         Not in universe         Not in universe                Not in labor force              0               0                      0   \n",
       "1    Male         Not in universe         Not in universe          Children or Armed Forces              0               0                      0   \n",
       "2  Female         Not in universe         Not in universe                Not in labor force              0               0                      0   \n",
       "3  Female         Not in universe         Not in universe          Children or Armed Forces              0               0                      0   \n",
       "4  Female         Not in universe         Not in universe          Children or Armed Forces              0               0                      0   \n",
       "\n",
       "      tax_filer_stat region_of_previous_residence state_of_previous_residence        detailed_household_and_family_stat  \\\n",
       "0           Nonfiler              Not in universe             Not in universe  Other Rel 18+ ever marr not in subfamily   \n",
       "1  Head of household                        South                    Arkansas                               Householder   \n",
       "2           Nonfiler              Not in universe             Not in universe   Child 18+ never marr Not in a subfamily   \n",
       "3           Nonfiler              Not in universe             Not in universe     Child <18 never marr not in subfamily   \n",
       "4           Nonfiler              Not in universe             Not in universe     Child <18 never marr not in subfamily   \n",
       "\n",
       "  detailed_household_summary_in_household   weight migration_code-change_in_msa migration_code-change_in_reg migration_code-move_within_reg  \\\n",
       "0           Other relative of householder  1700.09                          NaN                          NaN                            NaN   \n",
       "1                             Householder  1053.55                   MSA to MSA                  Same county                    Same county   \n",
       "2                       Child 18 or older   991.95                          NaN                          NaN                            NaN   \n",
       "3            Child under 18 never married  1758.14                     Nonmover                     Nonmover                       Nonmover   \n",
       "4            Child under 18 never married  1069.16                     Nonmover                     Nonmover                       Nonmover   \n",
       "\n",
       "      live_in_this_house_1_year_ago migration_prev_res_in_sunbelt  num_persons_worked_for_employer family_members_under_18 country_of_birth_father  \\\n",
       "0  Not in universe under 1 year old                           NaN                                0         Not in universe           United-States   \n",
       "1                                No                           Yes                                1         Not in universe           United-States   \n",
       "2  Not in universe under 1 year old                           NaN                                0         Not in universe                 Vietnam   \n",
       "3                               Yes               Not in universe                                0    Both parents present           United-States   \n",
       "4                               Yes               Not in universe                                0    Both parents present           United-States   \n",
       "\n",
       "  country_of_birth_mother country_of_birth_self                         citizenship  own_business_or_self_employed fill_inc_questionnaire_for_veteran's_admin  \\\n",
       "0           United-States         United-States   Native- Born in the United States                              0                            Not in universe   \n",
       "1           United-States         United-States   Native- Born in the United States                              0                            Not in universe   \n",
       "2                 Vietnam               Vietnam  Foreign born- Not a citizen of U S                              0                            Not in universe   \n",
       "3           United-States         United-States   Native- Born in the United States                              0                            Not in universe   \n",
       "4           United-States         United-States   Native- Born in the United States                              0                            Not in universe   \n",
       "\n",
       "   veterans_benefits  weeks_worked_in_year  year     label  \n",
       "0                  2                     0    95  - 50000.  \n",
       "1                  2                    52    94  - 50000.  \n",
       "2                  2                     0    95  - 50000.  \n",
       "3                  0                     0    94  - 50000.  \n",
       "4                  0                     0    94  - 50000.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Optional: wider display for DataFrame heads\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "# Path to your already-cleaned CSV (change if needed)\n",
    "CSV_PATH = \"census_clean.csv\"\n",
    "\n",
    "# Load the cleaned dataset (no file is written in this notebook)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a9573",
   "metadata": {},
   "source": [
    "# Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca380c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Feature builder: column dropping, value normalization, derived features ---\n",
    "\n",
    "\n",
    "class FeatureBuilder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # columns we plan to drop (if present)\n",
    "        self.drop_cols_ = [\n",
    "            \"year\", \"weight\",  # survey-only\n",
    "            \"state_of_previous_residence\",            # keep region instead\n",
    "            \"country_of_birth_father\", \"country_of_birth_mother\"  # keep citizenship or self\n",
    "        ]\n",
    "        # pick one of the household pair if both exist\n",
    "        self.household_pair_ = [\n",
    "            \"detailed_household_and_family_stat\",\n",
    "            \"detailed_household_summary_in_household\"\n",
    "        ]\n",
    "        # strings that mean missing/NA-ish in this dataset\n",
    "        self.missing_tokens_ = [\"Not in universe\", \"?\", \" Unknown\", \"unknown\", \"  ?\"]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "\n",
    "        # 1) unify missing-like tokens (categoricals)\n",
    "        obj_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "        for c in obj_cols:\n",
    "            df[c] = (\n",
    "                df[c].astype(str).str.strip()     \n",
    "                    .replace(self.missing_tokens_, \"Missing\")\n",
    "            )\n",
    "\n",
    "        # 2) if region is missing but state exists, create region from state (BEFORE dropping state)\n",
    "        if \"region_of_previous_residence\" not in df.columns and \"state_of_previous_residence\" in df.columns:\n",
    "            # Census 4-region mapping by full state name\n",
    "            northeast = {\n",
    "                \"Connecticut\",\"Maine\",\"Massachusetts\",\"New Hampshire\",\"Rhode Island\",\"Vermont\",\n",
    "                \"New Jersey\",\"New York\",\"Pennsylvania\"\n",
    "            }\n",
    "            midwest = {\n",
    "                \"Illinois\",\"Indiana\",\"Michigan\",\"Ohio\",\"Wisconsin\",\n",
    "                \"Iowa\",\"Kansas\",\"Minnesota\",\"Missouri\",\"Nebraska\",\"North Dakota\",\"South Dakota\"\n",
    "            }\n",
    "            south = {\n",
    "                \"Delaware\",\"District of Columbia\",\"Florida\",\"Georgia\",\"Maryland\",\"North Carolina\",\"South Carolina\",\"Virginia\",\"West Virginia\",\n",
    "                \"Alabama\",\"Kentucky\",\"Mississippi\",\"Tennessee\",\n",
    "                \"Arkansas\",\"Louisiana\",\"Oklahoma\",\"Texas\"\n",
    "            }\n",
    "            west = {\n",
    "                \"Arizona\",\"Colorado\",\"Idaho\",\"Montana\",\"Nevada\",\"New Mexico\",\"Utah\",\"Wyoming\",\n",
    "                \"Alaska\",\"California\",\"Hawaii\",\"Oregon\",\"Washington\"\n",
    "            }\n",
    "            def state_to_region(s):\n",
    "                s = str(s)\n",
    "                if s in (\"Missing\", \"Not in universe\", \"?\", \"unknown\", \"  ?\"):\n",
    "                    return \"Missing\"\n",
    "                if s in northeast: return \"Northeast\"\n",
    "                if s in midwest:   return \"Midwest\"\n",
    "                if s in south:     return \"South\"\n",
    "                if s in west:      return \"West\"\n",
    "                # for territories / foreign / odd tokens\n",
    "                if \"puerto\" in s.lower() or \"guam\" in s.lower() or \"outlying\" in s.lower():\n",
    "                    return \"Other\"\n",
    "                return \"Other\"\n",
    "            df[\"region_of_previous_residence\"] = df[\"state_of_previous_residence\"].map(state_to_region)\n",
    "\n",
    "        # 3) drop redundant/low-value columns (NOW after the mapping above)\n",
    "        drop_exist = [c for c in self.drop_cols_ if c in df.columns]\n",
    "        df = df.drop(columns=drop_exist, errors=\"ignore\")\n",
    "\n",
    "        # 4) if both household columns exist, keep the first one and drop the other\n",
    "        both_have = [c for c in self.household_pair_ if c in df.columns]\n",
    "        if len(both_have) == 2:\n",
    "            df = df.drop(columns=[both_have[1]])\n",
    "\n",
    "        # 5) derived features\n",
    "        if \"wage_per_hour\" in df.columns:\n",
    "            df[\"hourly_worker\"] = (df[\"wage_per_hour\"] > 0).astype(int)\n",
    "            df[\"log_wage_per_hour\"] = np.where(df[\"wage_per_hour\"] > 0, np.log1p(df[\"wage_per_hour\"]), 0.0)\n",
    "\n",
    "        for col in [\"capital_gains\", \"capital_losses\", \"dividends_from_stocks\"]:\n",
    "            if col in df.columns:\n",
    "                df[f\"has_{col}\"] = (df[col] > 0).astype(int)\n",
    "                df[f\"log_{col}\"] = np.where(df[col] > 0, np.log1p(df[col]), 0.0)\n",
    "\n",
    "        if \"weeks_worked_in_year\" in df.columns:\n",
    "            df[\"full_year_worker\"] = (df[\"weeks_worked_in_year\"] >= 50).astype(int)\n",
    "            df[\"no_work\"] = (df[\"weeks_worked_in_year\"] == 0).astype(int)\n",
    "\n",
    "        # 6) mobility flag (deterministic mapping based on your actual categories)\n",
    "\n",
    "        move_signals = []\n",
    "\n",
    "        # live_in_this_house_1_year_ago: Yes -> non-mover(0), No -> mover(1),\n",
    "        # \"Not in universe under 1 year old\" -> unknown (NaN)\n",
    "        if \"live_in_this_house_1_year_ago\" in df.columns:\n",
    "            s = df[\"live_in_this_house_1_year_ago\"].astype(str)\n",
    "            map_house = {\n",
    "                \"Yes\": 0,\n",
    "                \"No\": 1,\n",
    "                \"Not in universe under 1 year old\": np.nan,\n",
    "                \"Missing\": np.nan,   # in case it was normalized earlier\n",
    "            }\n",
    "            move_signals.append(s.map(map_house))\n",
    "\n",
    "        # migration_code-change_in_msa\n",
    "        # Nonmover -> 0; any to/from MSA/NonMSA/Abroad -> 1; Not identifiable/Missing -> NaN\n",
    "        if \"migration_code-change_in_msa\" in df.columns:\n",
    "            s = df[\"migration_code-change_in_msa\"].astype(str)\n",
    "            map_msa = {\n",
    "                \"Nonmover\": 0,\n",
    "                \"MSA to MSA\": 1,\n",
    "                \"NonMSA to nonMSA\": 1,\n",
    "                \"MSA to nonMSA\": 1,\n",
    "                \"NonMSA to MSA\": 1,\n",
    "                \"Abroad to MSA\": 1,\n",
    "                \"Abroad to nonMSA\": 1,\n",
    "                \"Not identifiable\": np.nan,\n",
    "                \"Not in universe\": np.nan,  # if normalization didn't run\n",
    "                \"Missing\": np.nan,\n",
    "            }\n",
    "            move_signals.append(s.map(map_msa))\n",
    "\n",
    "        # migration_code-change_in_reg\n",
    "        # \"Same county\"/\"Nonmover\" -> 0; any \"Different *\" or \"Abroad\" -> 1; Not in universe/Missing -> NaN\n",
    "        if \"migration_code-change_in_reg\" in df.columns:\n",
    "            s = df[\"migration_code-change_in_reg\"].astype(str)\n",
    "            map_reg = {\n",
    "                \"Nonmover\": 0,\n",
    "                \"Same county\": 0,\n",
    "                \"Different region\": 1,\n",
    "                \"Different county same state\": 1,\n",
    "                \"Different division same region\": 1,\n",
    "                \"Different state same division\": 1,\n",
    "                \"Abroad\": 1,\n",
    "                \"Not in universe\": np.nan,\n",
    "                \"Missing\": np.nan,\n",
    "            }\n",
    "            move_signals.append(s.map(map_reg))\n",
    "\n",
    "        # migration_code-move_within_reg\n",
    "        # \"Same county\"/\"Nonmover\" -> 0; any \"Different state ...\" or \"Different county same state\"/\"Abroad\" -> 1\n",
    "        if \"migration_code-move_within_reg\" in df.columns:\n",
    "            s = df[\"migration_code-move_within_reg\"].astype(str)\n",
    "            map_within = {\n",
    "                \"Nonmover\": 0,\n",
    "                \"Same county\": 0,\n",
    "                \"Different county same state\": 1,\n",
    "                \"Different state in South\": 1,\n",
    "                \"Different state in Northeast\": 1,\n",
    "                \"Different state in Midwest\": 1,\n",
    "                \"Different state in West\": 1,\n",
    "                \"Abroad\": 1,\n",
    "                \"Not in universe\": np.nan,\n",
    "                \"Missing\": np.nan,\n",
    "            }\n",
    "            move_signals.append(s.map(map_within))\n",
    "\n",
    "        #  migration_prev_res_in_sunbelt is not a direct move indicator, so we ignore here.\n",
    "\n",
    "        # Aggregate rule per row:\n",
    "        # if any signal == 1 -> mover (1)\n",
    "        # elif any signal == 0 -> non-mover (0)\n",
    "        # else -> unknown (NaN)  (SimpleImputer in your numeric pipeline will fill this, typically to 0)\n",
    "        if move_signals:\n",
    "            sig = pd.concat(move_signals, axis=1)\n",
    "            df[\"is_mover\"] = sig.apply(\n",
    "                lambda r: 1 if (r == 1).any() else (0 if (r == 0).any() else np.nan),\n",
    "                axis=1\n",
    "            ).astype(float)  # keep float so imputer can handle NaN\n",
    "        assert isinstance(df, pd.DataFrame), \"FeatureBuilder.transform must return a DataFrame\"\n",
    "        # age bucket\n",
    "        if \"age\" in df.columns:\n",
    "            df[\"age_bucket\"] = pd.cut(\n",
    "                df[\"age\"].astype(float),\n",
    "                bins=[0, 17, 24, 34, 44, 54, 64, 120],\n",
    "                labels=[\"u18\",\"18-24\",\"25-34\",\"35-44\",\"45-54\",\"55-64\",\"65+\"],\n",
    "                include_lowest=True\n",
    "            ).astype(\"category\")\n",
    "        \n",
    "        # Weeks worked bucket\n",
    "        if \"weeks_worked_in_year\" in df.columns:\n",
    "            w = df[\"weeks_worked_in_year\"].astype(float)\n",
    "            df[\"weeks_bucket\"] = pd.cut(\n",
    "                w, bins=[-1, 0, 26, 51, 100],\n",
    "                labels=[\"0\",\"1-26\",\"27-51\",\"52\"]\n",
    "            ).astype(\"category\")\n",
    "        \n",
    "        # Age x Education cross\n",
    "        if \"age_bucket\" in df.columns and \"education\" in df.columns:\n",
    "            df[\"age_edu\"] = (df[\"age_bucket\"].astype(str) + \"|\" + df[\"education\"].astype(str)).astype(\"category\")\n",
    "        has_cols = []\n",
    "        for c in [\"capital_gains\",\"capital_losses\",\"dividends_from_stocks\"]:\n",
    "            if c in df.columns:\n",
    "                has_cols.append((df[c] > 0).astype(int))\n",
    "        if has_cols:\n",
    "            df[\"has_any_capital\"] = pd.concat(has_cols, axis=1).max(axis=1).astype(int)\n",
    "\n",
    "        if \"capital_gains\" in df.columns and \"capital_losses\" in df.columns:\n",
    "            net = (df[\"capital_gains\"].fillna(0) - df[\"capital_losses\"].fillna(0))\n",
    "            df[\"cap_net\"] = net\n",
    "            df[\"log_cap_net_pos\"] = np.where(net > 0, np.log1p(net), 0.0)\n",
    "        # Married flag\n",
    "        if \"marital_stat\" in df.columns:\n",
    "            m = df[\"marital_stat\"].astype(str)\n",
    "            df[\"is_married\"] = m.str.contains(\"Married\", case=False, na=False).astype(int)\n",
    "\n",
    "        # Union member flag\n",
    "        if \"member_of_a_labor_union\" in df.columns:\n",
    "            u = df[\"member_of_a_labor_union\"].astype(str).str.strip().str.lower()\n",
    "            df[\"is_union\"] = u.eq(\"yes\").astype(int)\n",
    "\n",
    "        # Self-employed flag\n",
    "        if \"class_of_worker\" in df.columns:\n",
    "            cw = df[\"class_of_worker\"].astype(str).str.lower()\n",
    "            df[\"is_self_employed\"] = cw.str.contains(\"self-employed\", na=False).astype(int)\n",
    "        if \"migration_prev_res_in_sunbelt\" in df.columns:\n",
    "            s = df[\"migration_prev_res_in_sunbelt\"].astype(str).str.strip().str.lower()\n",
    "            s = s.replace({\"not in universe\": \"missing\"})\n",
    "            s = s.where(s.isin([\"yes\",\"no\",\"missing\"]), \"missing\")\n",
    "            df[\"prev_res_sunbelt\"] = s.astype(\"category\")\n",
    "        return df\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        # Be explicit to avoid surprises with Mixin versions\n",
    "        return self.transform(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea397fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Target Mean Encoder (renamed to avoid clash) ---\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "class TargetMeanEncoder1D(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, m=50):\n",
    "        self.m = m  # smoothing\n",
    "\n",
    "    def _to_series(self, X):\n",
    "        if isinstance(X, pd.Series):\n",
    "            return X\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            if X.shape[1] != 1:\n",
    "                raise ValueError(\"TargetMeanEncoder1D expects exactly 1 column.\")\n",
    "            return X.iloc[:, 0]\n",
    "        arr = np.asarray(X)\n",
    "        if arr.ndim == 2 and arr.shape[1] == 1:\n",
    "            arr = arr[:, 0]\n",
    "        return pd.Series(arr)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        s = self._to_series(X).reset_index(drop=True)\n",
    "        y = pd.Series(y).reset_index(drop=True)\n",
    "        self.global_mean_ = float(y.mean())\n",
    "        df_ = pd.DataFrame({\"cat\": s, \"y\": y})\n",
    "        means = df_.groupby(\"cat\")[\"y\"].mean()\n",
    "        counts = df_.groupby(\"cat\")[\"y\"].count()\n",
    "        m = self.m\n",
    "        enc = (means * counts + m * self.global_mean_) / (counts + m)\n",
    "        self.mapping_ = enc.to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        s = self._to_series(X)\n",
    "        mapped = s.map(self.mapping_)\n",
    "        mapped = pd.to_numeric(mapped, errors=\"coerce\")\n",
    "        out = mapped.fillna(float(self.global_mean_)).to_numpy().reshape(-1, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ab676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to map various income labels to binary 0/1\n",
    "def map_label_to_binary_census(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Map income label to {0,1} for Census/Adult variants.\n",
    "    Handles numeric 0/1; '<=50K'/'<=50K.'; '>50K'/'50000+.'; '- 50000.'; and generic patterns.\n",
    "    \"\"\"\n",
    "    # Case 1: already numeric 0/1\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        uniq = set(pd.unique(s))\n",
    "        if uniq <= {0, 1}:\n",
    "            return s.astype(int)\n",
    "\n",
    "    # Normalize strings: strip, lower, remove spaces and periods\n",
    "    s_norm = (\n",
    "        s.astype(str)\n",
    "         .str.strip()\n",
    "         .str.lower()\n",
    "         .str.replace(r\"\\s+\", \"\", regex=True)   # remove all spaces\n",
    "         .str.replace(\".\", \"\", regex=False)     # remove dots\n",
    "    )\n",
    "\n",
    "    # Exact tokens commonly seen\n",
    "    exact_map = {\n",
    "        \"<=50k\": 0, \"<=50k\": 0, \"<=50\": 0, \"<50k\": 0, \"<50\": 0,\n",
    "        \"-50000\": 0,            # maps ' - 50000.' → '-50000'\n",
    "        \">50k\": 1, \">50\": 1,\n",
    "        \"50000+\": 1,            # maps ' 50000+.' → '50000+'\n",
    "        \"50k+\": 1\n",
    "    }\n",
    "\n",
    "    # If all values are in the exact_map keys\n",
    "    if set(pd.unique(s_norm)) <= set(exact_map.keys()):\n",
    "        return s_norm.map(exact_map).astype(int)\n",
    "\n",
    "    # Fallback: pattern-based (covers mixed vocab)\n",
    "    is_high = (\n",
    "        s_norm.str.contains(r\"(>50k|>50|50000\\+|50k\\+)\", regex=True)\n",
    "    )\n",
    "    is_low = (\n",
    "        s_norm.str.contains(r\"(<\\=50k|<\\=50|<50k|<50|-50000)\", regex=True)\n",
    "    )\n",
    "\n",
    "    y = pd.Series(np.where(is_high, 1, np.where(is_low, 0, np.nan)), index=s.index)\n",
    "\n",
    "    # Final check\n",
    "    if y.isna().any():\n",
    "        print(\"Unmapped label examples (top 10):\")\n",
    "        print(s.loc[y.isna()].value_counts().head(10))\n",
    "        raise ValueError(\"Some labels could not be mapped. Update the mapping patterns.\")\n",
    "    return y.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a7379",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8adf46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y distribution:\n",
      " label\n",
      "0    183912\n",
      "1     12382\n",
      "Name: count, dtype: int64\n",
      "FeatureBuilder output shape: (196294, 56)\n",
      "numeric_cols: ['age', 'weeks_worked_in_year', 'log_wage_per_hour', 'log_capital_gains', 'log_capital_losses', 'log_dividends_from_stocks', 'hourly_worker', 'has_capital_gains', 'has_capital_losses', 'has_dividends_from_stocks', 'full_year_worker', 'no_work', 'is_mover', 'has_any_capital', 'cap_net', 'log_cap_net_pos', 'is_married', 'is_union', 'is_self_employed']\n",
      "low_card_cats: ['sex', 'marital_stat', 'education', 'class_of_worker', 'citizenship', 'region_of_previous_residence', 'age_bucket', 'weeks_bucket', 'age_edu', 'prev_res_sunbelt']\n",
      "high_card_cats: ['major_industry_code', 'major_occupation_code', 'detailed_industry_recode', 'detailed_occupation_recode', 'age_edu']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# ===================== 1) Label mapping & build raw X =====================\n",
    "TARGET_COL = \"label\"\n",
    "y = map_label_to_binary_census(df[TARGET_COL])\n",
    "print(\"y distribution:\\n\", y.value_counts(dropna=False))\n",
    "assert y.nunique() == 2, \"Label must contain both classes before splitting.\"\n",
    "# --- Build sample weights from survey weight column ---\n",
    "w_all = df[\"weight\"].astype(float)\n",
    "w_all = w_all / w_all.mean()  # normalize to mean=1 for numeric stability\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "\n",
    "# ===================== 2) Probe FeatureBuilder OUTPUT (no X_train needed) =====================\n",
    "# We only use this to discover which columns exist AFTER FeatureBuilder.\n",
    "# This does NOT use y and does NOT leak target information.\n",
    "tmp = FeatureBuilder().fit_transform(X.copy())\n",
    "print(\"FeatureBuilder output shape:\", tmp.shape)\n",
    "\n",
    "\n",
    "# Candidate lists we WANT to use\n",
    "numeric_cols_all = [\n",
    "    \"age\", \"weeks_worked_in_year\",\n",
    "    \"log_wage_per_hour\", \"log_capital_gains\", \"log_capital_losses\", \"log_dividends_from_stocks\",\n",
    "    \"hourly_worker\", \"has_capital_gains\", \"has_capital_losses\", \"has_dividends_from_stocks\",\n",
    "    \"full_year_worker\", \"no_work\", \"is_mover\",\"has_any_capital\", \"cap_net\", \"log_cap_net_pos\",\n",
    "    \"is_married\", \"is_union\", \"is_self_employed\"\n",
    "]\n",
    "low_card_cats_all = [\n",
    "    \"sex\", \"marital_stat\", \"education\", \"class_of_worker\",\n",
    "    \"citizenship\", \"region_of_previous_residence\",\"age_bucket\", \"weeks_bucket\", \"age_edu\",\n",
    "    \"prev_res_sunbelt\"\n",
    "]\n",
    "high_card_cats_all = [\n",
    "    \"major_industry_code\", \"major_occupation_code\",\n",
    "    \"detailed_industry_recode\", \"detailed_occupation_recode\"\n",
    "]\n",
    "if \"age_edu\" in tmp.columns:\n",
    "    nunq = tmp[\"age_edu\"].nunique(dropna=True)\n",
    "    if nunq <= 40:\n",
    "        low_card_cats_all.append(\"age_edu\")\n",
    "    else:\n",
    "        high_card_cats_all.append(\"age_edu\")\n",
    "\n",
    "# Keep only features that actually exist after FeatureBuilder\n",
    "numeric_cols   = [c for c in numeric_cols_all   if c in tmp.columns]\n",
    "low_card_cats  = [c for c in low_card_cats_all  if c in tmp.columns]\n",
    "high_card_cats = [c for c in high_card_cats_all if c in tmp.columns]\n",
    "\n",
    "print(\"numeric_cols:\", numeric_cols)\n",
    "print(\"low_card_cats:\", low_card_cats)\n",
    "print(\"high_card_cats:\", high_card_cats)\n",
    "\n",
    "# ===================== 3) Build preprocess & pipeline =====================\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# OneHot compatibility across sklearn versions\n",
    "try:\n",
    "    onehot = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "except TypeError:\n",
    "    onehot = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Target-mean encoder blocks for high-card categoricals\n",
    "tgt_blocks = [(f\"tgt_{col}\", TargetMeanEncoder1D(m=50), [col]) for col in high_card_cats]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipe, numeric_cols),\n",
    "        (\"ohe\", onehot, low_card_cats),\n",
    "        *tgt_blocks\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62f2b25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (117776, 41) (39259, 41) (39259, 41)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.25, stratify=y_trainval, random_state=42\n",
    ")\n",
    "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1db09519",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_trainval = w_all.loc[X_trainval.index]\n",
    "w_train    = w_all.loc[X_train.index]\n",
    "w_val      = w_all.loc[X_val.index]\n",
    "w_test     = w_all.loc[X_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f19ac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Build sample weights from survey 'weight' (after you have X_trainval / y_trainval) ====\n",
    "w_all = df[\"weight\"].astype(float)\n",
    "w_all = w_all / w_all.mean()  # normalize for numeric stability\n",
    "\n",
    "w_trainval = w_all.loc[X_trainval.index]\n",
    "\n",
    "# (optional) compute weighted class ratio for tree models\n",
    "neg_w = w_trainval[y_trainval == 0].sum()\n",
    "pos_w = w_trainval[y_trainval == 1].sum()\n",
    "spw_weighted = float(neg_w / max(pos_w, 1e-12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effbff1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette by k: {4: 0.23369178680130037, 5: 0.24590672378290468, 6: 0.2823499812513904, 7: 0.2585143891999381, 8: 0.32326370240677066}  -> best_k: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Numeric profile | weighted means]\n",
      "            age  weeks_worked_in_year  log_wage_per_hour  is_mover  full_year_worker  no_work\n",
      "segment                                                                                      \n",
      "0        58.336                 1.459              0.000     0.059             0.000    0.911\n",
      "1        37.271                45.928              0.000     0.102             0.733    0.000\n",
      "2        42.448                48.330              5.873     0.062             0.798    0.002\n",
      "3        35.985                44.844              6.722     0.089             0.706    0.028\n",
      "4        48.825                38.565              0.005     0.067             0.662    0.198\n",
      "5        43.866                41.381              0.435     0.076             0.698    0.118\n",
      "6        50.025                32.815              0.004     0.049             0.549    0.297\n",
      "7         9.494                 0.521              0.000     0.089             0.000    0.958\n",
      "\n",
      "[Categorical profile | top categories per segment]\n",
      "\n",
      "- sex (weighted %):\n",
      "sex      Female  Male\n",
      "segment              \n",
      "0          68.3  31.7\n",
      "1          47.9  52.1\n",
      "2          26.9  73.1\n",
      "3          51.9  48.1\n",
      "4          26.7  73.3\n",
      "5          27.7  72.3\n",
      "6          53.2  46.8\n",
      "7          49.1  50.9\n",
      "\n",
      "- education (weighted %):\n",
      "education  10th grade  7th and 8th grade  9th grade  Bachelors degree(BA AB BS)  Children  High school graduate  Some college but no degree\n",
      "segment                                                                                                                                    \n",
      "0                 NaN               11.1        NaN                         NaN       NaN                  36.4                        13.4\n",
      "1                 NaN                NaN        NaN                        14.6       NaN                  34.7                        21.4\n",
      "2                 NaN                NaN        NaN                        15.6       NaN                  31.8                        22.0\n",
      "3                 NaN                NaN        NaN                         9.5       NaN                  39.3                        24.5\n",
      "4                 NaN                NaN        NaN                        22.9       NaN                  26.6                        17.6\n",
      "5                 NaN                NaN        NaN                        21.5       NaN                  25.4                        17.9\n",
      "6                 NaN                NaN        NaN                        27.0       NaN                  25.9                        18.3\n",
      "7                 3.9                NaN        4.2                         NaN      78.2                   NaN                         NaN\n",
      "\n",
      "- class_of_worker (weighted %):\n",
      "class_of_worker  Federal government  Local government  Missing  Never worked  Private  Self-employed-not incorporated  State government\n",
      "segment                                                                                                                                \n",
      "0                               NaN               NaN     94.1           NaN      4.2                             0.9               NaN\n",
      "1                               NaN               7.9      NaN           NaN     71.4                             8.7               NaN\n",
      "2                               8.0              16.5      NaN           NaN     70.7                             NaN               NaN\n",
      "3                               NaN               6.9      NaN           NaN     86.7                             NaN               3.2\n",
      "4                               NaN               NaN     23.9           NaN     51.8                             7.9               NaN\n",
      "5                               NaN               NaN     16.5           NaN     58.1                             7.5               NaN\n",
      "6                               NaN               NaN     34.1           NaN     43.6                             7.1               NaN\n",
      "7                               NaN               NaN     97.0           0.7      2.0                             NaN               NaN\n",
      "\n",
      "- citizenship (weighted %):\n",
      "citizenship  Foreign born- Not a citizen of U S  Foreign born- U S citizen by naturalization  Native- Born abroad of American Parent(s)  \\\n",
      "segment                                                                                                                                   \n",
      "0                                           8.7                                          4.3                                        NaN   \n",
      "1                                           7.6                                          3.2                                        NaN   \n",
      "2                                           2.4                                          2.8                                        NaN   \n",
      "3                                           5.4                                          2.8                                        NaN   \n",
      "4                                           4.1                                          3.9                                        NaN   \n",
      "5                                           5.1                                          3.5                                        NaN   \n",
      "6                                           2.1                                          3.3                                        NaN   \n",
      "7                                           4.4                                          NaN                                        0.7   \n",
      "\n",
      "citizenship  Native- Born in the United States  \n",
      "segment                                         \n",
      "0                                         85.2  \n",
      "1                                         87.6  \n",
      "2                                         93.0  \n",
      "3                                         90.7  \n",
      "4                                         90.7  \n",
      "5                                         90.2  \n",
      "6                                         93.7  \n",
      "7                                         94.0  \n",
      "\n",
      "- region_of_previous_residence (weighted %):\n",
      "region_of_previous_residence  Midwest  Missing  South  West\n",
      "segment                                                    \n",
      "0                                 NaN     94.1    2.0   1.5\n",
      "1                                 NaN     89.8    3.7   2.5\n",
      "2                                 1.6     93.8    NaN   2.7\n",
      "3                                 2.5     91.1    2.8   NaN\n",
      "4                                 1.7     93.3    1.9   NaN\n",
      "5                                 NaN     92.4    2.4   1.9\n",
      "6                                 1.4     95.1    1.6   NaN\n",
      "7                                 NaN     91.1    3.2   2.2\n",
      "\n",
      "- age_bucket (weighted %):\n",
      "age_bucket  18-24  25-34  35-44  45-54  55-64   65+   u18\n",
      "segment                                                  \n",
      "0             NaN    NaN   13.5    NaN   15.6  45.8   NaN\n",
      "1             NaN   28.1   26.4   17.0    NaN   NaN   NaN\n",
      "2             NaN   23.6   28.4   20.9    NaN   NaN   NaN\n",
      "3            19.8   26.9   24.3    NaN    NaN   NaN   NaN\n",
      "4             NaN    NaN   23.7   21.0    NaN  22.4   NaN\n",
      "5             NaN   20.7   25.8   20.5    NaN   NaN   NaN\n",
      "6             NaN    NaN   22.7   21.0    NaN  21.9   NaN\n",
      "7             8.3    2.2    NaN    NaN    NaN   NaN  89.4\n",
      "\n",
      "- weeks_bucket (weighted %):\n",
      "weeks_bucket     0  1-26  27-51    52\n",
      "segment                              \n",
      "0             91.1   7.3    1.6   NaN\n",
      "1              NaN  11.6   18.2  70.3\n",
      "2              NaN   5.5   16.7  77.7\n",
      "3              NaN  10.3   19.6  67.2\n",
      "4             19.8   NaN   10.4  63.9\n",
      "5             11.8   NaN   12.6  66.8\n",
      "6             29.7   NaN   10.3  52.9\n",
      "7             95.8   3.8    0.3   NaN\n",
      "\n",
      "- is_union (weighted %):\n",
      "is_union      0     1\n",
      "segment              \n",
      "0         100.0   0.0\n",
      "1          98.9   1.1\n",
      "2          55.7  44.3\n",
      "3          83.7  16.3\n",
      "4         100.0   0.0\n",
      "5          98.0   2.0\n",
      "6          98.9   1.1\n",
      "7         100.0   0.0\n",
      "\n",
      "- is_self_employed (weighted %):\n",
      "is_self_employed      0     1\n",
      "segment                      \n",
      "0                  99.0   1.0\n",
      "1                  88.6  11.4\n",
      "2                 100.0   0.0\n",
      "3                 100.0   0.0\n",
      "4                  86.5  13.5\n",
      "5                  87.5  12.5\n",
      "6                  88.4  11.6\n",
      "7                  99.9   0.1\n",
      "\n",
      "- has_any_capital (weighted %):\n",
      "has_any_capital      0      1\n",
      "segment                      \n",
      "0                 99.9    0.1\n",
      "1                100.0    0.0\n",
      "2                  0.0  100.0\n",
      "3                 90.2    9.8\n",
      "4                  0.0  100.0\n",
      "5                  0.0  100.0\n",
      "6                  0.0  100.0\n",
      "7                100.0    0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------- 1) Unsupervised-friendly encoder for high-card categoricals ----------\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "class HashingCats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Hashing trick for multiple categorical columns: fast and target-free.\"\"\"\n",
    "    def __init__(self, cols, n_features=2**12, alternate_sign=False):\n",
    "        self.cols = cols\n",
    "        self.n_features = n_features\n",
    "        self.alternate_sign = alternate_sign\n",
    "        self.hasher = FeatureHasher(\n",
    "            n_features=n_features,\n",
    "            input_type=\"string\",\n",
    "            alternate_sign=alternate_sign\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # stateless\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not self.cols:\n",
    "            # return empty sparse matrix with right #rows\n",
    "            from scipy import sparse\n",
    "            return sparse.csr_matrix((len(X), self.n_features))\n",
    "        # build tokens like \"col=value\" per row\n",
    "        tokens = (\n",
    "            X[self.cols].astype(str)\n",
    "              .apply(lambda r: [f\"{c}={r[c]}\" for c in self.cols], axis=1)\n",
    "              .tolist()\n",
    "        )\n",
    "        return self.hasher.transform(tokens)\n",
    "\n",
    "# ---------- 2) Build unsupervised preprocess (no target-mean encoding) ----------\n",
    "# OneHot (version-compat)\n",
    "try:\n",
    "    onehot = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "except TypeError:\n",
    "    onehot = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "num_pipe_unsup = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler(with_mean=True, with_std=True))\n",
    "])\n",
    "\n",
    "hash_block = (\"hash\", HashingCats(cols=[], n_features=2**12), None)  # placeholder; set cols below\n",
    "\n",
    "# Extend low-card with engineered buckets if present\n",
    "\n",
    "low_card_all = [c for c in low_card_cats  if c in tmp.columns]\n",
    "high_card_all = [c for c in high_card_cats if c in tmp.columns]\n",
    "\n",
    "# update hashing block with actual high-card cols\n",
    "hash_block = (\"hash\", HashingCats(cols=high_card_all, n_features=2**12), high_card_all)\n",
    "\n",
    "pre_unsup = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\",  num_pipe_unsup, numeric_cols),\n",
    "        (\"ohe\",  onehot, low_card_all),\n",
    "        hash_block,\n",
    "    ],\n",
    "    remainder=\"drop\",              # keep only what we specified\n",
    "    sparse_threshold=1.0           # keep it sparse so SVD can handle efficiently\n",
    ")\n",
    "\n",
    "# Full feature builder + preprocess + SVD pipeline\n",
    "prep_pipe = Pipeline([\n",
    "    (\"feat\", FeatureBuilder()),\n",
    "    (\"prep\", pre_unsup),\n",
    "    (\"svd\", TruncatedSVD(n_components=30, random_state=42))  # 20~50 is often plenty\n",
    "])\n",
    "\n",
    "# ---------- 3) Prepare weights (normalized) ----------\n",
    "w_all = df[\"weight\"].astype(float)\n",
    "w_all = w_all / w_all.mean()\n",
    "\n",
    "# Split you already have: X_trainval, X_test, etc.\n",
    "# Fit prep on TrainVal to avoid any look-ahead\n",
    "Z_trainval = prep_pipe.fit_transform(X_trainval)\n",
    "Z_test     = prep_pipe.transform(X_test)\n",
    "\n",
    "# ---------- 4) Pick K by a quick silhouette sweep (unweighted) ----------\n",
    "k_candidates = list(range(4, 9))\n",
    "sil_scores = {}\n",
    "for k in k_candidates:\n",
    "    km = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
    "    km.fit(Z_trainval, sample_weight=w_trainval)  # weighted clustering\n",
    "    labels = km.labels_\n",
    "    # silhouette_score has no sample_weight; use as heuristic\n",
    "    try:\n",
    "        sil = silhouette_score(Z_trainval, labels, metric=\"euclidean\")\n",
    "    except Exception:\n",
    "        sil = np.nan\n",
    "    sil_scores[k] = sil\n",
    "best_k = max(sil_scores, key=lambda kk: (sil_scores[kk] if sil_scores[kk] == sil_scores[kk] else -1))\n",
    "print(\"Silhouette by k:\", sil_scores, \" -> best_k:\", best_k)\n",
    "\n",
    "# ---------- 5) Fit final model with best_k on TrainVal ----------\n",
    "kmeans = KMeans(n_clusters=best_k, n_init=\"auto\", random_state=42)\n",
    "kmeans.fit(Z_trainval, sample_weight=w_trainval)\n",
    "labels_trainval = pd.Series(kmeans.labels_, index=X_trainval.index, name=\"segment\")\n",
    "\n",
    "# also assign segments on Test\n",
    "labels_test = pd.Series(kmeans.predict(Z_test), index=X_test.index, name=\"segment\")\n",
    "\n",
    "# ---------- 6) Build weighted profiles per segment ----------\n",
    "# join back with original (post-FeatureBuilder) features for readable profiling\n",
    "Xtv_feat = prep_pipe.named_steps[\"feat\"].transform(X_trainval.copy())  # DataFrame with engineered columns\n",
    "Xte_feat = prep_pipe.named_steps[\"feat\"].transform(X_test.copy())\n",
    "\n",
    "def wmean_by_seg(s_values, seg, w):\n",
    "    num = (s_values * w).groupby(seg).sum()\n",
    "    den = w.groupby(seg).sum()\n",
    "    return (num / den).rename(s_values.name)\n",
    "\n",
    "def wprop_table(cat_series, seg, w):\n",
    "    # returns segment x category proportions (weighted)\n",
    "    dummies = pd.get_dummies(cat_series.astype(str), dummy_na=False)\n",
    "    wmat = dummies.mul(w.values.reshape(-1, 1))\n",
    "    grp = wmat.groupby(seg).sum()\n",
    "    grp = grp.div(grp.sum(axis=1), axis=0)\n",
    "    grp.columns.name = cat_series.name\n",
    "    return grp\n",
    "\n",
    "# pick a few numeric & categorical for marketing-readable profiles\n",
    "num_to_show = [c for c in [\"age\",\"weeks_worked_in_year\",\"log_wage_per_hour\",\"is_mover\",\n",
    "                           \"full_year_worker\",\"no_work\"] if c in Xtv_feat.columns]\n",
    "cat_to_show = [c for c in [\"sex\",\"education\",\"class_of_worker\",\"citizenship\",\n",
    "                           \"region_of_previous_residence\",\"age_bucket\",\"weeks_bucket\",\n",
    "                           \"is_union\",\"is_self_employed\",\"has_any_capital\"] if c in Xtv_feat.columns]\n",
    "\n",
    "w_seg = w_trainval.copy()\n",
    "seg_trainval = labels_trainval.reindex(X_trainval.index)\n",
    "\n",
    "# numeric summaries\n",
    "num_profiles = []\n",
    "for c in num_to_show:\n",
    "    num_profiles.append(wmean_by_seg(Xtv_feat[c], seg_trainval, w_seg))\n",
    "num_profiles = pd.concat(num_profiles, axis=1)\n",
    "print(\"\\n[Numeric profile | weighted means]\")\n",
    "print(num_profiles.round(3))\n",
    "\n",
    "# categorical summaries: show top-3 categories per segment for each field\n",
    "print(\"\\n[Categorical profile | top categories per segment]\")\n",
    "for c in cat_to_show:\n",
    "    tab = wprop_table(Xtv_feat[c], seg_trainval, w_seg)\n",
    "    top3 = tab.apply(lambda r: r.sort_values(ascending=False).head(3), axis=1)\n",
    "    print(f\"\\n- {c} (weighted %):\")\n",
    "    print((top3*100).round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53d96cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === KMeans K-selection with multiple indices, repeats, and weighted-share constraint ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    ")\n",
    "\n",
    "def _safe_kmeans(k, seed):\n",
    "    \"\"\"Return a KMeans instance compatible with different sklearn versions.\"\"\"\n",
    "    try:\n",
    "        return KMeans(n_clusters=k, n_init=\"auto\", random_state=seed)\n",
    "    except TypeError:\n",
    "        return KMeans(n_clusters=k, n_init=10, random_state=seed)\n",
    "\n",
    "def _weighted_cluster_share(labels, weights):\n",
    "    \"\"\"Compute weighted share per cluster; returns a pd.Series indexed by label.\"\"\"\n",
    "    weights = np.asarray(weights).reshape(-1)\n",
    "    df = pd.DataFrame({\"lab\": labels, \"w\": weights})\n",
    "    return df.groupby(\"lab\")[\"w\"].sum() / df[\"w\"].sum()\n",
    "\n",
    "def pick_kmeans_k(\n",
    "    Z,\n",
    "    weights=None,\n",
    "    k_list=range(4, 11),\n",
    "    n_repeats=8,\n",
    "    min_weighted_share=0.02,   # drop runs that create tiny weighted clusters\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate KMeans over k_list with multiple random seeds.\n",
    "    Primary rank: mean silhouette (higher is better).\n",
    "    Tie-breakers: mean CH (higher), mean DB (lower), mean inertia (lower).\n",
    "    Enforces a minimum weighted cluster share per run.\n",
    "    Returns: summary_df, best_k, final_model\n",
    "    \"\"\"\n",
    "    Z = np.asarray(Z)  # ensure array-like\n",
    "    n = Z.shape[0]\n",
    "    if weights is None:\n",
    "        weights = np.ones(n, float)\n",
    "    weights = np.asarray(weights).reshape(-1)\n",
    "\n",
    "    rows = []      # per-run records\n",
    "    agg = []       # per-k aggregates\n",
    "\n",
    "    # ---- sweep K and repeats ----\n",
    "    for k in k_list:\n",
    "        for rep in range(n_repeats):\n",
    "            seed = random_state + rep\n",
    "            km = _safe_kmeans(k, seed)\n",
    "            # fit with sample weights (sklearn KMeans supports sample_weight)\n",
    "            km.fit(Z, sample_weight=weights)\n",
    "            labels = km.labels_\n",
    "\n",
    "            # filter out degenerate runs with tiny weighted clusters\n",
    "            share = _weighted_cluster_share(labels, weights)\n",
    "            min_share = share.min()\n",
    "\n",
    "            # compute metrics (silhouette/DB/CH ignore weights by sklearn design)\n",
    "            try:\n",
    "                sil = silhouette_score(Z, labels, metric=\"euclidean\")\n",
    "            except Exception:\n",
    "                sil = np.nan\n",
    "            try:\n",
    "                db = davies_bouldin_score(Z, labels)\n",
    "            except Exception:\n",
    "                db = np.nan\n",
    "            try:\n",
    "                ch = calinski_harabasz_score(Z, labels)\n",
    "            except Exception:\n",
    "                ch = np.nan\n",
    "\n",
    "            rows.append({\n",
    "                \"k\": k,\n",
    "                \"rep\": rep,\n",
    "                \"seed\": seed,\n",
    "                \"silhouette\": sil,\n",
    "                \"db\": db,\n",
    "                \"ch\": ch,\n",
    "                \"inertia\": float(km.inertia_),\n",
    "                \"min_weighted_share\": float(min_share),\n",
    "                \"valid\": bool(min_share >= min_weighted_share),\n",
    "            })\n",
    "\n",
    "    runs = pd.DataFrame(rows)\n",
    "\n",
    "    # ---- aggregate per k only over valid runs ----\n",
    "    def _agg(df):\n",
    "        return pd.Series({\n",
    "            \"n_valid\": df[\"valid\"].sum(),\n",
    "            \"n_total\": len(df),\n",
    "            \"sil_mean\": df.loc[df[\"valid\"], \"silhouette\"].mean(),\n",
    "            \"sil_std\":  df.loc[df[\"valid\"], \"silhouette\"].std(),\n",
    "            \"ch_mean\":  df.loc[df[\"valid\"], \"ch\"].mean(),\n",
    "            \"db_mean\":  df.loc[df[\"valid\"], \"db\"].mean(),\n",
    "            \"inertia_mean\": df.loc[df[\"valid\"], \"inertia\"].mean(),\n",
    "            \"min_share_min\": df.loc[df[\"valid\"], \"min_weighted_share\"].min(),\n",
    "            \"min_share_mean\": df.loc[df[\"valid\"], \"min_weighted_share\"].mean(),\n",
    "        })\n",
    "\n",
    "    summary = runs.groupby(\"k\").apply(_agg).sort_index()\n",
    "\n",
    "    # ---- pick best_k by a robust composite rule ----\n",
    "    # Primary: highest sil_mean (nan -> -inf). Tie-breakers: higher ch_mean, lower db_mean, lower inertia_mean.\n",
    "    def rank_key(krow):\n",
    "        s = krow[\"sil_mean\"]\n",
    "        ch = krow[\"ch_mean\"]\n",
    "        db = krow[\"db_mean\"]\n",
    "        ine = krow[\"inertia_mean\"]\n",
    "        # convert NaNs to conservative values\n",
    "        s  = -np.inf if pd.isna(s)  else s\n",
    "        ch = -np.inf if pd.isna(ch) else ch\n",
    "        db =  np.inf if pd.isna(db) else db\n",
    "        ine=  np.inf if pd.isna(ine) else ine\n",
    "        # Higher sil, higher CH, lower DB, lower inertia\n",
    "        return (-s, -ch, db, ine)\n",
    "\n",
    "    # drop k with zero valid runs\n",
    "    summary_valid = summary[summary[\"n_valid\"] > 0].copy()\n",
    "    if summary_valid.empty:\n",
    "        raise RuntimeError(\"All runs invalid under the min_weighted_share constraint. \"\n",
    "                           \"Relax min_weighted_share or check data preprocessing.\")\n",
    "\n",
    "    best_k = min(summary_valid.index, key=lambda kk: rank_key(summary_valid.loc[kk]))\n",
    "\n",
    "    # ---- fit final model on all data (best_k) with a fresh seed ----\n",
    "    final_seed = random_state + 999\n",
    "    final_km = _safe_kmeans(best_k, final_seed)\n",
    "    final_km.fit(Z, sample_weight=weights)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n[KMeans CV-like sweep] summary (valid runs only):\")\n",
    "        print(summary_valid[[\n",
    "            \"n_valid\",\"n_total\",\"sil_mean\",\"sil_std\",\"ch_mean\",\"db_mean\",\"inertia_mean\",\n",
    "            \"min_share_min\",\"min_share_mean\"\n",
    "        ]].round(3).sort_values(\"sil_mean\", ascending=False))\n",
    "        print(f\"\\n[Selected best_k] {best_k}  |  \"\n",
    "              f\"sil_mean={summary_valid.loc[best_k,'sil_mean']:.3f}  |  \"\n",
    "              f\"CH={summary_valid.loc[best_k,'ch_mean']:.1f}  |  \"\n",
    "              f\"DB={summary_valid.loc[best_k,'db_mean']:.3f}  |  \"\n",
    "              f\"min_weighted_share≈{summary_valid.loc[best_k,'min_share_min']:.3f}\")\n",
    "\n",
    "    return summary_valid, int(best_k), final_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3ff6c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[KMeans CV-like sweep] summary (valid runs only):\n",
      "    n_valid  n_total  sil_mean  sil_std    ch_mean  db_mean  inertia_mean  min_share_min  min_share_mean\n",
      "k                                                                                                       \n",
      "10      1.0      8.0     0.339      NaN  32314.869    1.316   1403200.261          0.021           0.021\n",
      "9       3.0      8.0     0.328    0.002  34643.118    1.290   1450862.010          0.021           0.021\n",
      "8       6.0      8.0     0.327    0.019  34860.245    1.288   1572844.411          0.021           0.021\n",
      "7       5.0      8.0     0.293    0.023  29923.507    1.413   1878169.348          0.021           0.027\n",
      "6       6.0      8.0     0.282    0.019  27206.178    1.468   2151877.292          0.021           0.029\n",
      "4       8.0      8.0     0.264    0.035  28258.186    1.540   2614253.112          0.021           0.042\n",
      "5       8.0      8.0     0.256    0.027  26760.512    1.562   2393291.999          0.021           0.031\n",
      "\n",
      "[Selected best_k] 10  |  sil_mean=0.339  |  CH=32314.9  |  DB=1.316  |  min_weighted_share≈0.021\n",
      "\n",
      "[Final KMeans] weighted_share (%)\n",
      " segment\n",
      "0    25.4\n",
      "1     4.3\n",
      "2    23.4\n",
      "3    16.0\n",
      "4     8.6\n",
      "5     5.6\n",
      "6     4.2\n",
      "7     2.1\n",
      "8     3.8\n",
      "9     6.8\n",
      "Name: weight, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rt/qjkw9ljx5h3fr3tbpb26t3s00000gn/T/ipykernel_73189/2214396932.py:104: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  summary = runs.groupby(\"k\").apply(_agg).sort_index()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summary, best_k, kmeans_final = pick_kmeans_k(\n",
    "    Z=Z_trainval,\n",
    "    weights=w_trainval.values,\n",
    "    k_list=range(4, 11),      \n",
    "    n_repeats=8,              \n",
    "    min_weighted_share=0.02,  \n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "labels_trainval = pd.Series(kmeans_final.labels_, index=X_trainval.index, name=\"segment\")\n",
    "labels_test = pd.Series(kmeans_final.predict(Z_test), index=X_test.index, name=\"segment\")\n",
    "\n",
    "wshare = 100.0 * w_trainval.groupby(labels_trainval).sum() / w_trainval.sum()\n",
    "print(\"\\n[Final KMeans] weighted_share (%)\\n\", wshare.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea3e35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Weighted share % | TrainVal]\n",
      " segment\n",
      "0    25.4\n",
      "2    23.4\n",
      "3    16.0\n",
      "4     8.6\n",
      "9     6.8\n",
      "5     5.6\n",
      "1     4.3\n",
      "6     4.2\n",
      "8     3.8\n",
      "7     2.1\n",
      "Name: weight, dtype: float64\n",
      "\n",
      "[Weighted share % | Test]\n",
      " segment\n",
      "0    25.1\n",
      "2    23.5\n",
      "3    15.8\n",
      "4     8.8\n",
      "9     6.8\n",
      "5     5.5\n",
      "1     4.4\n",
      "6     4.3\n",
      "8     3.8\n",
      "7     2.0\n",
      "Name: weight, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Numeric profile | weighted means]\n",
      "             age  weeks_worked_in_year  log_wage_per_hour  is_mover  full_year_worker  no_work\n",
      "segment                                                                                      \n",
      "0         9.498                 0.519              0.000     0.000             0.000    0.958\n",
      "1        43.815                44.629              0.000     0.001             0.716    0.029\n",
      "2        35.964                45.861              0.000     0.000             0.732    0.000\n",
      "3        59.791                 1.017              0.000     0.000             0.000    0.930\n",
      "4        50.082                32.671              0.004     0.048             0.547    0.300\n",
      "5        35.985                44.844              6.722     0.089             0.706    0.028\n",
      "6        43.511                45.051              0.000     0.061             0.743    0.045\n",
      "7        43.866                41.381              0.435     0.076             0.698    0.118\n",
      "8        48.341                39.306              0.450     0.066             0.673    0.183\n",
      "9        25.686                21.319              0.000     1.000             0.300    0.483\n",
      "\n",
      "[Categorical profile | top categories per segment]\n",
      "\n",
      "- sex (weighted %):\n",
      "sex      Female  Male\n",
      "segment              \n",
      "0          48.9  51.1\n",
      "1          64.7  35.3\n",
      "2          47.2  52.8\n",
      "3          68.2  31.8\n",
      "4          53.3  46.7\n",
      "5          51.9  48.1\n",
      "6          37.6  62.4\n",
      "7          27.7  72.3\n",
      "8          26.7  73.3\n",
      "9          52.1  47.9\n",
      "\n",
      "- education (weighted %):\n",
      "education  10th grade  7th and 8th grade  9th grade  Bachelors degree(BA AB BS)  Children  High school graduate  Some college but no degree\n",
      "segment                                                                                                                                    \n",
      "0                 3.9                NaN        4.1                         NaN      78.3                   NaN                         NaN\n",
      "1                 NaN                NaN        NaN                         9.3       NaN                  38.5                        23.1\n",
      "2                 NaN                NaN        NaN                        15.1       NaN                  34.0                        21.1\n",
      "3                 NaN               11.4        NaN                         NaN       NaN                  36.6                        12.9\n",
      "4                 NaN                NaN        NaN                        27.0       NaN                  25.9                        18.2\n",
      "5                 NaN                NaN        NaN                         9.5       NaN                  39.3                        24.5\n",
      "6                 NaN                NaN        NaN                        16.0       NaN                  35.7                        20.0\n",
      "7                 NaN                NaN        NaN                        21.5       NaN                  25.4                        17.9\n",
      "8                 NaN                NaN        NaN                        22.3       NaN                  27.0                        18.0\n",
      "9                 NaN                NaN        NaN                         NaN      27.3                  22.8                        15.0\n",
      "\n",
      "- class_of_worker (weighted %):\n",
      "class_of_worker  Federal government  Local government  Missing  Never worked  Private  Self-employed-incorporated  Self-employed-not incorporated  \\\n",
      "segment                                                                                                                                             \n",
      "0                               NaN               NaN     97.2           0.7      1.9                         NaN                             NaN   \n",
      "1                               NaN               8.9      5.9           NaN     76.9                         NaN                             NaN   \n",
      "2                               NaN               9.2      NaN           NaN     80.1                         NaN                             NaN   \n",
      "3                               NaN               0.2     96.3           NaN      3.0                         NaN                             NaN   \n",
      "4                               NaN               NaN     34.4           NaN     43.9                         NaN                             6.5   \n",
      "5                               NaN               6.9      NaN           NaN     86.7                         NaN                             NaN   \n",
      "6                               0.0               NaN      NaN           NaN      NaN                        22.9                            77.1   \n",
      "7                               NaN               NaN     16.5           NaN     58.1                         NaN                             7.5   \n",
      "8                               NaN               NaN     22.1           NaN     53.3                         NaN                             7.3   \n",
      "9                               NaN               2.9     50.8           NaN     42.5                         NaN                             NaN   \n",
      "\n",
      "class_of_worker  State government  \n",
      "segment                            \n",
      "0                             NaN  \n",
      "1                             NaN  \n",
      "2                             4.6  \n",
      "3                             NaN  \n",
      "4                             NaN  \n",
      "5                             3.2  \n",
      "6                             NaN  \n",
      "7                             NaN  \n",
      "8                             NaN  \n",
      "9                             NaN  \n",
      "\n",
      "- citizenship (weighted %):\n",
      "citizenship  Foreign born- Not a citizen of U S  Foreign born- U S citizen by naturalization  Native- Born abroad of American Parent(s)  \\\n",
      "segment                                                                                                                                   \n",
      "0                                           4.1                                          NaN                                        0.7   \n",
      "1                                           4.8                                          3.0                                        NaN   \n",
      "2                                           8.0                                          3.2                                        NaN   \n",
      "3                                           8.2                                          4.5                                        NaN   \n",
      "4                                           2.1                                          3.3                                        NaN   \n",
      "5                                           5.4                                          2.8                                        NaN   \n",
      "6                                           5.8                                          4.5                                        NaN   \n",
      "7                                           5.1                                          3.5                                        NaN   \n",
      "8                                           4.0                                          3.8                                        NaN   \n",
      "9                                          10.5                                          1.7                                        NaN   \n",
      "\n",
      "citizenship  Native- Born in the United States  \n",
      "segment                                         \n",
      "0                                         94.4  \n",
      "1                                         90.5  \n",
      "2                                         87.3  \n",
      "3                                         85.5  \n",
      "4                                         93.7  \n",
      "5                                         90.7  \n",
      "6                                         88.6  \n",
      "7                                         90.2  \n",
      "8                                         90.8  \n",
      "9                                         85.8  \n",
      "\n",
      "- region_of_previous_residence (weighted %):\n",
      "region_of_previous_residence  Abroad  Midwest  Missing  Northeast  South  West\n",
      "segment                                                                       \n",
      "0                                0.0      0.0    100.0        NaN    NaN   NaN\n",
      "1                                NaN      NaN     99.9        0.0    0.1   NaN\n",
      "2                                0.0      0.0    100.0        NaN    NaN   NaN\n",
      "3                                0.0      0.0    100.0        NaN    NaN   NaN\n",
      "4                                NaN      1.3     95.2        NaN    1.5   NaN\n",
      "5                                NaN      2.5     91.1        NaN    2.8   NaN\n",
      "6                                NaN      NaN     93.9        NaN    2.5   1.5\n",
      "7                                NaN      NaN     92.4        NaN    2.4   1.9\n",
      "8                                NaN      NaN     93.4        NaN    1.9   1.8\n",
      "9                                NaN     22.1      NaN        NaN   36.0  24.6\n",
      "\n",
      "- age_bucket (weighted %):\n",
      "age_bucket  18-24  25-34  35-44  45-54  55-64   65+   u18\n",
      "segment                                                  \n",
      "0             8.1    2.3    NaN    NaN    NaN   NaN  89.6\n",
      "1             NaN   20.3   33.4   24.8    NaN   NaN   NaN\n",
      "2            17.8   29.5   25.3    NaN    NaN   NaN   NaN\n",
      "3             NaN    NaN   12.6    NaN   16.2  48.8   NaN\n",
      "4             NaN    NaN   22.7   20.9    NaN  22.0   NaN\n",
      "5            19.8   26.9   24.3    NaN    NaN   NaN   NaN\n",
      "6             NaN   20.4   29.5   23.5    NaN   NaN   NaN\n",
      "7             NaN   20.7   25.8   20.5    NaN   NaN   NaN\n",
      "8             NaN    NaN   24.0   21.0    NaN  21.5   NaN\n",
      "9            19.5   23.8    NaN    NaN    NaN   NaN  31.6\n",
      "\n",
      "- weeks_bucket (weighted %):\n",
      "weeks_bucket     0  1-26  27-51    52\n",
      "segment                              \n",
      "0             95.8   3.9    0.3   NaN\n",
      "1              NaN  11.5   16.7  69.0\n",
      "2              NaN  11.7   17.7  70.6\n",
      "3             93.0   6.2    0.9   NaN\n",
      "4             30.0   NaN   10.3  52.7\n",
      "5              NaN  10.3   19.6  67.2\n",
      "6              NaN   8.5   18.9  68.1\n",
      "7             11.8   NaN   12.6  66.8\n",
      "8             18.3   NaN   10.9  65.0\n",
      "9             48.3   NaN   11.5  28.8\n",
      "\n",
      "- is_union (weighted %):\n",
      "is_union      0     1\n",
      "segment              \n",
      "0         100.0   0.0\n",
      "1          98.9   1.1\n",
      "2          98.7   1.3\n",
      "3         100.0   0.0\n",
      "4          98.9   1.1\n",
      "5          83.7  16.3\n",
      "6         100.0   0.0\n",
      "7          98.0   2.0\n",
      "8          96.6   3.4\n",
      "9          99.5   0.5\n",
      "\n",
      "- is_self_employed (weighted %):\n",
      "is_self_employed      0      1\n",
      "segment                       \n",
      "0                 100.0    0.0\n",
      "1                 100.0    0.0\n",
      "2                 100.0    0.0\n",
      "3                 100.0    0.0\n",
      "4                  89.2   10.8\n",
      "5                 100.0    0.0\n",
      "6                   0.0  100.0\n",
      "7                  87.5   12.5\n",
      "8                  87.5   12.5\n",
      "9                  99.8    0.2\n",
      "\n",
      "- has_any_capital (weighted %):\n",
      "has_any_capital      0      1\n",
      "segment                      \n",
      "0                100.0    0.0\n",
      "1                100.0    0.0\n",
      "2                100.0    0.0\n",
      "3                 99.9    0.1\n",
      "4                  0.0  100.0\n",
      "5                 90.2    9.8\n",
      "6                 98.2    1.8\n",
      "7                  0.0  100.0\n",
      "8                  0.0  100.0\n",
      "9                 99.8    0.2\n",
      "\n",
      "[Supervised lift | weighted positive rate by segment]\n",
      "TrainVal:\n",
      " segment\n",
      "0     0.0\n",
      "1     4.7\n",
      "2     7.0\n",
      "3     0.7\n",
      "4    21.4\n",
      "5     4.2\n",
      "6    11.4\n",
      "7    29.7\n",
      "8    32.8\n",
      "9     1.9\n",
      "Name: pos_rate, dtype: float64\n",
      "Test:\n",
      " segment\n",
      "0     0.0\n",
      "1     5.5\n",
      "2     7.1\n",
      "3     0.7\n",
      "4    20.3\n",
      "5     3.7\n",
      "6    11.3\n",
      "7    32.6\n",
      "8    34.4\n",
      "9     1.6\n",
      "Name: pos_rate, dtype: float64\n",
      "\n",
      "[Marketing summary | TrainVal]\n",
      "   share_%    age  weeks_worked_in_year  log_wage_per_hour  female_%  bachelor_%  private_worker_%  pos_rate_%\n",
      "0     25.4   9.50                  0.52               0.00     48.88        0.55              1.90        0.01\n",
      "2     23.4  35.96                 45.86               0.00     47.24       15.08             80.07        7.04\n",
      "3     16.0  59.79                  1.02               0.00     68.22        6.06              3.04        0.70\n",
      "4      8.6  50.08                 32.67               0.00     53.30       27.02             43.93       21.40\n",
      "9      6.8  25.69                 21.32               0.00     52.09        8.84             42.54        1.87\n",
      "5      5.6  35.99                 44.84               6.72     51.88        9.54             86.75        4.18\n",
      "1      4.3  43.81                 44.63               0.00     64.73        9.26             76.88        4.72\n",
      "6      4.2  43.51                 45.05               0.00     37.60       15.99              0.00       11.41\n",
      "8      3.8  48.34                 39.31               0.45     26.69       22.30             53.25       32.76\n",
      "7      2.1  43.87                 41.38               0.43     27.66       21.46             58.15       29.68\n"
     ]
    }
   ],
   "source": [
    "# === Print final KMeans segmentation results ===\n",
    "\n",
    "\n",
    "# Expect these to exist from your run:\n",
    "# - kmeans_final  (fitted on Z_trainval with best_k)\n",
    "# - Z_test, Z_trainval\n",
    "# - labels_trainval, labels_test  (we created them when picking K)\n",
    "# - prep_pipe  (FeatureBuilder + preprocess + SVD pipeline used for clustering)\n",
    "# - X_trainval, X_test, y_trainval, y_test (y_* optional for supervised lift)\n",
    "# - w_trainval, w_test  (survey weights)\n",
    "\n",
    "# 1) Weighted share by segment (TrainVal & Test)\n",
    "wshare_tv = 100.0 * w_trainval.groupby(labels_trainval).sum() / w_trainval.sum()\n",
    "wshare_te = 100.0 * w_test.groupby(labels_test).sum() / w_test.sum()\n",
    "print(\"\\n[Weighted share % | TrainVal]\\n\", wshare_tv.round(1).sort_values(ascending=False))\n",
    "print(\"\\n[Weighted share % | Test]\\n\", wshare_te.round(1).sort_values(ascending=False))\n",
    "\n",
    "# 2) Build a readable feature frame (post-FeatureBuilder) for profiling\n",
    "Xtv_feat = prep_pipe.named_steps[\"feat\"].transform(X_trainval.copy())  # DataFrame with engineered cols\n",
    "Xte_feat = prep_pipe.named_steps[\"feat\"].transform(X_test.copy())\n",
    "\n",
    "# ---- helpers (weighted means / weighted category table) ----\n",
    "def wmean_by_seg(s_values, seg, w):\n",
    "    num = (s_values * w).groupby(seg).sum()\n",
    "    den = w.groupby(seg).sum()\n",
    "    return (num / den).rename(s_values.name)\n",
    "\n",
    "def wprop_table(cat_series, seg, w):\n",
    "    dummies = pd.get_dummies(cat_series.astype(str), dummy_na=False)\n",
    "    wmat = dummies.mul(w.values.reshape(-1, 1))\n",
    "    grp = wmat.groupby(seg).sum()\n",
    "    grp = grp.div(grp.sum(axis=1), axis=0)\n",
    "    grp.columns.name = cat_series.name\n",
    "    return grp\n",
    "\n",
    "# 3) Numeric profile (weighted means)\n",
    "num_to_show = [c for c in [\n",
    "    \"age\",\"weeks_worked_in_year\",\"log_wage_per_hour\",\n",
    "    \"is_mover\",\"full_year_worker\",\"no_work\"\n",
    "] if c in Xtv_feat.columns]\n",
    "\n",
    "if num_to_show:\n",
    "    num_profiles = pd.concat(\n",
    "        [wmean_by_seg(Xtv_feat[c], labels_trainval, w_trainval) for c in num_to_show],\n",
    "        axis=1\n",
    "    ).round(3)\n",
    "    print(\"\\n[Numeric profile | weighted means]\\n\", num_profiles)\n",
    "else:\n",
    "    print(\"\\n[Numeric profile] No numeric columns found in Xtv_feat subset.\")\n",
    "\n",
    "# 4) Categorical profile (top-3 weighted % per segment)\n",
    "cat_to_show = [c for c in [\n",
    "    \"sex\",\"education\",\"class_of_worker\",\"citizenship\",\n",
    "    \"region_of_previous_residence\",\"age_bucket\",\"weeks_bucket\",\n",
    "    \"is_union\",\"is_self_employed\",\"has_any_capital\"\n",
    "] if c in Xtv_feat.columns]\n",
    "\n",
    "print(\"\\n[Categorical profile | top categories per segment]\")\n",
    "for c in cat_to_show:\n",
    "    tab = wprop_table(Xtv_feat[c], labels_trainval, w_trainval)  # segment x categories (proportions)\n",
    "    # For each segment (row), take top-3 categories\n",
    "    top3 = tab.apply(lambda r: r.sort_values(ascending=False).head(3), axis=1)\n",
    "    print(f\"\\n- {c} (weighted %):\")\n",
    "    print((top3 * 100).round(1))\n",
    "\n",
    "# 5) (Optional) Supervised lift by segment (weighted positive rate), if y_* available\n",
    "try:\n",
    "    lift_tv = wmean_by_seg(y_trainval.astype(int), labels_trainval, w_trainval).rename(\"pos_rate\")\n",
    "    lift_te = wmean_by_seg(y_test.astype(int), labels_test, w_test).rename(\"pos_rate\")\n",
    "    print(\"\\n[Supervised lift | weighted positive rate by segment]\")\n",
    "    print(\"TrainVal:\\n\", (100*lift_tv).round(1))\n",
    "    print(\"Test:\\n\", (100*lift_te).round(1))\n",
    "except Exception:\n",
    "    pass  # y_* not present; skip\n",
    "\n",
    "# 6) A compact marketing summary table (one line per segment)\n",
    "#    You can customize the fields below to match your storytelling.\n",
    "def safe_pick_prop(tab, cat, default=np.nan):\n",
    "    \"\"\"Pick weighted proportion of a category from a segment x category table.\"\"\"\n",
    "    return tab[cat] if (cat in tab.columns) else pd.Series(default, index=tab.index)\n",
    "\n",
    "summary = pd.DataFrame(index=sorted(labels_trainval.unique()))\n",
    "summary[\"share_%\"] = wshare_tv.reindex(summary.index).round(1)\n",
    "\n",
    "# add a few numeric KPIs\n",
    "for col in [\"age\",\"weeks_worked_in_year\",\"log_wage_per_hour\"]:\n",
    "    if col in Xtv_feat.columns:\n",
    "        summary[col] = wmean_by_seg(Xtv_feat[col], labels_trainval, w_trainval).reindex(summary.index)\n",
    "\n",
    "# add a few categorical KPIs (as %)\n",
    "if \"sex\" in Xtv_feat.columns:\n",
    "    sex_tab = wprop_table(Xtv_feat[\"sex\"], labels_trainval, w_trainval)\n",
    "    summary[\"female_%\"] = (100 * safe_pick_prop(sex_tab, \"Female\", 0)).reindex(summary.index)\n",
    "\n",
    "if \"education\" in Xtv_feat.columns:\n",
    "    edu_tab = wprop_table(Xtv_feat[\"education\"], labels_trainval, w_trainval)\n",
    "    summary[\"bachelor_%\"] = (100 * safe_pick_prop(edu_tab, \"Bachelors degree(BA AB BS)\", 0)).reindex(summary.index)\n",
    "\n",
    "if \"class_of_worker\" in Xtv_feat.columns:\n",
    "    cow_tab = wprop_table(Xtv_feat[\"class_of_worker\"], labels_trainval, w_trainval)\n",
    "    summary[\"private_worker_%\"] = (100 * safe_pick_prop(cow_tab, \"Private\", 0)).reindex(summary.index)\n",
    "\n",
    "# optional supervised rate (if y available)\n",
    "try:\n",
    "    summary[\"pos_rate_%\"] = (100 * lift_tv).reindex(summary.index)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"\\n[Marketing summary | TrainVal]\")\n",
    "print(summary.round(2).sort_values(\"share_%\", ascending=False))\n",
    "\n",
    "# 7) (Optional) Save segment assignment for downstream usage\n",
    "seg_trainval_df = pd.DataFrame({\n",
    "    \"segment\": labels_trainval,\n",
    "    \"weight\": w_trainval.values,\n",
    "}, index=X_trainval.index)\n",
    "\n",
    "# if y is present, include it for later lift/targeting analyses\n",
    "try:\n",
    "    seg_trainval_df[\"y\"] = y_trainval.values\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Example save (uncomment if you want a CSV)\n",
    "# seg_trainval_df.to_csv(\"segments_trainval.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b1cfa9",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8b917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HDBSCAN PPS] weighted_share (%)\n",
      " segment\n",
      "-1     25.0\n",
      " 0      2.0\n",
      " 1      3.4\n",
      " 2      1.1\n",
      " 3      4.1\n",
      " 4      6.9\n",
      " 5      3.5\n",
      " 6      1.2\n",
      " 7      2.5\n",
      " 8      2.6\n",
      " 9      4.8\n",
      " 10    16.4\n",
      " 11     9.7\n",
      " 12    10.2\n",
      " 13     1.5\n",
      " 14     1.9\n",
      " 15     1.4\n",
      " 16     1.8\n",
      "Name: weight, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HDBSCAN] Numeric profile (weighted means)\n",
      "             age  weeks_worked_in_year  log_wage_per_hour  is_mover  full_year_worker  no_work\n",
      "segment                                                                                      \n",
      "-1       39.957                22.009              0.239     0.160             0.210    0.371\n",
      " 0       43.856                41.237              0.341     0.075             0.696    0.120\n",
      " 1       48.546                38.486              0.277     0.066             0.658    0.198\n",
      " 2       40.926                49.008              4.871     0.015             0.833    0.007\n",
      " 3       34.523                44.425              6.629     0.063             0.692    0.028\n",
      " 4       50.862                30.385              0.000     0.000             0.518    0.354\n",
      " 5       43.493                47.654              0.000     0.000             0.801    0.000\n",
      " 6       29.984                51.505              0.000     1.000             0.974    0.000\n",
      " 7       11.211                 0.065              0.000     1.000             0.000    0.991\n",
      " 8       43.904                51.953              0.000     0.000             1.000    0.000\n",
      " 9       68.244                 0.120              0.000     0.000             0.000    0.988\n",
      " 10      35.939                49.679              0.000     0.000             0.922    0.000\n",
      " 11       6.924                 0.000              0.000     0.000             0.000    1.000\n",
      " 12       6.935                 0.000              0.000     0.000             0.000    1.000\n",
      " 13      17.843                 0.000              0.000     0.000             0.000    1.000\n",
      " 14      71.910                 0.000              0.000     0.000             0.000    1.000\n",
      " 15      18.047                 0.000              0.000     0.000             0.000    1.000\n",
      " 16      68.670                 0.000              0.000     0.000             0.000    1.000\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Build embeddings (dense) ---\n",
    "Z_trainval = prep_pipe.fit_transform(X_trainval)\n",
    "Z_test     = prep_pipe.transform(X_test)\n",
    "\n",
    "# --- 2) Weighted PPS subsample for fitting HDBSCAN ---\n",
    "import hdbscan\n",
    "rng = np.random.default_rng(42)\n",
    "n_sub = min(len(Z_trainval), 60000)  # cap for speed\n",
    "p = (w_trainval.values / w_trainval.values.sum())\n",
    "idx_sub = rng.choice(len(Z_trainval), size=n_sub, replace=False, p=p)\n",
    "Z_sub = Z_trainval[idx_sub]\n",
    "\n",
    "# Set min_cluster_size relative to the subsample size you actually fit on\n",
    "min_cluster_size = max(200, int(0.01 * n_sub))\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    min_samples=None,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    approx_min_span_tree=True,\n",
    "    prediction_data=True   # must be True for approximate_predict\n",
    ")\n",
    "\n",
    "# Fit on the subsample (no sample_weight accepted by HDBSCAN.fit in stable releases)\n",
    "clusterer.fit(Z_sub)\n",
    "\n",
    "# --- 3) Assign labels for the FULL TrainVal via approximate_predict ---\n",
    "# Do NOT use clusterer.labels_ here. It only has length n_sub.\n",
    "tr_labels, tr_strength = hdbscan.approximate_predict(clusterer, Z_trainval)\n",
    "labels_trainval = pd.Series(tr_labels, index=X_trainval.index, name=\"segment\")\n",
    "\n",
    "# Assign for Test as well\n",
    "te_labels, te_strength = hdbscan.approximate_predict(clusterer, Z_test)\n",
    "labels_test = pd.Series(te_labels, index=X_test.index, name=\"segment\")\n",
    "\n",
    "# --- 4) Weighted summaries (use survey weights only in profiling/evaluation) ---\n",
    "# weighted share (including noise = -1)\n",
    "wshare = 100.0 * w_trainval.groupby(labels_trainval).sum() / w_trainval.sum()\n",
    "print(\"\\n[HDBSCAN PPS] weighted_share (%)\\n\", wshare.round(1))\n",
    "\n",
    "# If you want profiles:\n",
    "Xtv_feat = prep_pipe.named_steps[\"feat\"].transform(X_trainval.copy())\n",
    "\n",
    "def wmean_by_seg(s_values, seg, w):\n",
    "    num = (s_values * w).groupby(seg).sum()\n",
    "    den = w.groupby(seg).sum()\n",
    "    return (num / den).rename(s_values.name)\n",
    "\n",
    "def wprop_table(cat_series, seg, w):\n",
    "    dummies = pd.get_dummies(cat_series.astype(str), dummy_na=False)\n",
    "    wmat = dummies.mul(w.values.reshape(-1, 1))\n",
    "    \n",
    "    grp = wmat.groupby(seg).sum()\n",
    "    grp = grp.div(grp.sum(axis=1), axis=0)\n",
    "    grp.columns.name = cat_series.name\n",
    "    return grp\n",
    "\n",
    "num_to_show = [c for c in [\"age\",\"weeks_worked_in_year\",\"log_wage_per_hour\",\"is_mover\",\n",
    "                           \"full_year_worker\",\"no_work\"] if c in Xtv_feat.columns]\n",
    "num_prof = pd.concat([wmean_by_seg(Xtv_feat[c], labels_trainval, w_trainval) for c in num_to_show], axis=1)\n",
    "print(\"\\n[HDBSCAN] Numeric profile (weighted means)\\n\", num_prof.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11d7d58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HDBSCAN PPS] weighted_share (%)\n",
      " segment\n",
      "-1     25.0\n",
      " 0      2.0\n",
      " 1      3.4\n",
      " 2      1.1\n",
      " 3      4.1\n",
      " 4      6.9\n",
      " 5      3.5\n",
      " 6      1.2\n",
      " 7      2.5\n",
      " 8      2.6\n",
      " 9      4.8\n",
      " 10    16.4\n",
      " 11     9.7\n",
      " 12    10.2\n",
      " 13     1.5\n",
      " 14     1.9\n",
      " 15     1.4\n",
      " 16     1.8\n",
      "Name: weight, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HDBSCAN] Numeric profile (weighted means)\n",
      "             age  weeks_worked_in_year  log_wage_per_hour  is_mover  full_year_worker  no_work\n",
      "segment                                                                                      \n",
      "-1       39.957                22.009              0.239     0.160             0.210    0.371\n",
      " 0       43.856                41.237              0.341     0.075             0.696    0.120\n",
      " 1       48.546                38.486              0.277     0.066             0.658    0.198\n",
      " 2       40.926                49.008              4.871     0.015             0.833    0.007\n",
      " 3       34.523                44.425              6.629     0.063             0.692    0.028\n",
      " 4       50.862                30.385              0.000     0.000             0.518    0.354\n",
      " 5       43.493                47.654              0.000     0.000             0.801    0.000\n",
      " 6       29.984                51.505              0.000     1.000             0.974    0.000\n",
      " 7       11.211                 0.065              0.000     1.000             0.000    0.991\n",
      " 8       43.904                51.953              0.000     0.000             1.000    0.000\n",
      " 9       68.244                 0.120              0.000     0.000             0.000    0.988\n",
      " 10      35.939                49.679              0.000     0.000             0.922    0.000\n",
      " 11       6.924                 0.000              0.000     0.000             0.000    1.000\n",
      " 12       6.935                 0.000              0.000     0.000             0.000    1.000\n",
      " 13      17.843                 0.000              0.000     0.000             0.000    1.000\n",
      " 14      71.910                 0.000              0.000     0.000             0.000    1.000\n",
      " 15      18.047                 0.000              0.000     0.000             0.000    1.000\n",
      " 16      68.670                 0.000              0.000     0.000             0.000    1.000\n",
      "\n",
      "[HDBSCAN] weighted_share TEST (%)\n",
      " segment\n",
      "-1     25.2\n",
      " 0      2.0\n",
      " 1      3.4\n",
      " 2      1.1\n",
      " 3      4.1\n",
      " 4      7.1\n",
      " 5      3.6\n",
      " 6      1.2\n",
      " 7      2.5\n",
      " 8      2.6\n",
      " 9      4.8\n",
      " 10    16.3\n",
      " 11     9.5\n",
      " 12    10.2\n",
      " 13     1.3\n",
      " 14     1.8\n",
      " 15     1.5\n",
      " 16     1.8\n",
      "Name: weight, dtype: float64\n",
      "[HDBSCAN] silhouette_test: 0.2702465341269679\n",
      "[HDBSCAN] pos_rate by segment (TEST, weighted %):\n",
      " segment\n",
      "-1      4.7\n",
      " 0     32.2\n",
      " 1     30.6\n",
      " 2      7.3\n",
      " 3      2.4\n",
      " 4     19.2\n",
      " 5     12.2\n",
      " 6      5.5\n",
      " 7      0.0\n",
      " 8      7.8\n",
      " 9      0.9\n",
      " 10     8.8\n",
      " 11     0.0\n",
      " 12     0.0\n",
      " 13     0.0\n",
      " 14     1.3\n",
      " 15     0.0\n",
      " 16     0.1\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Build embeddings (dense) ---\n",
    "Z_trainval = prep_pipe.fit_transform(X_trainval)\n",
    "Z_test     = prep_pipe.transform(X_test)\n",
    "\n",
    "# --- 2) Weighted PPS subsample for fitting HDBSCAN ---\n",
    "import hdbscan\n",
    "rng = np.random.default_rng(42)\n",
    "n_sub = min(len(Z_trainval), 60000)  # cap for speed\n",
    "p = (w_trainval.values / w_trainval.values.sum())\n",
    "idx_sub = rng.choice(len(Z_trainval), size=n_sub, replace=False, p=p)\n",
    "Z_sub = Z_trainval[idx_sub]\n",
    "\n",
    "# Set min_cluster_size relative to the subsample size you actually fit on\n",
    "min_cluster_size = max(200, int(0.01 * n_sub))\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    min_samples=None,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    approx_min_span_tree=True,\n",
    "    prediction_data=True   # must be True for approximate_predict\n",
    ")\n",
    "\n",
    "# Fit on the subsample (no sample_weight accepted by HDBSCAN.fit in stable releases)\n",
    "clusterer.fit(Z_sub)\n",
    "\n",
    "# --- 3) Assign labels for the FULL TrainVal via approximate_predict ---\n",
    "# Do NOT use clusterer.labels_ here. It only has length n_sub.\n",
    "tr_labels, tr_strength = hdbscan.approximate_predict(clusterer, Z_trainval)\n",
    "labels_trainval = pd.Series(tr_labels, index=X_trainval.index, name=\"segment\")\n",
    "\n",
    "# Assign for Test as well\n",
    "te_labels, te_strength = hdbscan.approximate_predict(clusterer, Z_test)\n",
    "labels_test = pd.Series(te_labels, index=X_test.index, name=\"segment\")\n",
    "\n",
    "# --- 4) Weighted summaries (use survey weights only in profiling/evaluation) ---\n",
    "# weighted share (including noise = -1)\n",
    "wshare = 100.0 * w_trainval.groupby(labels_trainval).sum() / w_trainval.sum()\n",
    "print(\"\\n[HDBSCAN PPS] weighted_share (%)\\n\", wshare.round(1))\n",
    "\n",
    "# If you want profiles:\n",
    "Xtv_feat = prep_pipe.named_steps[\"feat\"].transform(X_trainval.copy())\n",
    "\n",
    "def wmean_by_seg(s_values, seg, w):\n",
    "    num = (s_values * w).groupby(seg).sum()\n",
    "    den = w.groupby(seg).sum()\n",
    "    return (num / den).rename(s_values.name)\n",
    "\n",
    "def wprop_table(cat_series, seg, w):\n",
    "    dummies = pd.get_dummies(cat_series.astype(str), dummy_na=False)\n",
    "    wmat = dummies.mul(w.values.reshape(-1, 1))\n",
    "    \n",
    "    grp = wmat.groupby(seg).sum()\n",
    "    grp = grp.div(grp.sum(axis=1), axis=0)\n",
    "    grp.columns.name = cat_series.name\n",
    "    return grp\n",
    "\n",
    "num_to_show = [c for c in [\"age\",\"weeks_worked_in_year\",\"log_wage_per_hour\",\"is_mover\",\n",
    "                           \"full_year_worker\",\"no_work\"] if c in Xtv_feat.columns]\n",
    "num_prof = pd.concat([wmean_by_seg(Xtv_feat[c], labels_trainval, w_trainval) for c in num_to_show], axis=1)\n",
    "print(\"\\n[HDBSCAN] Numeric profile (weighted means)\\n\", num_prof.round(3))\n",
    "# After: clusterer.fit(...) and Z_test = prep_pipe.transform(X_test)\n",
    "\n",
    "\n",
    "# assign test labels\n",
    "te_labels, te_strength = hdbscan.approximate_predict(clusterer, Z_test)\n",
    "labels_test = pd.Series(te_labels, index=X_test.index, name=\"segment\")\n",
    "\n",
    "# weighted share on TEST (includes noise = -1)\n",
    "wshare_test = 100.0 * w_test.groupby(labels_test).sum() / w_test.sum()\n",
    "print(\"\\n[HDBSCAN] weighted_share TEST (%)\\n\", wshare_test.round(1))\n",
    "\n",
    "# optional: test silhouette (internal quality; unweighted)\n",
    "from sklearn.metrics import silhouette_score\n",
    "print(\"[HDBSCAN] silhouette_test:\", silhouette_score(Z_test, labels_test))\n",
    "\n",
    "# optional: supervised lift by segment on TEST (if y_test exists)\n",
    "lift_te = ((w_test * y_test).groupby(labels_test).sum() /\n",
    "           w_test.groupby(labels_test).sum())\n",
    "print(\"[HDBSCAN] pos_rate by segment (TEST, weighted %):\\n\", (100*lift_te).round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "391ea64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HDBSCAN] weighted_share TEST (%)\n",
      " segment\n",
      "-1     25.2\n",
      " 0      2.0\n",
      " 1      3.4\n",
      " 2      1.1\n",
      " 3      4.1\n",
      " 4      7.1\n",
      " 5      3.6\n",
      " 6      1.2\n",
      " 7      2.5\n",
      " 8      2.6\n",
      " 9      4.8\n",
      " 10    16.3\n",
      " 11     9.5\n",
      " 12    10.2\n",
      " 13     1.3\n",
      " 14     1.8\n",
      " 15     1.5\n",
      " 16     1.8\n",
      "Name: weight, dtype: float64\n",
      "\n",
      "[HDBSCAN] avg membership strength by segment (TEST)\n",
      " segment\n",
      "-1     0.000\n",
      " 0     0.916\n",
      " 1     0.849\n",
      " 2     0.999\n",
      " 3     0.862\n",
      " 4     0.966\n",
      " 5     0.888\n",
      " 6     0.997\n",
      " 7     0.891\n",
      " 8     0.963\n",
      " 9     0.589\n",
      " 10    0.823\n",
      " 11    0.958\n",
      " 12    0.960\n",
      " 13    0.825\n",
      " 14    0.933\n",
      " 15    0.835\n",
      " 16    0.992\n",
      "Name: strength, dtype: float64\n",
      "\n",
      "[HDBSCAN] Numeric profile (weighted means) — TEST\n",
      "             age  weeks_worked_in_year  log_wage_per_hour  is_mover  full_year_worker  no_work\n",
      "segment                                                                                      \n",
      "-1       39.770                22.012              0.230     0.157             0.209    0.372\n",
      " 0       43.456                42.037              0.427     0.075             0.705    0.123\n",
      " 1       48.767                38.999              0.281     0.046             0.664    0.182\n",
      " 2       40.969                49.124              4.828     0.020             0.825    0.001\n",
      " 3       34.717                44.337              6.669     0.076             0.713    0.028\n",
      " 4       50.434                31.045              0.000     0.000             0.533    0.340\n",
      " 5       43.705                47.838              0.000     0.000             0.797    0.000\n",
      " 6       29.972                51.483              0.000     1.000             0.976    0.000\n",
      " 7       10.830                 0.059              0.000     1.000             0.000    0.993\n",
      " 8       43.966                51.950              0.000     0.000             1.000    0.000\n",
      " 9       68.581                 0.061              0.000     0.000             0.000    0.992\n",
      " 10      36.061                49.487              0.000     0.000             0.915    0.000\n",
      " 11       7.121                 0.000              0.000     0.000             0.000    1.000\n",
      " 12       6.816                 0.000              0.000     0.000             0.000    1.000\n",
      " 13      17.900                 0.000              0.000     0.000             0.000    1.000\n",
      " 14      72.596                 0.000              0.000     0.000             0.000    1.000\n",
      " 15      17.965                 0.000              0.000     0.000             0.000    1.000\n",
      " 16      68.713                 0.000              0.000     0.000             0.000    1.000\n",
      "\n",
      "[HDBSCAN] Categorical profile (top-3 per segment) — TEST\n",
      "\n",
      "- sex (weighted %):\n",
      "sex      Female   Male\n",
      "segment               \n",
      "-1         58.8   41.2\n",
      " 0         27.7   72.3\n",
      " 1         31.2   68.8\n",
      " 2         41.0   59.0\n",
      " 3         54.6   45.4\n",
      " 4         52.9   47.1\n",
      " 5         36.9   63.1\n",
      " 6         37.8   62.2\n",
      " 7         49.8   50.2\n",
      " 8         64.5   35.5\n",
      " 9         77.6   22.4\n",
      " 10        42.8   57.2\n",
      " 11       100.0    0.0\n",
      " 12         0.0  100.0\n",
      " 13         0.0  100.0\n",
      " 14         0.0  100.0\n",
      " 15       100.0    0.0\n",
      " 16       100.0    0.0\n",
      "\n",
      "- education (weighted %):\n",
      "education  10th grade  7th and 8th grade  9th grade  Bachelors degree(BA AB BS)  Children  High school graduate  Some college but no degree\n",
      "segment                                                                                                                                    \n",
      "-1                NaN                NaN        NaN                        11.9       NaN                  37.3                        21.8\n",
      " 0                NaN                NaN        NaN                        23.6       NaN                  25.0                        19.5\n",
      " 1                NaN                NaN        NaN                        22.2       NaN                  27.5                        17.6\n",
      " 2                NaN                NaN        NaN                        12.1       NaN                  40.9                        19.9\n",
      " 3                NaN                NaN        NaN                         9.0       NaN                  38.5                        23.0\n",
      " 4                NaN                NaN        NaN                        27.6       NaN                  24.6                        18.7\n",
      " 5                NaN                NaN        NaN                        15.7       NaN                  33.3                        20.4\n",
      " 6                NaN                NaN        NaN                        17.1       NaN                  30.4                        16.8\n",
      " 7                NaN                NaN        4.4                         NaN      76.2                   4.4                         NaN\n",
      " 8                NaN                NaN        NaN                         9.1       NaN                  35.4                        25.6\n",
      " 9                NaN               17.3        NaN                         NaN       NaN                  32.1                        10.0\n",
      " 10               NaN                NaN        NaN                        15.7       NaN                  30.9                        20.8\n",
      " 11               0.0                NaN        NaN                         0.0     100.0                   NaN                         NaN\n",
      " 12               0.0                NaN        NaN                         0.0     100.0                   NaN                         NaN\n",
      " 13              26.4               20.1       27.9                         NaN       NaN                   NaN                         NaN\n",
      " 14               NaN               19.6        NaN                         NaN       NaN                  19.0                        10.0\n",
      " 15              23.4               20.4       27.8                         NaN       NaN                   NaN                         NaN\n",
      " 16              14.8               14.7        NaN                         NaN       NaN                  37.7                         NaN\n",
      "\n",
      "- class_of_worker (weighted %):\n",
      "class_of_worker  Federal government  Local government  Missing  Never worked  Private  Self-employed-incorporated  Self-employed-not incorporated  \\\n",
      "segment                                                                                                                                             \n",
      "-1                              NaN               5.6     42.6           NaN     40.5                         NaN                             NaN   \n",
      " 0                              NaN               NaN     16.9           NaN     58.9                         NaN                             7.7   \n",
      " 1                              NaN               6.8     22.8           NaN     50.7                         NaN                             NaN   \n",
      " 2                              NaN              27.3      NaN           NaN     56.7                         NaN                             NaN   \n",
      " 3                              NaN               5.5      NaN           NaN     90.9                         NaN                             NaN   \n",
      " 4                              NaN               5.6     38.7           NaN     50.2                         NaN                             NaN   \n",
      " 5                              0.0               NaN      NaN           NaN      NaN                        25.5                            74.5   \n",
      " 6                              NaN               4.2      NaN           NaN     90.4                         NaN                             NaN   \n",
      " 7                              NaN               NaN     99.6           0.4      0.0                         NaN                             NaN   \n",
      " 8                              NaN               7.4      NaN           NaN     82.8                         NaN                             NaN   \n",
      " 9                              NaN               NaN     99.8           0.1      0.1                         NaN                             NaN   \n",
      " 10                             NaN               7.2      NaN           NaN     83.8                         NaN                             NaN   \n",
      " 11                             0.0               0.0    100.0           NaN      NaN                         NaN                             NaN   \n",
      " 12                             0.0               0.0    100.0           NaN      NaN                         NaN                             NaN   \n",
      " 13                             0.0               NaN     96.4           3.6      NaN                         NaN                             NaN   \n",
      " 14                             0.0               0.0    100.0           NaN      NaN                         NaN                             NaN   \n",
      " 15                             0.0               NaN     94.7           5.3      NaN                         NaN                             NaN   \n",
      " 16                             0.0               0.0    100.0           NaN      NaN                         NaN                             NaN   \n",
      "\n",
      "class_of_worker  State government  \n",
      "segment                            \n",
      "-1                            NaN  \n",
      " 0                            NaN  \n",
      " 1                            NaN  \n",
      " 2                           10.7  \n",
      " 3                            2.1  \n",
      " 4                            NaN  \n",
      " 5                            NaN  \n",
      " 6                            3.0  \n",
      " 7                            NaN  \n",
      " 8                            5.3  \n",
      " 9                            NaN  \n",
      " 10                           4.3  \n",
      " 11                           NaN  \n",
      " 12                           NaN  \n",
      " 13                           NaN  \n",
      " 14                           NaN  \n",
      " 15                           NaN  \n",
      " 16                           NaN  \n",
      "\n",
      "- citizenship (weighted %):\n",
      "citizenship  Foreign born- Not a citizen of U S  Foreign born- U S citizen by naturalization  Native- Born abroad of American Parent(s)  \\\n",
      "segment                                                                                                                                   \n",
      "-1                                         11.9                                          3.6                                        NaN   \n",
      " 0                                          4.5                                          3.2                                        NaN   \n",
      " 1                                          3.8                                          4.3                                        NaN   \n",
      " 2                                          3.0                                          4.3                                        NaN   \n",
      " 3                                          5.0                                          2.6                                        NaN   \n",
      " 4                                          1.9                                          3.8                                        NaN   \n",
      " 5                                          5.5                                          3.7                                        NaN   \n",
      " 6                                         10.0                                          2.8                                        NaN   \n",
      " 7                                          7.4                                          NaN                                        1.1   \n",
      " 8                                          3.2                                          2.5                                        NaN   \n",
      " 9                                          5.0                                          5.2                                        NaN   \n",
      " 10                                         6.4                                          3.1                                        NaN   \n",
      " 11                                         3.2                                          NaN                                        0.7   \n",
      " 12                                         2.9                                          NaN                                        0.6   \n",
      " 13                                         4.2                                          1.5                                        NaN   \n",
      " 14                                         NaN                                          3.7                                        0.5   \n",
      " 15                                         0.6                                          NaN                                        0.6   \n",
      " 16                                         NaN                                          0.6                                        0.3   \n",
      "\n",
      "citizenship  Native- Born in the United States  \n",
      "segment                                         \n",
      "-1                                        82.7  \n",
      " 0                                        91.1  \n",
      " 1                                        90.4  \n",
      " 2                                        91.4  \n",
      " 3                                        91.0  \n",
      " 4                                        93.1  \n",
      " 5                                        90.0  \n",
      " 6                                        84.8  \n",
      " 7                                        90.8  \n",
      " 8                                        91.8  \n",
      " 9                                        88.4  \n",
      " 10                                       89.1  \n",
      " 11                                       95.7  \n",
      " 12                                       95.8  \n",
      " 13                                       93.2  \n",
      " 14                                       95.6  \n",
      " 15                                       98.3  \n",
      " 16                                       98.9  \n",
      "\n",
      "- region_of_previous_residence (weighted %):\n",
      "region_of_previous_residence  Abroad  Midwest  Missing  Northeast  South  West\n",
      "segment                                                                       \n",
      "-1                               NaN      NaN     84.3        NaN    5.5   4.1\n",
      " 0                               NaN      NaN     92.5        NaN    2.8   2.0\n",
      " 1                               NaN      NaN     95.4        1.2    1.6   NaN\n",
      " 2                               NaN      0.7     98.0        NaN    NaN   1.0\n",
      " 3                               NaN      1.7     92.4        NaN    3.2   NaN\n",
      " 4                               0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 5                               0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 6                               NaN     28.3      NaN        NaN   31.2  26.4\n",
      " 7                               NaN     20.8      NaN        NaN   35.8  27.8\n",
      " 8                               0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 9                               0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 10                              0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 11                              0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 12                              0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 13                              0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 14                              0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 15                              0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 16                              0.0      0.0    100.0        NaN    NaN   NaN\n",
      "\n",
      "- age_bucket (weighted %):\n",
      "age_bucket  18-24  25-34  35-44  45-54  55-64   65+    u18\n",
      "segment                                                   \n",
      "-1           17.7   22.1   20.1    NaN    NaN   NaN    NaN\n",
      " 0            NaN   23.8   24.0   20.7    NaN   NaN    NaN\n",
      " 1            NaN    NaN   25.0   19.5    NaN  22.7    NaN\n",
      " 2            NaN   22.4   32.6   27.8    NaN   NaN    NaN\n",
      " 3           21.8   29.7   22.4    NaN    NaN   NaN    NaN\n",
      " 4            NaN    NaN   22.9   18.5    NaN  23.4    NaN\n",
      " 5            NaN   18.6   30.2   25.6    NaN   NaN    NaN\n",
      " 6           23.2   57.8   13.4    NaN    NaN   NaN    NaN\n",
      " 7            7.3    5.0    NaN    NaN    NaN   NaN   84.1\n",
      " 8            NaN   17.1   38.0   25.9    NaN   NaN    NaN\n",
      " 9            NaN    NaN    NaN    6.8   12.2  69.9    NaN\n",
      " 10           NaN   30.6   27.9   17.6    NaN   NaN    NaN\n",
      " 11           0.0    0.0    NaN    NaN    NaN   NaN  100.0\n",
      " 12           0.0    0.0    NaN    NaN    NaN   NaN  100.0\n",
      " 13          12.0    5.6    NaN    NaN    NaN   NaN   78.4\n",
      " 14           0.0    NaN    NaN    NaN   11.3  88.7    NaN\n",
      " 15          15.5    7.5    NaN    NaN    NaN   NaN   74.7\n",
      " 16           NaN    NaN    NaN    6.2   16.0  77.8    NaN\n",
      "\n",
      "- weeks_bucket (weighted %):\n",
      "weeks_bucket      0  1-26  27-51    52\n",
      "segment                               \n",
      "-1             37.2  21.8   21.8   NaN\n",
      " 0             12.3   NaN   13.9  66.5\n",
      " 1             18.2   NaN   10.0  64.5\n",
      " 2              NaN   4.0   16.0  79.8\n",
      " 3              NaN  12.6   17.6  67.1\n",
      " 4             34.0   NaN    7.9  51.4\n",
      " 5              NaN   7.3   19.4  73.3\n",
      " 6              NaN   0.9    5.6  93.6\n",
      " 7             99.3   0.7    0.0   NaN\n",
      " 8              0.0   NaN    2.9  97.1\n",
      " 9             99.2   0.8    0.0   NaN\n",
      " 10             NaN   5.5    6.0  88.6\n",
      " 11           100.0   0.0    0.0   NaN\n",
      " 12           100.0   0.0    0.0   NaN\n",
      " 13           100.0   0.0    0.0   NaN\n",
      " 14           100.0   0.0    0.0   NaN\n",
      " 15           100.0   0.0    0.0   NaN\n",
      " 16           100.0   0.0    0.0   NaN\n",
      "\n",
      "- is_union (weighted %):\n",
      "is_union      0      1\n",
      "segment               \n",
      "-1         98.3    1.7\n",
      " 0         99.5    0.5\n",
      " 1        100.0    0.0\n",
      " 2          0.0  100.0\n",
      " 3        100.0    0.0\n",
      " 4        100.0    0.0\n",
      " 5        100.0    0.0\n",
      " 6        100.0    0.0\n",
      " 7        100.0    0.0\n",
      " 8        100.0    0.0\n",
      " 9        100.0    0.0\n",
      " 10       100.0    0.0\n",
      " 11       100.0    0.0\n",
      " 12       100.0    0.0\n",
      " 13       100.0    0.0\n",
      " 14       100.0    0.0\n",
      " 15       100.0    0.0\n",
      " 16       100.0    0.0\n",
      "\n",
      "- is_self_employed (weighted %):\n",
      "is_self_employed      0      1\n",
      "segment                       \n",
      "-1                 92.9    7.1\n",
      " 0                 86.7   13.3\n",
      " 1                 87.8   12.2\n",
      " 2                100.0    0.0\n",
      " 3                100.0    0.0\n",
      " 4                100.0    0.0\n",
      " 5                  0.0  100.0\n",
      " 6                100.0    0.0\n",
      " 7                100.0    0.0\n",
      " 8                100.0    0.0\n",
      " 9                100.0    0.0\n",
      " 10               100.0    0.0\n",
      " 11               100.0    0.0\n",
      " 12               100.0    0.0\n",
      " 13               100.0    0.0\n",
      " 14               100.0    0.0\n",
      " 15               100.0    0.0\n",
      " 16               100.0    0.0\n",
      "\n",
      "- has_any_capital (weighted %):\n",
      "has_any_capital      0      1\n",
      "segment                      \n",
      "-1                89.2   10.8\n",
      " 0                 0.0  100.0\n",
      " 1                 0.0  100.0\n",
      " 2                99.4    0.6\n",
      " 3                99.4    0.6\n",
      " 4                 0.0  100.0\n",
      " 5               100.0    0.0\n",
      " 6               100.0    0.0\n",
      " 7               100.0    0.0\n",
      " 8               100.0    0.0\n",
      " 9               100.0    0.0\n",
      " 10              100.0    0.0\n",
      " 11              100.0    0.0\n",
      " 12              100.0    0.0\n",
      " 13              100.0    0.0\n",
      " 14              100.0    0.0\n",
      " 15              100.0    0.0\n",
      " 16              100.0    0.0\n",
      "\n",
      "[HDBSCAN] Segment performance on TEST (weighted)\n",
      "          weighted_share_%  pos_rate_%  lift  avg_strength\n",
      "segment                                                  \n",
      " 0                    2.0        32.2  4.89         0.916\n",
      " 1                    3.4        30.6  4.64         0.849\n",
      " 4                    7.1        19.2  2.91         0.966\n",
      " 5                    3.6        12.2  1.85         0.888\n",
      " 10                  16.3         8.8  1.33         0.823\n",
      " 8                    2.6         7.8  1.19         0.963\n",
      " 2                    1.1         7.3  1.10         0.999\n",
      " 6                    1.2         5.5  0.84         0.997\n",
      "-1                   25.2         4.7  0.71         0.000\n",
      " 3                    4.1         2.4  0.36         0.862\n",
      " 14                   1.8         1.3  0.20         0.933\n",
      " 9                    4.8         0.9  0.13         0.589\n",
      " 16                   1.8         0.1  0.02         0.992\n",
      " 7                    2.5         0.0  0.00         0.891\n",
      " 11                   9.5         0.0  0.00         0.958\n",
      " 12                  10.2         0.0  0.00         0.960\n",
      " 13                   1.3         0.0  0.00         0.825\n",
      " 15                   1.5         0.0  0.00         0.835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# --- 0) Ensure we have engineered features for TEST (post-FeatureBuilder) ---\n",
    "# If you already have Xte_feat from earlier, you can skip this block.\n",
    "Xte_feat = prep_pipe.named_steps[\"feat\"].transform(X_test.copy())\n",
    "\n",
    "# --- 1) Helpers (weighted mean & weighted categorical % by segment) ---\n",
    "def wmean_by_seg(values, seg, w):\n",
    "    \"\"\"Weighted mean of a numeric Series grouped by segment.\"\"\"\n",
    "    num = (values * w).groupby(seg).sum()\n",
    "    den = w.groupby(seg).sum()\n",
    "    return (num / den).rename(values.name)\n",
    "\n",
    "def wprop_table(cat_series, seg, w):\n",
    "    \"\"\"Weighted proportion table (segment x category).\"\"\"\n",
    "    dummies = pd.get_dummies(cat_series.astype(str), dummy_na=False)\n",
    "    wmat = dummies.mul(w.values.reshape(-1, 1))\n",
    "    grp = wmat.groupby(seg).sum()\n",
    "    grp = grp.div(grp.sum(axis=1), axis=0)\n",
    "    grp.columns.name = cat_series.name\n",
    "    return grp\n",
    "\n",
    "# --- 2) Basic segment sizes and strength on TEST ---\n",
    "# (labels_test and te_strength come from hdbscan.approximate_predict)\n",
    "seg_te = labels_test.reindex(X_test.index)\n",
    "w_te = w_test.reindex(X_test.index)\n",
    "\n",
    "weighted_share_te = 100.0 * w_te.groupby(seg_te).sum() / w_te.sum()\n",
    "print(\"\\n[HDBSCAN] weighted_share TEST (%)\\n\", weighted_share_te.round(1))\n",
    "\n",
    "# Optional: average membership strength per segment (higher ~ more confident)\n",
    "strength_te = pd.Series(te_strength, index=X_test.index, name=\"strength\")\n",
    "avg_strength = wmean_by_seg(strength_te, seg_te, w_te)\n",
    "print(\"\\n[HDBSCAN] avg membership strength by segment (TEST)\\n\", avg_strength.round(3))\n",
    "\n",
    "# --- 3) Numeric profile (weighted means) on TEST ---\n",
    "num_to_show = [\n",
    "    c for c in [\"age\",\"weeks_worked_in_year\",\"log_wage_per_hour\",\"is_mover\",\n",
    "                \"full_year_worker\",\"no_work\"] if c in Xte_feat.columns\n",
    "]\n",
    "num_profiles_test = pd.concat([wmean_by_seg(Xte_feat[c], seg_te, w_te) for c in num_to_show], axis=1)\n",
    "print(\"\\n[HDBSCAN] Numeric profile (weighted means) — TEST\\n\", num_profiles_test.round(3))\n",
    "\n",
    "# --- 4) Categorical profile (top-3 per segment) on TEST ---\n",
    "cat_to_show = [\n",
    "    c for c in [\"sex\",\"education\",\"class_of_worker\",\"citizenship\",\n",
    "                \"region_of_previous_residence\",\"age_bucket\",\"weeks_bucket\",\n",
    "                \"is_union\",\"is_self_employed\",\"has_any_capital\"]\n",
    "    if c in Xte_feat.columns\n",
    "]\n",
    "\n",
    "print(\"\\n[HDBSCAN] Categorical profile (top-3 per segment) — TEST\")\n",
    "for c in cat_to_show:\n",
    "    tab = wprop_table(Xte_feat[c], seg_te, w_te)  # segment x category proportions\n",
    "    top3 = tab.apply(lambda r: r.sort_values(ascending=False).head(3), axis=1)\n",
    "    print(f\"\\n- {c} (weighted %):\")\n",
    "    print((top3 * 100).round(1))\n",
    "\n",
    "# --- 5) Supervised lift on TEST (if you have y_test): pos rate and lift vs overall ---\n",
    "if 'y_test' in globals():\n",
    "    pos_rate_seg = ((w_te * y_test).groupby(seg_te).sum() / w_te.groupby(seg_te).sum())\n",
    "    overall_pos = float((w_te * y_test).sum() / w_te.sum())\n",
    "    lift = (pos_rate_seg / overall_pos).rename(\"lift\")\n",
    "    pr_table = pd.DataFrame({\n",
    "        \"weighted_share_%\": weighted_share_te.round(1),\n",
    "        \"pos_rate_%\": (100 * pos_rate_seg).round(1),\n",
    "        \"lift\": lift.round(2),\n",
    "        \"avg_strength\": avg_strength.round(3),\n",
    "    }).sort_values(\"lift\", ascending=False)\n",
    "    print(\"\\n[HDBSCAN] Segment performance on TEST (weighted)\\n\", pr_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c95e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HDBSCAN|trainval tuning] top-5\n",
      "     mcs  min_samples method  sil_tv  noise_tv  min_share_tv  valid  score\n",
      "6    600          NaN    eom   0.276     0.250         0.015   True  0.171\n",
      "10   600          5.0    eom   0.329     0.078         0.011   True  0.141\n",
      "8    600          1.0    eom   0.329     0.068         0.011   True  0.131\n",
      "11   600          5.0   leaf   0.164     0.190         0.012   True  0.121\n",
      "14  1200          1.0    eom   0.340     0.047         0.022   True  0.119\n",
      "\n",
      "[Chosen params] {'min_cluster_size': 600, 'min_samples': None, 'cluster_selection_method': 'eom'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HDBSCAN] weighted_share TEST (%)\n",
      " segment\n",
      "-1     25.0\n",
      " 0      2.0\n",
      " 1      1.1\n",
      " 2      3.3\n",
      " 3      4.2\n",
      " 4      7.2\n",
      " 5      3.6\n",
      " 6      1.2\n",
      " 7      2.6\n",
      " 8      4.8\n",
      " 9      2.6\n",
      " 10    16.4\n",
      " 11     9.5\n",
      " 12    10.2\n",
      " 13     1.3\n",
      " 14     1.8\n",
      " 15     1.8\n",
      " 16     1.5\n",
      "Name: weight, dtype: float64\n",
      "\n",
      "[HDBSCAN] avg membership strength by segment (TEST)\n",
      " segment\n",
      "-1     0.000\n",
      " 0     0.931\n",
      " 1     0.994\n",
      " 2     0.840\n",
      " 3     0.863\n",
      " 4     0.966\n",
      " 5     0.887\n",
      " 6     0.998\n",
      " 7     0.886\n",
      " 8     0.576\n",
      " 9     0.960\n",
      " 10    0.819\n",
      " 11    0.958\n",
      " 12    0.960\n",
      " 13    0.749\n",
      " 14    0.931\n",
      " 15    0.980\n",
      " 16    0.866\n",
      "Name: strength, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HDBSCAN] Numeric profile (weighted means) — TEST\n",
      "             age  weeks_worked_in_year  log_wage_per_hour  is_mover  full_year_worker  no_work\n",
      "segment                                                                                      \n",
      "-1       40.002                21.880              0.209     0.155             0.207    0.376\n",
      " 0       43.438                42.087              0.425     0.075             0.707    0.123\n",
      " 1       41.030                49.109              4.968     0.021             0.828    0.003\n",
      " 2       48.817                39.038              0.227     0.044             0.665    0.182\n",
      " 3       34.646                44.240              6.669     0.082             0.709    0.027\n",
      " 4       50.417                31.036              0.000     0.000             0.533    0.340\n",
      " 5       43.678                47.562              0.000     0.000             0.792    0.004\n",
      " 6       29.851                51.288              0.000     1.000             0.964    0.000\n",
      " 7       11.075                 0.084              0.000     1.000             0.000    0.992\n",
      " 8       68.557                 0.074              0.000     0.000             0.000    0.991\n",
      " 9       43.932                51.950              0.000     0.000             1.000    0.000\n",
      " 10      36.013                49.340              0.000     0.000             0.911    0.000\n",
      " 11       7.121                 0.000              0.000     0.000             0.000    1.000\n",
      " 12       6.816                 0.000              0.000     0.000             0.000    1.000\n",
      " 13      17.894                 0.000              0.000     0.000             0.000    1.000\n",
      " 14      72.557                 0.000              0.000     0.000             0.000    1.000\n",
      " 15      67.674                 0.000              0.000     0.000             0.000    1.000\n",
      " 16      17.965                 0.000              0.000     0.000             0.000    1.000\n",
      "\n",
      "[HDBSCAN] Categorical profile (top-3 per segment) — TEST\n",
      "\n",
      "- sex (weighted %):\n",
      "sex      Female   Male\n",
      "segment               \n",
      "-1         58.8   41.2\n",
      " 0         28.0   72.0\n",
      " 1         40.4   59.6\n",
      " 2         31.1   68.9\n",
      " 3         54.4   45.6\n",
      " 4         52.9   47.1\n",
      " 5         37.1   62.9\n",
      " 6         38.3   61.7\n",
      " 7         50.2   49.8\n",
      " 8         77.6   22.4\n",
      " 9         64.6   35.4\n",
      " 10        42.8   57.2\n",
      " 11       100.0    0.0\n",
      " 12         0.0  100.0\n",
      " 13         0.0  100.0\n",
      " 14         0.0  100.0\n",
      " 15       100.0    0.0\n",
      " 16       100.0    0.0\n",
      "\n",
      "- education (weighted %):\n",
      "education  10th grade  7th and 8th grade  9th grade  Bachelors degree(BA AB BS)  Children  High school graduate  Some college but no degree\n",
      "segment                                                                                                                                    \n",
      "-1                NaN                NaN        NaN                        12.0       NaN                  37.7                        21.6\n",
      " 0                NaN                NaN        NaN                        23.6       NaN                  24.9                        19.4\n",
      " 1                NaN                NaN        NaN                        12.1       NaN                  40.6                        20.9\n",
      " 2                NaN                NaN        NaN                        21.9       NaN                  27.3                        17.7\n",
      " 3                NaN                NaN        NaN                         9.0       NaN                  38.4                        22.9\n",
      " 4                NaN                NaN        NaN                        27.6       NaN                  24.5                        18.7\n",
      " 5                NaN                NaN        NaN                        15.7       NaN                  33.3                        20.2\n",
      " 6                NaN                NaN        NaN                        16.7       NaN                  30.6                        17.0\n",
      " 7                NaN                NaN        4.3                         NaN      75.4                   5.1                         NaN\n",
      " 8                NaN               17.3        NaN                         NaN       NaN                  32.1                        10.1\n",
      " 9                NaN                NaN        NaN                         9.1       NaN                  35.5                        25.6\n",
      " 10               NaN                NaN        NaN                        15.5       NaN                  31.1                        20.7\n",
      " 11               0.0                NaN        NaN                         0.0     100.0                   NaN                         NaN\n",
      " 12               0.0                NaN        NaN                         0.0     100.0                   NaN                         NaN\n",
      " 13              26.5               20.2       27.9                         NaN       NaN                   NaN                         NaN\n",
      " 14               NaN               19.9        NaN                         NaN       NaN                  17.8                        10.2\n",
      " 15              15.5               15.4        NaN                         NaN       NaN                  31.1                         NaN\n",
      " 16              23.4               20.4       27.8                         NaN       NaN                   NaN                         NaN\n",
      "\n",
      "- class_of_worker (weighted %):\n",
      "class_of_worker  Federal government  Local government  Missing  Never worked  Private  Self-employed-incorporated  Self-employed-not incorporated  \\\n",
      "segment                                                                                                                                             \n",
      "-1                              NaN               5.7     43.1           NaN     40.1                         NaN                             NaN   \n",
      " 0                              NaN               NaN     16.8           NaN     58.7                         NaN                             7.7   \n",
      " 1                              NaN              26.2      NaN           NaN     58.2                         NaN                             NaN   \n",
      " 2                              NaN               6.9     22.8           NaN     50.6                         NaN                             NaN   \n",
      " 3                              NaN               5.5      NaN           NaN     90.9                         NaN                             NaN   \n",
      " 4                              NaN               5.6     38.8           NaN     50.2                         NaN                             NaN   \n",
      " 5                              0.0               NaN      NaN           NaN      NaN                        25.5                            74.5   \n",
      " 6                              NaN               4.1      NaN           NaN     90.7                         NaN                             NaN   \n",
      " 7                              NaN               NaN     99.5           0.5      0.0                         NaN                             NaN   \n",
      " 8                              NaN               NaN     99.8           0.1      0.1                         NaN                             NaN   \n",
      " 9                              NaN               7.3      NaN           NaN     82.8                         NaN                             NaN   \n",
      " 10                             NaN               7.0      NaN           NaN     84.0                         NaN                             NaN   \n",
      " 11                             0.0               0.0    100.0           NaN      NaN                         NaN                             NaN   \n",
      " 12                             0.0               0.0    100.0           NaN      NaN                         NaN                             NaN   \n",
      " 13                             0.0               NaN     96.4           3.6      NaN                         NaN                             NaN   \n",
      " 14                             0.0               0.0    100.0           NaN      NaN                         NaN                             NaN   \n",
      " 15                             0.0               0.0    100.0           NaN      NaN                         NaN                             NaN   \n",
      " 16                             0.0               NaN     94.7           5.3      NaN                         NaN                             NaN   \n",
      "\n",
      "class_of_worker  State government  \n",
      "segment                            \n",
      "-1                            NaN  \n",
      " 0                            NaN  \n",
      " 1                           10.6  \n",
      " 2                            NaN  \n",
      " 3                            2.0  \n",
      " 4                            NaN  \n",
      " 5                            NaN  \n",
      " 6                            3.0  \n",
      " 7                            NaN  \n",
      " 8                            NaN  \n",
      " 9                            5.4  \n",
      " 10                           4.3  \n",
      " 11                           NaN  \n",
      " 12                           NaN  \n",
      " 13                           NaN  \n",
      " 14                           NaN  \n",
      " 15                           NaN  \n",
      " 16                           NaN  \n",
      "\n",
      "- citizenship (weighted %):\n",
      "citizenship  Foreign born- Not a citizen of U S  Foreign born- U S citizen by naturalization  Native- Born abroad of American Parent(s)  \\\n",
      "segment                                                                                                                                   \n",
      "-1                                         11.8                                          3.6                                        NaN   \n",
      " 0                                          4.4                                          3.2                                        NaN   \n",
      " 1                                          3.0                                          4.0                                        NaN   \n",
      " 2                                          3.9                                          4.3                                        NaN   \n",
      " 3                                          5.0                                          2.5                                        NaN   \n",
      " 4                                          1.9                                          3.8                                        NaN   \n",
      " 5                                          5.6                                          3.6                                        NaN   \n",
      " 6                                          9.6                                          3.1                                        NaN   \n",
      " 7                                          7.5                                          NaN                                        1.0   \n",
      " 8                                          5.0                                          5.2                                        NaN   \n",
      " 9                                          3.1                                          2.5                                        NaN   \n",
      " 10                                         6.5                                          3.1                                        NaN   \n",
      " 11                                         3.2                                          NaN                                        0.7   \n",
      " 12                                         2.9                                          NaN                                        0.6   \n",
      " 13                                         3.8                                          1.5                                        NaN   \n",
      " 14                                         NaN                                          3.7                                        0.5   \n",
      " 15                                         NaN                                          0.4                                        0.3   \n",
      " 16                                         0.6                                          NaN                                        0.6   \n",
      "\n",
      "citizenship  Native- Born in the United States  \n",
      "segment                                         \n",
      "-1                                        82.7  \n",
      " 0                                        91.1  \n",
      " 1                                        91.7  \n",
      " 2                                        90.2  \n",
      " 3                                        91.1  \n",
      " 4                                        93.1  \n",
      " 5                                        90.0  \n",
      " 6                                        84.9  \n",
      " 7                                        90.7  \n",
      " 8                                        88.4  \n",
      " 9                                        91.9  \n",
      " 10                                       89.0  \n",
      " 11                                       95.7  \n",
      " 12                                       95.8  \n",
      " 13                                       93.8  \n",
      " 14                                       95.4  \n",
      " 15                                       98.9  \n",
      " 16                                       98.3  \n",
      "\n",
      "- region_of_previous_residence (weighted %):\n",
      "region_of_previous_residence  Abroad  Midwest  Missing  Northeast  South  West\n",
      "segment                                                                       \n",
      "-1                               NaN      NaN     84.5        NaN    5.4   4.1\n",
      " 0                               NaN      NaN     92.5        NaN    2.8   2.0\n",
      " 1                               NaN      0.8     97.9        NaN    NaN   1.0\n",
      " 2                               NaN      NaN     95.6        1.2    1.5   NaN\n",
      " 3                               NaN      1.8     91.8        NaN    3.5   NaN\n",
      " 4                               0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 5                               0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 6                               NaN     28.6      NaN        NaN   31.1  26.2\n",
      " 7                               NaN     20.9      NaN        NaN   35.8  27.7\n",
      " 8                               0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 9                               0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 10                              0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 11                              0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 12                              0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 13                              0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 14                              0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 15                              0.0      0.0    100.0        NaN    NaN   NaN\n",
      " 16                              0.0      0.0    100.0        NaN    NaN   NaN\n",
      "\n",
      "- age_bucket (weighted %):\n",
      "age_bucket  18-24  25-34  35-44  45-54  55-64   65+    u18\n",
      "segment                                                   \n",
      "-1           17.5   22.0   20.1    NaN    NaN   NaN    NaN\n",
      " 0            NaN   23.8   24.0   20.9    NaN   NaN    NaN\n",
      " 1            NaN   22.0   32.5   28.6    NaN   NaN    NaN\n",
      " 2            NaN    NaN   25.0   19.6    NaN  22.7    NaN\n",
      " 3           21.8   30.0   22.2    NaN    NaN   NaN    NaN\n",
      " 4            NaN    NaN   22.9   18.5    NaN  23.4    NaN\n",
      " 5            NaN   18.8   30.0   25.3    NaN   NaN    NaN\n",
      " 6           23.7   57.3   13.2    NaN    NaN   NaN    NaN\n",
      " 7            7.2    5.7    NaN    NaN    NaN   NaN   83.2\n",
      " 8            NaN    NaN    NaN    6.8   12.2  69.8    NaN\n",
      " 9            NaN   17.3   38.0   25.8    NaN   NaN    NaN\n",
      " 10           NaN   30.5   27.8   17.5    NaN   NaN    NaN\n",
      " 11           0.0    0.0    NaN    NaN    NaN   NaN  100.0\n",
      " 12           0.0    0.0    NaN    NaN    NaN   NaN  100.0\n",
      " 13          12.1    5.5    NaN    NaN    NaN   NaN   78.4\n",
      " 14           0.0    NaN    NaN    NaN   11.4  88.6    NaN\n",
      " 15           NaN    NaN    NaN    8.1   16.9  75.0    NaN\n",
      " 16          15.5    7.5    NaN    NaN    NaN   NaN   74.7\n",
      "\n",
      "- weeks_bucket (weighted %):\n",
      "weeks_bucket      0  1-26  27-51    52\n",
      "segment                               \n",
      "-1             37.6  21.5   21.8   NaN\n",
      " 0             12.3   NaN   13.8  66.7\n",
      " 1              NaN   3.8   15.6  80.3\n",
      " 2             18.2   NaN   10.0  64.6\n",
      " 3              NaN  12.9   17.6  66.7\n",
      " 4             34.0   NaN    7.9  51.3\n",
      " 5              NaN   7.5   19.4  72.7\n",
      " 6              NaN   1.1    6.5  92.5\n",
      " 7             99.2   0.7    0.1   NaN\n",
      " 8             99.1   0.9    0.0   NaN\n",
      " 9              0.0   NaN    2.9  97.1\n",
      " 10             NaN   5.9    5.9  88.2\n",
      " 11           100.0   0.0    0.0   NaN\n",
      " 12           100.0   0.0    0.0   NaN\n",
      " 13           100.0   0.0    0.0   NaN\n",
      " 14           100.0   0.0    0.0   NaN\n",
      " 15           100.0   0.0    0.0   NaN\n",
      " 16           100.0   0.0    0.0   NaN\n",
      "\n",
      "- is_union (weighted %):\n",
      "is_union      0      1\n",
      "segment               \n",
      "-1         98.6    1.4\n",
      " 0         99.0    1.0\n",
      " 1          0.0  100.0\n",
      " 2        100.0    0.0\n",
      " 3        100.0    0.0\n",
      " 4        100.0    0.0\n",
      " 5        100.0    0.0\n",
      " 6        100.0    0.0\n",
      " 7        100.0    0.0\n",
      " 8        100.0    0.0\n",
      " 9        100.0    0.0\n",
      " 10       100.0    0.0\n",
      " 11       100.0    0.0\n",
      " 12       100.0    0.0\n",
      " 13       100.0    0.0\n",
      " 14       100.0    0.0\n",
      " 15       100.0    0.0\n",
      " 16       100.0    0.0\n",
      "\n",
      "- is_self_employed (weighted %):\n",
      "is_self_employed      0      1\n",
      "segment                       \n",
      "-1                 93.0    7.0\n",
      " 0                 86.7   13.3\n",
      " 1                100.0    0.0\n",
      " 2                 87.8   12.2\n",
      " 3                100.0    0.0\n",
      " 4                100.0    0.0\n",
      " 5                  0.0  100.0\n",
      " 6                100.0    0.0\n",
      " 7                100.0    0.0\n",
      " 8                100.0    0.0\n",
      " 9                100.0    0.0\n",
      " 10               100.0    0.0\n",
      " 11               100.0    0.0\n",
      " 12               100.0    0.0\n",
      " 13               100.0    0.0\n",
      " 14               100.0    0.0\n",
      " 15               100.0    0.0\n",
      " 16               100.0    0.0\n",
      "\n",
      "- has_any_capital (weighted %):\n",
      "has_any_capital      0      1\n",
      "segment                      \n",
      "-1                89.3   10.7\n",
      " 0                 0.0  100.0\n",
      " 1                93.9    6.1\n",
      " 2                 0.0  100.0\n",
      " 3                99.1    0.9\n",
      " 4                 0.0  100.0\n",
      " 5               100.0    0.0\n",
      " 6               100.0    0.0\n",
      " 7               100.0    0.0\n",
      " 8               100.0    0.0\n",
      " 9               100.0    0.0\n",
      " 10              100.0    0.0\n",
      " 11              100.0    0.0\n",
      " 12              100.0    0.0\n",
      " 13              100.0    0.0\n",
      " 14              100.0    0.0\n",
      " 15              100.0    0.0\n",
      " 16              100.0    0.0\n",
      "\n",
      "[HDBSCAN] silhouette_test: 0.27057741133410124\n",
      "\n",
      "[HDBSCAN] Segment performance on TEST (weighted)\n",
      "          weighted_share_%  pos_rate_%  lift  avg_strength\n",
      "segment                                                  \n",
      " 0                    2.0        32.5  4.92         0.931\n",
      " 2                    3.3        30.4  4.61         0.840\n",
      " 4                    7.2        19.2  2.91         0.966\n",
      " 5                    3.6        12.2  1.85         0.887\n",
      " 10                  16.4         8.8  1.33         0.819\n",
      " 9                    2.6         7.7  1.17         0.960\n",
      " 1                    1.1         7.5  1.14         0.994\n",
      " 6                    1.2         5.3  0.81         0.998\n",
      "-1                   25.0         4.7  0.71         0.000\n",
      " 3                    4.2         2.3  0.36         0.863\n",
      " 14                   1.8         1.3  0.20         0.931\n",
      " 8                    4.8         0.9  0.13         0.576\n",
      " 15                   1.8         0.1  0.02         0.980\n",
      " 11                   9.5         0.0  0.00         0.958\n",
      " 12                  10.2         0.0  0.00         0.960\n",
      " 13                   1.3         0.0  0.00         0.749\n",
      " 7                    2.6         0.0  0.00         0.886\n",
      " 16                   1.5         0.0  0.00         0.866\n"
     ]
    }
   ],
   "source": [
    "# === HDBSCAN quick tuning on TRAINVAL (unsupervised only), then final TEST report ===\n",
    "\n",
    "\n",
    "# 0) Embeddings: fit prep on trainval, transform both (no leakage into test)\n",
    "Z_tv  = prep_pipe.fit_transform(X_trainval)\n",
    "Z_te  = prep_pipe.transform(X_test)\n",
    "\n",
    "# 1) PPS subsample on trainval for fitting speed (HDBSCAN.fit has no sample_weight)\n",
    "rng      = np.random.default_rng(42)\n",
    "N_CAP    = 60000\n",
    "n_tv     = len(Z_tv)\n",
    "n_sub    = int(min(n_tv, N_CAP))\n",
    "p_tv     = w_trainval.values / w_trainval.values.sum()\n",
    "idx_sub  = rng.choice(n_tv, size=n_sub, replace=False, p=p_tv)\n",
    "Z_sub    = Z_tv[idx_sub]\n",
    "\n",
    "def weighted_share_by_label(labels, w):\n",
    "    w = pd.Series(w)\n",
    "    lab = pd.Series(labels, index=w.index[:len(labels)])\n",
    "    return w.groupby(lab).sum() / w.sum()\n",
    "\n",
    "def min_weighted_cluster_share(labels, w, ignore_noise=True):\n",
    "    lab = pd.Series(labels)\n",
    "    if ignore_noise:\n",
    "        mask = (lab != -1)\n",
    "        lab = lab[mask]\n",
    "        w   = pd.Series(w).loc[mask.values]\n",
    "    sh = weighted_share_by_label(lab.values, w.values)\n",
    "    return float(sh.min()) if len(sh) else 0.0\n",
    "\n",
    "# 2) Tiny grid (fast)\n",
    "min_cluster_size_fracs = [0.005, 0.01, 0.02]   # ~0.5%-2% of subsample\n",
    "min_samples_list       = [None, 1, 5]\n",
    "methods                = [\"eom\", \"leaf\"]\n",
    "\n",
    "MIN_CLUSTER_SHARE = 0.01   # drop configs that create tiny clusters (<1% weighted)\n",
    "NOISE_TARGET      = 0.20   # prefer ~20% noise; soft preference\n",
    "NOISE_TOL         = 0.20\n",
    "\n",
    "rows = []\n",
    "for frac in min_cluster_size_fracs:\n",
    "    mcs = max(100, int(frac * n_sub))\n",
    "    for ms in min_samples_list:\n",
    "        for meth in methods:\n",
    "            clus = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=mcs,\n",
    "                min_samples=ms,\n",
    "                metric=\"euclidean\",\n",
    "                cluster_selection_method=meth,\n",
    "                approx_min_span_tree=True,\n",
    "                prediction_data=True\n",
    "            )\n",
    "            # fit on subsample\n",
    "            clus.fit(Z_sub)\n",
    "            # assign all trainval\n",
    "            tv_labels, _ = hdbscan.approximate_predict(clus, Z_tv)\n",
    "\n",
    "            # metrics on trainval (internal-only)\n",
    "            try:\n",
    "                sil = silhouette_score(Z_tv, tv_labels) if len(np.unique(tv_labels)) > 1 else np.nan\n",
    "            except Exception:\n",
    "                sil = np.nan\n",
    "            share_tv  = weighted_share_by_label(tv_labels, w_trainval.values)\n",
    "            noise_tv  = float(share_tv.get(-1, 0.0))\n",
    "            min_share = min_weighted_cluster_share(tv_labels, w_trainval.values, ignore_noise=True)\n",
    "            valid     = (min_share >= MIN_CLUSTER_SHARE) and (np.unique(tv_labels[tv_labels!=-1]).size >= 2)\n",
    "\n",
    "            # rank: ↑sil, noise close to target, no tiny clusters\n",
    "            noise_dev = abs(noise_tv - NOISE_TARGET)\n",
    "            score = (0.8 * np.nan_to_num(sil, nan=-1.0) - 0.2 * (noise_dev / NOISE_TOL))\n",
    "\n",
    "            rows.append({\n",
    "                \"mcs\": mcs, \"min_samples\": ms, \"method\": meth,\n",
    "                \"sil_tv\": sil, \"noise_tv\": noise_tv, \"min_share_tv\": min_share,\n",
    "                \"valid\": valid, \"score\": score\n",
    "            })\n",
    "\n",
    "tune = pd.DataFrame(rows)\n",
    "tune_v = tune[tune.valid].copy()\n",
    "if tune_v.empty:\n",
    "    raise RuntimeError(\"All HDBSCAN configs invalid under constraints; relax MIN_CLUSTER_SHARE or increase min_cluster_size.\")\n",
    "tune_v = tune_v.sort_values([\"score\",\"sil_tv\"], ascending=[False, False])\n",
    "\n",
    "print(\"\\n[HDBSCAN|trainval tuning] top-5\")\n",
    "print(tune_v.head(5).round(3))\n",
    "\n",
    "best = tune_v.iloc[0].to_dict()\n",
    "best_params = {\n",
    "    \"min_cluster_size\": int(best[\"mcs\"]),\n",
    "    \"min_samples\": None if pd.isna(best[\"min_samples\"]) else int(best[\"min_samples\"]),\n",
    "    \"cluster_selection_method\": best[\"method\"]\n",
    "}\n",
    "print(\"\\n[Chosen params]\", best_params)\n",
    "\n",
    "# 3) Refit on trainval with chosen params (subsample again for speed), then report on TEST\n",
    "idx_sub2 = rng.choice(n_tv, size=n_sub, replace=False, p=p_tv)\n",
    "clus_final = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=best_params[\"min_cluster_size\"],\n",
    "    min_samples=best_params[\"min_samples\"],\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=best_params[\"cluster_selection_method\"],\n",
    "    approx_min_span_tree=True,\n",
    "    prediction_data=True\n",
    ").fit(Z_tv[idx_sub2])\n",
    "\n",
    "# assign TEST\n",
    "te_labels, te_strength = hdbscan.approximate_predict(clus_final, Z_te)\n",
    "labels_test = pd.Series(te_labels, index=X_test.index, name=\"segment\")\n",
    "strength_te = pd.Series(te_strength, index=X_test.index, name=\"strength\")\n",
    "\n",
    "# 4) TEST report \n",
    "w_te = w_test.reindex(X_test.index)\n",
    "weighted_share_te = 100.0 * w_te.groupby(labels_test).sum() / w_te.sum()\n",
    "print(\"\\n[HDBSCAN] weighted_share TEST (%)\\n\", weighted_share_te.round(1))\n",
    "\n",
    "def wmean_by_seg(values, seg, w):\n",
    "    num = (values * w).groupby(seg).sum()\n",
    "    den = w.groupby(seg).sum()\n",
    "    return (num / den).rename(values.name)\n",
    "\n",
    "avg_strength = wmean_by_seg(strength_te, labels_test, w_te)\n",
    "print(\"\\n[HDBSCAN] avg membership strength by segment (TEST)\\n\", avg_strength.round(3))\n",
    "\n",
    "Xte_feat = prep_pipe.named_steps[\"feat\"].transform(X_test.copy())\n",
    "\n",
    "# numeric profile\n",
    "num_to_show = [c for c in [\"age\",\"weeks_worked_in_year\",\"log_wage_per_hour\",\"is_mover\",\n",
    "                           \"full_year_worker\",\"no_work\"] if c in Xte_feat.columns]\n",
    "num_prof_te = pd.concat([wmean_by_seg(Xte_feat[c], labels_test, w_te) for c in num_to_show], axis=1)\n",
    "print(\"\\n[HDBSCAN] Numeric profile (weighted means) — TEST\\n\", num_prof_te.round(3))\n",
    "\n",
    "# categorical profile (top-3)\n",
    "def wprop_table(cat_series, seg, w):\n",
    "    dummies = pd.get_dummies(cat_series.astype(str), dummy_na=False)\n",
    "    wmat = dummies.mul(w.values.reshape(-1, 1))\n",
    "    grp = wmat.groupby(seg).sum()\n",
    "    grp = grp.div(grp.sum(axis=1), axis=0)\n",
    "    grp.columns.name = cat_series.name\n",
    "    return grp\n",
    "\n",
    "cat_to_show = [c for c in [\"sex\",\"education\",\"class_of_worker\",\"citizenship\",\n",
    "                           \"region_of_previous_residence\",\"age_bucket\",\"weeks_bucket\",\n",
    "                           \"is_union\",\"is_self_employed\",\"has_any_capital\"]\n",
    "               if c in Xte_feat.columns]\n",
    "print(\"\\n[HDBSCAN] Categorical profile (top-3 per segment) — TEST\")\n",
    "for c in cat_to_show:\n",
    "    tab = wprop_table(Xte_feat[c], labels_test, w_te)\n",
    "    top3 = tab.apply(lambda r: r.sort_values(ascending=False).head(3), axis=1)\n",
    "    print(f\"\\n- {c} (weighted %):\")\n",
    "    print((100 * top3).round(1))\n",
    "\n",
    "# internal quality on TEST\n",
    "from sklearn.metrics import silhouette_score\n",
    "print(\"\\n[HDBSCAN] silhouette_test:\", silhouette_score(Z_te, labels_test))\n",
    "\n",
    "# optional supervised lift on TEST (does not influence selection)\n",
    "if 'y_test' in globals():\n",
    "    pos_rate = ((w_te * y_test).groupby(labels_test).sum() / w_te.groupby(labels_test).sum())\n",
    "    overall  = float((w_te * y_test).sum() / w_te.sum())\n",
    "    lift     = (pos_rate / overall).rename(\"lift\")\n",
    "    pr_table = pd.DataFrame({\n",
    "        \"weighted_share_%\": weighted_share_te.round(1),\n",
    "        \"pos_rate_%\": (100*pos_rate).round(1),\n",
    "        \"lift\": lift.round(2),\n",
    "        \"avg_strength\": avg_strength.round(3),\n",
    "    }).sort_values(\"lift\", ascending=False)\n",
    "    print(\"\\n[HDBSCAN] Segment performance on TEST (weighted)\\n\", pr_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cf2d08",
   "metadata": {},
   "source": [
    "# GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25944c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Gaussian Mixture Models (soft clustering; choose K by weighted BIC) ===\n",
    "\n",
    "# 0) 30-dim embeddings from your prep_pipe (dense)\n",
    "Z_trainval = prep_pipe.fit_transform(X_trainval)\n",
    "Z_test     = prep_pipe.transform(X_test)\n",
    "\n",
    "# ---- helpers ---------------------------------------------------------------\n",
    "def weighted_bic(gmm, Z, w):\n",
    "    \"\"\"Compute a BIC-like criterion with sample weights.\n",
    "       BIC_w = -2 * sum_i w_i * log p(x_i) + p * log(sum_i w_i)\n",
    "    \"\"\"\n",
    "    # per-sample log-likelihood\n",
    "    ll = gmm.score_samples(Z)                   # shape (n,)\n",
    "    n_eff = float(w.sum())\n",
    "\n",
    "    # number of free parameters for GMM\n",
    "    k = gmm.n_components\n",
    "    d = Z.shape[1]\n",
    "    cov = gmm.covariance_type\n",
    "    if cov == \"full\":\n",
    "        cov_params = k * d * (d + 1) // 2\n",
    "    elif cov == \"diag\":\n",
    "        cov_params = k * d\n",
    "    elif cov == \"tied\":\n",
    "        cov_params = d * (d + 1) // 2\n",
    "    elif cov == \"spherical\":\n",
    "        cov_params = k\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown covariance_type: {cov}\")\n",
    "    n_params = k * d + cov_params + (k - 1)     # means + covs + weights(k-1)\n",
    "\n",
    "    return -2.0 * float((w * ll).sum()) + n_params * np.log(n_eff)\n",
    "\n",
    "def pps_resample(Z, w, n_sub=None, rng=None):\n",
    "    \"\"\"Probability-proportional-to-size resampling WITH replacement.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(42)\n",
    "    n = len(Z)\n",
    "    if n_sub is None:\n",
    "        n_sub = n                              # keep same size\n",
    "    p = w / w.sum()\n",
    "    idx = rng.choice(n, size=n_sub, replace=True, p=p)\n",
    "    return Z[idx]\n",
    "\n",
    "# ---- (A) fit-time PPS to approximate weights --------------------------------\n",
    "Z_sub = pps_resample(Z_trainval, w_trainval.values, n_sub=len(Z_trainval))  # or a cap like min(n, 120000)\n",
    "\n",
    "# ---- (B) sweep K by weighted BIC; tie-break by silhouette -------------------\n",
    "Ks = range(4, 11)\n",
    "results = []\n",
    "\n",
    "for k in Ks:\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=k,\n",
    "        covariance_type=\"full\",\n",
    "        reg_covar=1e-6,\n",
    "        random_state=42,\n",
    "        max_iter=500,\n",
    "        init_params=\"kmeans\"\n",
    "    )\n",
    "    # fit WITHOUT sample_weight (not supported in your sklearn)\n",
    "    gmm.fit(Z_sub)\n",
    "\n",
    "    # evaluate on FULL data with weights: weighted BIC (lower is better)\n",
    "    bic_w = weighted_bic(gmm, Z_trainval, w_trainval.values)\n",
    "\n",
    "    # optional: silhouette on hard labels (unweighted heuristic)\n",
    "    labels = gmm.predict(Z_trainval)\n",
    "    try:\n",
    "        sil = silhouette_score(Z_trainval, labels, metric=\"euclidean\")\n",
    "    except Exception:\n",
    "        sil = np.nan\n",
    "\n",
    "    results.append((k, bic_w, sil, gmm))\n",
    "\n",
    "# pick best by weighted BIC, break ties by higher silhouette\n",
    "results.sort(key=lambda t: (t[1], -np.nan_to_num(t[2], nan=-1e9)))\n",
    "best_k, best_bic_w, best_sil, best_model = results[0]\n",
    "print(f\"[GMM] best_k={best_k} | weighted-BIC={best_bic_w:.1f} | silhouette={best_sil:.3f}\")\n",
    "\n",
    "# ---- (C) label & probability, profiles -------------------------------------\n",
    "labels_trainval = pd.Series(best_model.predict(Z_trainval), index=X_trainval.index, name=\"segment\")\n",
    "proba_trainval  = pd.DataFrame(best_model.predict_proba(Z_trainval), index=X_trainval.index)\n",
    "labels_test     = pd.Series(best_model.predict(Z_test), index=X_test.index, name=\"segment\")\n",
    "proba_test      = pd.DataFrame(best_model.predict_proba(Z_test), index=X_test.index)\n",
    "\n",
    "core_mask = proba_trainval.max(axis=1) >= 0.7\n",
    "print(f\"[GMM] core members ratio: {core_mask.mean():.2%}\")\n",
    "\n",
    "wshare = 100 * w_trainval.groupby(labels_trainval).sum() / w_trainval.sum()\n",
    "print(\"\\n[GMM] weighted_share (%)\\n\", wshare.round(1))\n",
    "\n",
    "Xtv_feat = prep_pipe.named_steps[\"feat\"].transform(X_trainval.copy())\n",
    "\n",
    "def wmean_by_seg(s_values, seg, w):\n",
    "    num = (s_values * w).groupby(seg).sum()\n",
    "    den = w.groupby(seg).sum()\n",
    "    return (num / den).rename(s_values.name)\n",
    "\n",
    "num_to_show = [c for c in [\"age\",\"weeks_worked_in_year\",\"log_wage_per_hour\",\"is_mover\",\n",
    "                           \"full_year_worker\",\"no_work\"] if c in Xtv_feat.columns]\n",
    "num_prof = pd.concat([wmean_by_seg(Xtv_feat[c], labels_trainval, w_trainval) for c in num_to_show], axis=1)\n",
    "print(\"\\n[GMM] Numeric profile (weighted means)\\n\", num_prof.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4b173bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GMM] weighted_share TEST (%)\n",
      " segment\n",
      "0     8.9\n",
      "1    22.4\n",
      "2    23.0\n",
      "3    14.4\n",
      "4     3.8\n",
      "5    12.8\n",
      "6     5.5\n",
      "7     0.2\n",
      "8     1.9\n",
      "9     7.0\n",
      "Name: weight, dtype: float64\n",
      "\n",
      "[GMM] avg max-membership prob by segment (TEST)\n",
      " segment\n",
      "0    1.0\n",
      "1    1.0\n",
      "2    1.0\n",
      "3    1.0\n",
      "4    1.0\n",
      "5    1.0\n",
      "6    1.0\n",
      "7    1.0\n",
      "8    1.0\n",
      "9    1.0\n",
      "Name: max_prob, dtype: float64\n",
      "\n",
      "[GMM] Numeric profile (weighted means) — TEST\n",
      "             age  weeks_worked_in_year  log_wage_per_hour  is_mover  full_year_worker  no_work\n",
      "segment                                                                                      \n",
      "0        49.571                33.293              0.000     0.047             0.561    0.287\n",
      "1         8.170                 0.000              0.000     0.000             0.000    1.000\n",
      "2        39.467                51.922              0.000     0.000             1.000    0.000\n",
      "3        60.819                 0.000              0.000     0.000             0.000    1.000\n",
      "4        48.348                39.849              0.459     0.049             0.682    0.168\n",
      "5        32.963                21.664              0.000     0.000             0.000    0.176\n",
      "6        36.068                44.702              6.744     0.097             0.722    0.027\n",
      "7        40.264                49.625              7.055     0.002             0.882    0.016\n",
      "8        43.736                41.496              0.000     0.080             0.693    0.131\n",
      "9        26.574                20.935              0.000     1.000             0.289    0.491\n",
      "\n",
      "[GMM] Categorical profile (top-3 per segment) — TEST\n",
      "\n",
      "- sex (weighted %):\n",
      "sex      Female  Male\n",
      "segment              \n",
      "0          51.9  48.1\n",
      "1          48.7  51.3\n",
      "2          46.1  53.9\n",
      "3          67.7  32.3\n",
      "4          30.6  69.4\n",
      "5          54.6  45.4\n",
      "6          51.2  48.8\n",
      "7          31.6  68.4\n",
      "8          27.5  72.5\n",
      "9          50.7  49.3\n",
      "\n",
      "- education (weighted %):\n",
      "education  7th and 8th grade  9th grade  Bachelors degree(BA AB BS)  Children  High school graduate  Some college but no degree\n",
      "segment                                                                                                                        \n",
      "0                        NaN        NaN                        27.4       NaN                  23.9                        19.6\n",
      "1                        2.5        3.2                         NaN      87.8                   NaN                         NaN\n",
      "2                        NaN        NaN                        15.9       NaN                  34.7                        21.6\n",
      "3                       11.8        NaN                         NaN       NaN                  36.4                        11.6\n",
      "4                        NaN        NaN                        22.7       NaN                  26.9                        16.9\n",
      "5                        NaN        NaN                         8.9       NaN                  29.5                        21.2\n",
      "6                        NaN        NaN                         9.6       NaN                  39.2                        23.0\n",
      "7                        NaN        NaN                        10.4       NaN                  33.4                        27.6\n",
      "8                        NaN        NaN                        24.4       NaN                  24.4                        18.9\n",
      "9                        NaN        NaN                         NaN      27.5                  24.8                        13.3\n",
      "\n",
      "- class_of_worker (weighted %):\n",
      "class_of_worker  Federal government  Local government  Missing  Private  Self-employed-not incorporated  State government\n",
      "segment                                                                                                                  \n",
      "0                               NaN               NaN     32.6     45.2                             7.5               NaN\n",
      "1                               0.0               0.0    100.0      NaN                             NaN               NaN\n",
      "2                               NaN               8.5      NaN     69.2                             9.1               NaN\n",
      "3                               0.0               0.0    100.0      NaN                             NaN               NaN\n",
      "4                               NaN               7.5     21.0     50.2                             NaN               NaN\n",
      "5                               NaN               NaN     24.7     56.0                             7.2               NaN\n",
      "6                               NaN               6.7      NaN     87.3                             NaN               3.4\n",
      "7                              16.5               4.2      NaN     77.8                             NaN               NaN\n",
      "8                               NaN               NaN     17.9     57.4                             8.2               NaN\n",
      "9                               NaN               NaN     50.1     40.5                             2.9               NaN\n",
      "\n",
      "- citizenship (weighted %):\n",
      "citizenship  Foreign born- Not a citizen of U S  Foreign born- U S citizen by naturalization  Native- Born abroad of American Parent(s)  \\\n",
      "segment                                                                                                                                   \n",
      "0                                           1.9                                          3.3                                        NaN   \n",
      "1                                           3.7                                          NaN                                        0.7   \n",
      "2                                           7.5                                          3.4                                        NaN   \n",
      "3                                           7.7                                          4.6                                        NaN   \n",
      "4                                           3.9                                          4.3                                        NaN   \n",
      "5                                           8.4                                          2.7                                        NaN   \n",
      "6                                           4.5                                          2.7                                        NaN   \n",
      "7                                           4.9                                          6.5                                        NaN   \n",
      "8                                           4.4                                          3.1                                        NaN   \n",
      "9                                          10.0                                          2.2                                        NaN   \n",
      "\n",
      "citizenship  Native- Born in the United States  \n",
      "segment                                         \n",
      "0                                         93.6  \n",
      "1                                         95.0  \n",
      "2                                         87.5  \n",
      "3                                         86.2  \n",
      "4                                         90.4  \n",
      "5                                         87.5  \n",
      "6                                         91.5  \n",
      "7                                         87.0  \n",
      "8                                         91.2  \n",
      "9                                         85.6  \n",
      "\n",
      "- region_of_previous_residence (weighted %):\n",
      "region_of_previous_residence  Abroad  Midwest  Missing  Northeast  South  West\n",
      "segment                                                                       \n",
      "0                                NaN      1.4     95.3        NaN    1.6   NaN\n",
      "1                                0.0      0.0    100.0        NaN    NaN   NaN\n",
      "2                                0.0      0.0    100.0        NaN    NaN   NaN\n",
      "3                                0.0      0.0    100.0        NaN    NaN   NaN\n",
      "4                                NaN      NaN     95.1        1.2    1.8   NaN\n",
      "5                                0.0      0.0    100.0        NaN    NaN   NaN\n",
      "6                                NaN      2.6     90.3        NaN    3.6   NaN\n",
      "7                                0.0      0.2     99.8        NaN    NaN   NaN\n",
      "8                                NaN      NaN     92.0        NaN    2.9   2.1\n",
      "9                                NaN     20.6      NaN        NaN   34.8  27.7\n",
      "\n",
      "- age_bucket (weighted %):\n",
      "age_bucket  18-24  25-34  35-44  45-54  55-64   65+   u18\n",
      "segment                                                  \n",
      "0             NaN    NaN   23.6   19.0    NaN  21.3   NaN\n",
      "1             3.1    0.0    NaN    NaN    NaN   NaN  96.9\n",
      "2             NaN   27.3   29.4   20.2    NaN   NaN   NaN\n",
      "3             NaN    NaN   11.9    NaN   15.3  50.8   NaN\n",
      "4             NaN    NaN   26.3   20.4    NaN  21.1   NaN\n",
      "5            29.2   20.9   17.2    NaN    NaN   NaN   NaN\n",
      "6            18.5   28.9   23.8    NaN    NaN   NaN   NaN\n",
      "7             NaN   20.5   27.6   27.1    NaN   NaN   NaN\n",
      "8             NaN   24.1   23.6   21.0    NaN   NaN   NaN\n",
      "9            17.3   24.5    NaN    NaN    NaN   NaN  31.3\n",
      "\n",
      "- weeks_bucket (weighted %):\n",
      "weeks_bucket      0  1-26  27-51    52\n",
      "segment                               \n",
      "0              28.7   NaN   10.1  53.6\n",
      "1             100.0   0.0    0.0   NaN\n",
      "2               0.0   NaN    4.5  95.5\n",
      "3             100.0   0.0    0.0   NaN\n",
      "4              16.8   NaN   10.0  66.4\n",
      "5              17.6  43.6   38.8   NaN\n",
      "6               NaN  11.6   17.3  68.3\n",
      "7               NaN   1.7   15.3  81.4\n",
      "8              13.1   NaN   13.8  65.5\n",
      "9              49.1   NaN   12.0  27.4\n",
      "\n",
      "- is_union (weighted %):\n",
      "is_union      0     1\n",
      "segment              \n",
      "0          98.8   1.2\n",
      "1         100.0   0.0\n",
      "2          98.8   1.2\n",
      "3         100.0   0.0\n",
      "4          96.5   3.5\n",
      "5          99.6   0.4\n",
      "6          84.5  15.5\n",
      "7          74.0  26.0\n",
      "8          98.8   1.2\n",
      "9          99.7   0.3\n",
      "\n",
      "- is_self_employed (weighted %):\n",
      "is_self_employed      0     1\n",
      "segment                      \n",
      "0                  88.4  11.6\n",
      "1                 100.0   0.0\n",
      "2                  87.4  12.6\n",
      "3                 100.0   0.0\n",
      "4                  86.3  13.7\n",
      "5                  91.7   8.3\n",
      "6                 100.0   0.0\n",
      "7                 100.0   0.0\n",
      "8                  85.9  14.1\n",
      "9                  96.4   3.6\n",
      "\n",
      "- has_any_capital (weighted %):\n",
      "has_any_capital      0      1\n",
      "segment                      \n",
      "0                  0.0  100.0\n",
      "1                100.0    0.0\n",
      "2                100.0    0.0\n",
      "3                100.0    0.0\n",
      "4                  0.0  100.0\n",
      "5                100.0    0.0\n",
      "6                 90.7    9.3\n",
      "7                  0.0  100.0\n",
      "8                  0.0  100.0\n",
      "9                100.0    0.0\n",
      "\n",
      "[GMM] silhouette_test: 0.31945712000314885\n",
      "\n",
      "[GMM] Segment performance on TEST (weighted)\n",
      "          weighted_share_%  pos_rate_%  lift  avg_conf\n",
      "segment                                              \n",
      "4                     3.8        34.4  5.21       1.0\n",
      "8                     1.9        33.6  5.09       1.0\n",
      "7                     0.2        21.2  3.22       1.0\n",
      "0                     8.9        20.2  3.06       1.0\n",
      "2                    23.0         9.5  1.44       1.0\n",
      "6                     5.5         3.7  0.57       1.0\n",
      "9                     7.0         2.0  0.30       1.0\n",
      "5                    12.8         1.5  0.23       1.0\n",
      "3                    14.4         0.6  0.10       1.0\n",
      "1                    22.4         0.0  0.00       1.0\n"
     ]
    }
   ],
   "source": [
    "# === Test-set reporting for the chosen GMM ===\n",
    "# Assumes you already have:\n",
    "# - best_model (fitted GMM)\n",
    "# - Z_test (SVD embeddings of X_test)\n",
    "# - labels_test, proba_test from best_model on Z_test\n",
    "# - prep_pipe (FeatureBuilder + preprocess + SVD), w_test, y_test (optional)\n",
    "\n",
    "# 0) Build post-FeatureBuilder features for TEST (readable profiling)\n",
    "Xte_feat = prep_pipe.named_steps[\"feat\"].transform(X_test.copy())\n",
    "\n",
    "# 1) Helpers (weighted mean / weighted category %)\n",
    "def wmean_by_seg(values, seg, w):\n",
    "    \"\"\"Weighted mean grouped by segment.\"\"\"\n",
    "    num = (values * w).groupby(seg).sum()\n",
    "    den = w.groupby(seg).sum()\n",
    "    return (num / den).rename(values.name)\n",
    "\n",
    "def wprop_table(cat_series, seg, w):\n",
    "    \"\"\"Weighted proportion table: rows=segment, cols=category.\"\"\"\n",
    "    dummies = pd.get_dummies(cat_series.astype(str), dummy_na=False)\n",
    "    wmat = dummies.mul(w.values.reshape(-1, 1))\n",
    "    grp = wmat.groupby(seg).sum()\n",
    "    grp = grp.div(grp.sum(axis=1), axis=0)\n",
    "    grp.columns.name = cat_series.name\n",
    "    return grp\n",
    "\n",
    "# 2) Basic segment sizes and confidence on TEST\n",
    "seg_te = labels_test.reindex(X_test.index)\n",
    "w_te   = w_test.reindex(X_test.index)\n",
    "\n",
    "weighted_share_te = 100.0 * w_te.groupby(seg_te).sum() / w_te.sum()\n",
    "print(\"\\n[GMM] weighted_share TEST (%)\\n\", weighted_share_te.round(1))\n",
    "\n",
    "# confidence = max posterior prob per sample, averaged (weighted) per segment\n",
    "max_prob_te = proba_test.max(axis=1).rename(\"max_prob\")\n",
    "avg_conf_te = wmean_by_seg(max_prob_te, seg_te, w_te)\n",
    "print(\"\\n[GMM] avg max-membership prob by segment (TEST)\\n\", avg_conf_te.round(3))\n",
    "\n",
    "# 3) Numeric profile (weighted means) on TEST\n",
    "num_to_show = [\n",
    "    c for c in [\"age\",\"weeks_worked_in_year\",\"log_wage_per_hour\",\n",
    "                \"is_mover\",\"full_year_worker\",\"no_work\"]\n",
    "    if c in Xte_feat.columns\n",
    "]\n",
    "num_profiles_test = pd.concat(\n",
    "    [wmean_by_seg(Xte_feat[c], seg_te, w_te) for c in num_to_show], axis=1\n",
    ")\n",
    "print(\"\\n[GMM] Numeric profile (weighted means) — TEST\\n\", num_profiles_test.round(3))\n",
    "\n",
    "# 4) Categorical profile (weighted %; top-3 per segment) on TEST\n",
    "cat_to_show = [\n",
    "    c for c in [\"sex\",\"education\",\"class_of_worker\",\"citizenship\",\n",
    "                \"region_of_previous_residence\",\"age_bucket\",\"weeks_bucket\",\n",
    "                \"is_union\",\"is_self_employed\",\"has_any_capital\"]\n",
    "    if c in Xte_feat.columns\n",
    "]\n",
    "print(\"\\n[GMM] Categorical profile (top-3 per segment) — TEST\")\n",
    "for c in cat_to_show:\n",
    "    tab = wprop_table(Xte_feat[c], seg_te, w_te)\n",
    "    top3 = tab.apply(lambda r: r.sort_values(ascending=False).head(3), axis=1)\n",
    "    print(f\"\\n- {c} (weighted %):\")\n",
    "    print((100 * top3).round(1))\n",
    "\n",
    "# 5) Optional: internal quality & supervised lift on TEST\n",
    "from sklearn.metrics import silhouette_score\n",
    "print(\"\\n[GMM] silhouette_test:\", silhouette_score(Z_test, seg_te))\n",
    "\n",
    "if 'y_test' in globals():\n",
    "    pos_rate = ((w_te * y_test).groupby(seg_te).sum() / w_te.groupby(seg_te).sum())\n",
    "    overall  = float((w_te * y_test).sum() / w_te.sum())\n",
    "    lift     = (pos_rate / overall).rename(\"lift\")\n",
    "    pr_table = pd.DataFrame({\n",
    "        \"weighted_share_%\": weighted_share_te.round(1),\n",
    "        \"pos_rate_%\": (100 * pos_rate).round(1),\n",
    "        \"lift\": lift.round(2),\n",
    "        \"avg_conf\": avg_conf_te.round(3),\n",
    "    }).sort_values(\"lift\", ascending=False)\n",
    "    print(\"\\n[GMM] Segment performance on TEST (weighted)\\n\", pr_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a575e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GMM quick-tune] best: k=13 | cov=full | reg=1e-06 | seed=42 | BIC_w=-16504827.9 | sil=0.326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxzhang/bertopic_env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GMM] weighted_share TEST (%)\n",
      " segment\n",
      "0      7.3\n",
      "1      4.7\n",
      "2     22.4\n",
      "3      3.4\n",
      "4      8.8\n",
      "5      2.0\n",
      "6     14.3\n",
      "7      0.3\n",
      "8     12.6\n",
      "9      6.7\n",
      "10    11.8\n",
      "11     4.2\n",
      "12     1.4\n",
      "Name: weight, dtype: float64\n",
      "\n",
      "[GMM] avg max-membership prob by segment (TEST)\n",
      " segment\n",
      "0     1.0\n",
      "1     1.0\n",
      "2     1.0\n",
      "3     1.0\n",
      "4     1.0\n",
      "5     1.0\n",
      "6     1.0\n",
      "7     1.0\n",
      "8     1.0\n",
      "9     1.0\n",
      "10    1.0\n",
      "11    1.0\n",
      "12    1.0\n",
      "Name: max_prob, dtype: float64\n",
      "\n",
      "[GMM] Numeric profile (weighted means) — TEST\n",
      "             age  weeks_worked_in_year  log_wage_per_hour  is_mover  full_year_worker  no_work\n",
      "segment                                                                                      \n",
      "0        26.982                51.910              0.000     0.000             1.000    0.000\n",
      "1        35.410                44.037              6.689     0.106             0.707    0.031\n",
      "2         8.170                 0.000              0.000     0.000             0.000    1.000\n",
      "3        48.811                39.090              0.344     0.047             0.665    0.181\n",
      "4        49.612                33.120              0.000     0.047             0.558    0.290\n",
      "5        43.457                42.150              0.567     0.073             0.708    0.121\n",
      "6        60.922                 0.000              0.000     0.000             0.000    1.000\n",
      "7        44.866                46.464              0.108     0.043             0.833    0.073\n",
      "8        45.321                51.943              0.000     0.000             1.000    0.000\n",
      "9        26.136                20.203              0.000     1.000             0.279    0.508\n",
      "10       32.295                20.974              0.000     0.000             0.000    0.185\n",
      "11       43.528                44.600              0.000     0.060             0.720    0.048\n",
      "12       41.339                48.557              4.602     0.060             0.809    0.004\n",
      "\n",
      "[GMM] Categorical profile (top-3 per segment) — TEST\n",
      "\n",
      "- sex (weighted %):\n",
      "sex      Female  Male\n",
      "segment              \n",
      "0          43.8  56.2\n",
      "1          54.7  45.3\n",
      "2          48.7  51.3\n",
      "3          31.3  68.7\n",
      "4          51.7  48.3\n",
      "5          27.8  72.2\n",
      "6          67.6  32.4\n",
      "7          23.0  77.0\n",
      "8          49.6  50.4\n",
      "9          51.0  49.0\n",
      "10         55.5  44.5\n",
      "11         38.4  61.6\n",
      "12         41.5  58.5\n",
      "\n",
      "- education (weighted %):\n",
      "education  11th grade  7th and 8th grade  9th grade  Bachelors degree(BA AB BS)  Children  High school graduate  Masters degree(MA MS MEng MEd MSW MBA)  \\\n",
      "segment                                                                                                                                                   \n",
      "0                 NaN                NaN        NaN                        17.8       NaN                  31.7                                     NaN   \n",
      "1                 NaN                NaN        NaN                        10.1       NaN                  37.8                                     NaN   \n",
      "2                 NaN                2.5        3.2                         NaN      87.8                   NaN                                     NaN   \n",
      "3                 NaN                NaN        NaN                        22.0       NaN                  27.6                                     NaN   \n",
      "4                 NaN                NaN        NaN                        27.4       NaN                  24.0                                     NaN   \n",
      "5                 NaN                NaN        NaN                        23.2       NaN                  25.1                                     NaN   \n",
      "6                 NaN               11.9        NaN                         NaN       NaN                  36.7                                     NaN   \n",
      "7                 NaN                NaN        NaN                        31.7       NaN                   NaN                                    17.9   \n",
      "8                 NaN                NaN        NaN                        14.6       NaN                  36.6                                     NaN   \n",
      "9                 NaN                NaN        NaN                         NaN      28.7                  24.6                                     NaN   \n",
      "10                8.8                NaN        NaN                         NaN       NaN                  29.0                                     NaN   \n",
      "11                NaN                NaN        NaN                        14.8       NaN                  34.0                                     NaN   \n",
      "12                NaN                NaN        NaN                        14.9       NaN                  38.0                                     NaN   \n",
      "\n",
      "education  Prof school degree (MD DDS DVM LLB JD)  Some college but no degree  \n",
      "segment                                                                        \n",
      "0                                             NaN                        24.7  \n",
      "1                                             NaN                        22.8  \n",
      "2                                             NaN                         NaN  \n",
      "3                                             NaN                        17.9  \n",
      "4                                             NaN                        19.7  \n",
      "5                                             NaN                        19.6  \n",
      "6                                             NaN                        11.7  \n",
      "7                                            23.9                         NaN  \n",
      "8                                             NaN                        20.3  \n",
      "9                                             NaN                        12.6  \n",
      "10                                            NaN                        21.2  \n",
      "11                                            NaN                        20.9  \n",
      "12                                            NaN                        19.4  \n",
      "\n",
      "- class_of_worker (weighted %):\n",
      "class_of_worker  Federal government  Local government  Missing  Private  Self-employed-incorporated  Self-employed-not incorporated  State government\n",
      "segment                                                                                                                                              \n",
      "0                               NaN               6.0      NaN     85.4                         NaN                             NaN               3.6\n",
      "1                               NaN               5.5      NaN     90.3                         NaN                             NaN               2.3\n",
      "2                               0.0               0.0    100.0      NaN                         NaN                             NaN               NaN\n",
      "3                               NaN               6.9     22.6     50.8                         NaN                             NaN               NaN\n",
      "4                               NaN               NaN     33.0     45.4                         NaN                             7.6               NaN\n",
      "5                               NaN               NaN     16.4     59.0                         NaN                             7.5               NaN\n",
      "6                               0.0               0.0    100.0      NaN                         NaN                             NaN               NaN\n",
      "7                               NaN               NaN      NaN     44.9                        26.3                            13.2               NaN\n",
      "8                               NaN              10.9      NaN     76.7                         NaN                             NaN               5.5\n",
      "9                               NaN               2.2     52.2     42.0                         NaN                             NaN               NaN\n",
      "10                              NaN               5.5     27.8     60.7                         NaN                             NaN               NaN\n",
      "11                              0.0               NaN      NaN      NaN                        24.0                            76.0               NaN\n",
      "12                              NaN              28.6      NaN     55.6                         NaN                             NaN              10.4\n",
      "\n",
      "- citizenship (weighted %):\n",
      "citizenship  Foreign born- Not a citizen of U S  Foreign born- U S citizen by naturalization  Native- Born abroad of American Parent(s)  \\\n",
      "segment                                                                                                                                   \n",
      "0                                           9.8                                          2.1                                        NaN   \n",
      "1                                           4.8                                          2.4                                        NaN   \n",
      "2                                           3.7                                          NaN                                        0.7   \n",
      "3                                           3.8                                          4.2                                        NaN   \n",
      "4                                           1.9                                          3.3                                        NaN   \n",
      "5                                           4.5                                          3.4                                        NaN   \n",
      "6                                           7.6                                          4.5                                        NaN   \n",
      "7                                           6.4                                          6.2                                        NaN   \n",
      "8                                           6.8                                          3.9                                        NaN   \n",
      "9                                          10.2                                          2.1                                        NaN   \n",
      "10                                          8.5                                          2.8                                        NaN   \n",
      "11                                          5.9                                          3.8                                        NaN   \n",
      "12                                          2.5                                          3.8                                        NaN   \n",
      "\n",
      "citizenship  Native- Born in the United States  \n",
      "segment                                         \n",
      "0                                         86.3  \n",
      "1                                         91.6  \n",
      "2                                         95.0  \n",
      "3                                         90.5  \n",
      "4                                         93.6  \n",
      "5                                         90.9  \n",
      "6                                         86.3  \n",
      "7                                         85.6  \n",
      "8                                         87.5  \n",
      "9                                         85.4  \n",
      "10                                        87.2  \n",
      "11                                        89.4  \n",
      "12                                        92.7  \n",
      "\n",
      "- region_of_previous_residence (weighted %):\n",
      "region_of_previous_residence  Abroad  Midwest  Missing  Northeast  South  West\n",
      "segment                                                                       \n",
      "0                                0.0      0.0    100.0        NaN    NaN   NaN\n",
      "1                                NaN      2.8     89.4        NaN    4.1   NaN\n",
      "2                                0.0      0.0    100.0        NaN    NaN   NaN\n",
      "3                                NaN      NaN     95.3        1.2    1.7   NaN\n",
      "4                                NaN      1.4     95.3        NaN    1.6   NaN\n",
      "5                                NaN      NaN     92.7        NaN    2.7   1.9\n",
      "6                                0.0      0.0    100.0        NaN    NaN   NaN\n",
      "7                                NaN      NaN     95.7        1.1    NaN   2.2\n",
      "8                                0.0      0.0    100.0        NaN    NaN   NaN\n",
      "9                                NaN     20.8      NaN        NaN   34.9  27.7\n",
      "10                               0.0      0.0    100.0        NaN    NaN   NaN\n",
      "11                               NaN      NaN     94.0        NaN    2.1   1.6\n",
      "12                               NaN      2.4     94.0        NaN    NaN   2.1\n",
      "\n",
      "- age_bucket (weighted %):\n",
      "age_bucket  18-24  25-34  35-44  45-54  55-64   65+   u18\n",
      "segment                                                  \n",
      "0            27.7   69.7    NaN    NaN    NaN   NaN   2.6\n",
      "1            20.4   29.5   22.2    NaN    NaN   NaN   NaN\n",
      "2             3.1    0.0    NaN    NaN    NaN   NaN  96.9\n",
      "3             NaN    NaN   25.0   19.6    NaN  22.7   NaN\n",
      "4             NaN    NaN   23.6   18.7    NaN  21.5   NaN\n",
      "5             NaN   23.8   23.9   21.5    NaN   NaN   NaN\n",
      "6             NaN    NaN   11.6    NaN   15.4  51.1   NaN\n",
      "7             NaN   12.5   40.1   29.2    NaN   NaN   NaN\n",
      "8             NaN    NaN   46.1   30.2   14.5   NaN   NaN\n",
      "9            17.5   24.2    NaN    NaN    NaN   NaN  32.6\n",
      "10           30.9   20.7   16.8    NaN    NaN   NaN   NaN\n",
      "11            NaN   20.0   28.7   24.0    NaN   NaN   NaN\n",
      "12            NaN   22.4   32.4   28.7    NaN   NaN   NaN\n",
      "\n",
      "- weeks_bucket (weighted %):\n",
      "weeks_bucket      0  1-26  27-51    52\n",
      "segment                               \n",
      "0               0.0   NaN    5.2  94.8\n",
      "1               NaN  12.9   17.5  66.6\n",
      "2             100.0   0.0    0.0   NaN\n",
      "3              18.1   NaN   10.2  64.6\n",
      "4              29.0   NaN   10.0  53.4\n",
      "5              12.1   NaN   13.9  66.8\n",
      "6             100.0   0.0    0.0   NaN\n",
      "7               7.3   NaN    8.6  81.4\n",
      "8               0.0   NaN    3.3  96.7\n",
      "9              50.8   NaN   11.3  26.7\n",
      "10             18.5  44.6   36.9   NaN\n",
      "11              NaN   8.8   20.5  65.8\n",
      "12              NaN   4.6   16.6  78.4\n",
      "\n",
      "- is_union (weighted %):\n",
      "is_union      0      1\n",
      "segment               \n",
      "0         100.0    0.0\n",
      "1         100.0    0.0\n",
      "2         100.0    0.0\n",
      "3          99.5    0.5\n",
      "4         100.0    0.0\n",
      "5          96.8    3.2\n",
      "6         100.0    0.0\n",
      "7          98.0    2.0\n",
      "8         100.0    0.0\n",
      "9         100.0    0.0\n",
      "10        100.0    0.0\n",
      "11        100.0    0.0\n",
      "12          0.0  100.0\n",
      "\n",
      "- is_self_employed (weighted %):\n",
      "is_self_employed      0      1\n",
      "segment                       \n",
      "0                 100.0    0.0\n",
      "1                 100.0    0.0\n",
      "2                 100.0    0.0\n",
      "3                  87.8   12.2\n",
      "4                  88.3   11.7\n",
      "5                  87.0   13.0\n",
      "6                 100.0    0.0\n",
      "7                  60.5   39.5\n",
      "8                 100.0    0.0\n",
      "9                 100.0    0.0\n",
      "10                100.0    0.0\n",
      "11                  0.0  100.0\n",
      "12                100.0    0.0\n",
      "\n",
      "- has_any_capital (weighted %):\n",
      "has_any_capital      0      1\n",
      "segment                      \n",
      "0                100.0    0.0\n",
      "1                 91.0    9.0\n",
      "2                100.0    0.0\n",
      "3                  0.0  100.0\n",
      "4                  0.0  100.0\n",
      "5                  0.0  100.0\n",
      "6                100.0    0.0\n",
      "7                  0.0  100.0\n",
      "8                100.0    0.0\n",
      "9                100.0    0.0\n",
      "10               100.0    0.0\n",
      "11               100.0    0.0\n",
      "12                78.4   21.6\n",
      "\n",
      "[GMM] silhouette_test: 0.30825253495174515\n",
      "\n",
      "[GMM] Segment performance on TEST (weighted)\n",
      "          weighted_share_%  pos_rate_%   lift  avg_conf\n",
      "segment                                               \n",
      "7                     0.3        85.8  13.01       1.0\n",
      "5                     2.0        32.6   4.94       1.0\n",
      "3                     3.4        31.1   4.72       1.0\n",
      "4                     8.8        20.1   3.04       1.0\n",
      "8                    12.6        11.3   1.71       1.0\n",
      "11                    4.2        11.3   1.71       1.0\n",
      "12                    1.4        10.1   1.53       1.0\n",
      "0                     7.3         4.7   0.72       1.0\n",
      "1                     4.7         3.3   0.50       1.0\n",
      "9                     6.7         1.6   0.25       1.0\n",
      "10                   11.8         1.2   0.18       1.0\n",
      "6                    14.3         0.6   0.10       1.0\n",
      "2                    22.4         0.0   0.00       1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -- 0) Build 30-D embeddings (reuse your prep_pipe)\n",
    "Z_trainval = prep_pipe.fit_transform(X_trainval)\n",
    "Z_test     = prep_pipe.transform(X_test)\n",
    "\n",
    "# -- 1) Helpers --------------------------------------------------------------\n",
    "def weighted_bic(gmm, Z, w):\n",
    "    \"\"\"BIC-like criterion with sample weights:\n",
    "       BIC_w = -2 * sum_i w_i * log p(x_i) + p * log(sum_i w_i)\n",
    "    \"\"\"\n",
    "    ll = gmm.score_samples(Z)  # (n,)\n",
    "    n_eff = float(w.sum())\n",
    "\n",
    "    k = gmm.n_components\n",
    "    d = Z.shape[1]\n",
    "    cov = gmm.covariance_type\n",
    "    if cov == \"full\":\n",
    "        cov_params = k * d * (d + 1) // 2\n",
    "    elif cov == \"diag\":\n",
    "        cov_params = k * d\n",
    "    elif cov == \"tied\":\n",
    "        cov_params = d * (d + 1) // 2\n",
    "    elif cov == \"spherical\":\n",
    "        cov_params = k\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown covariance_type: {cov}\")\n",
    "    n_params = k * d + cov_params + (k - 1)  # means + covs + weights(k-1)\n",
    "\n",
    "    return -2.0 * float((w * ll).sum()) + n_params * np.log(n_eff)\n",
    "\n",
    "def pps_resample(Z, w, n_sub=None, rng=None):\n",
    "    \"\"\"Probability-proportional-to-size resampling (with replacement).\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(42)\n",
    "    n = len(Z)\n",
    "    if n_sub is None:\n",
    "        n_sub = n\n",
    "    p = np.asarray(w).reshape(-1)\n",
    "    p = p / p.sum()\n",
    "    idx = rng.choice(n, size=n_sub, replace=True, p=p)\n",
    "    return Z[idx]\n",
    "\n",
    "# -- 2) Tiny tuning sweep (fast) --------------------------------------------\n",
    "rng = np.random.default_rng(123)\n",
    "# cap the subsample to speed up EM; enlarge if you want a bit more accuracy\n",
    "n_sub_cap = min(len(Z_trainval), 80000)\n",
    "Z_sub = pps_resample(Z_trainval, w_trainval.values, n_sub=n_sub_cap, rng=rng)\n",
    "\n",
    "# small grid centered around typical good values; adjust if you like\n",
    "k_coarse   = [6, 8, 10, 12]            # coarse K\n",
    "cov_types  = [\"full\", \"diag\"]          # two covariance structures\n",
    "reg_list   = [1e-6, 1e-5]              # small regularization\n",
    "seeds      = [17, 23]                  # 2 seeds to keep it fast\n",
    "max_iter   = 300\n",
    "\n",
    "cands = []\n",
    "for k in k_coarse:\n",
    "    for cov in cov_types:\n",
    "        for reg in reg_list:\n",
    "            for sd in seeds:\n",
    "                gmm = GaussianMixture(\n",
    "                    n_components=k,\n",
    "                    covariance_type=cov,\n",
    "                    reg_covar=reg,\n",
    "                    random_state=int(sd),\n",
    "                    max_iter=max_iter,\n",
    "                    init_params=\"kmeans\",\n",
    "                    n_init=1,  # keep it fast\n",
    "                )\n",
    "                gmm.fit(Z_sub)  # no sample_weight in your sklearn\n",
    "                bic_w = weighted_bic(gmm, Z_trainval, w_trainval.values)\n",
    "                try:\n",
    "                    sil = silhouette_score(Z_trainval, gmm.predict(Z_trainval), metric=\"euclidean\")\n",
    "                except Exception:\n",
    "                    sil = np.nan\n",
    "                cands.append({\n",
    "                    \"k\": k, \"cov\": cov, \"reg\": reg, \"seed\": int(sd),\n",
    "                    \"bic_w\": float(bic_w), \"sil\": float(sil), \"model\": gmm\n",
    "                })\n",
    "\n",
    "# pick top-3 by BIC_w (lower better), break ties by higher silhouette\n",
    "cands.sort(key=lambda d: (d[\"bic_w\"], -np.nan_to_num(d[\"sil\"], nan=-1e9)))\n",
    "top = cands[:3]\n",
    "\n",
    "# quick local refine around top K values (±1), one seed to save time\n",
    "k_refine = sorted(set(sum(([max(2, t[\"k\"]-1), t[\"k\"]+1] for t in top), [])))\n",
    "for k in k_refine:\n",
    "    for cov in cov_types:\n",
    "        for reg in reg_list:\n",
    "            gmm = GaussianMixture(\n",
    "                n_components=k,\n",
    "                covariance_type=cov,\n",
    "                reg_covar=reg,\n",
    "                random_state=42,\n",
    "                max_iter=max_iter,\n",
    "                init_params=\"kmeans\",\n",
    "                n_init=1,\n",
    "            )\n",
    "            gmm.fit(Z_sub)\n",
    "            bic_w = weighted_bic(gmm, Z_trainval, w_trainval.values)\n",
    "            try:\n",
    "                sil = silhouette_score(Z_trainval, gmm.predict(Z_trainval), metric=\"euclidean\")\n",
    "            except Exception:\n",
    "                sil = np.nan\n",
    "            cands.append({\n",
    "                \"k\": k, \"cov\": cov, \"reg\": reg, \"seed\": 42,\n",
    "                \"bic_w\": float(bic_w), \"sil\": float(sil), \"model\": gmm\n",
    "            })\n",
    "\n",
    "# final pick\n",
    "cands.sort(key=lambda d: (d[\"bic_w\"], -np.nan_to_num(d[\"sil\"], nan=-1e9)))\n",
    "best = cands[0]\n",
    "print(f\"[GMM quick-tune] best: k={best['k']} | cov={best['cov']} | reg={best['reg']} \"\n",
    "      f\"| seed={best['seed']} | BIC_w={best['bic_w']:.1f} | sil={best['sil']:.3f}\")\n",
    "\n",
    "# -- 3) Refit the best hyperparams on FULL TrainVal (one final fit) ----------\n",
    "best_model = GaussianMixture(\n",
    "    n_components=best[\"k\"],\n",
    "    covariance_type=best[\"cov\"],\n",
    "    reg_covar=best[\"reg\"],\n",
    "    random_state=best[\"seed\"],\n",
    "    max_iter=max_iter,\n",
    "    init_params=\"kmeans\",\n",
    "    n_init=1,\n",
    ")\n",
    "best_model.fit(Z_trainval)\n",
    "\n",
    "# label & proba on TEST\n",
    "labels_test = pd.Series(best_model.predict(Z_test), index=X_test.index, name=\"segment\")\n",
    "proba_test  = pd.DataFrame(best_model.predict_proba(Z_test), index=X_test.index)\n",
    "\n",
    "# -- 4) Final TEST report (same style as you used) ---------------------------\n",
    "# readable TEST features (post-FeatureBuilder)\n",
    "Xte_feat = prep_pipe.named_steps[\"feat\"].transform(X_test.copy())\n",
    "\n",
    "def wmean_by_seg(values, seg, w):\n",
    "    \"\"\"Weighted mean grouped by segment.\"\"\"\n",
    "    num = (values * w).groupby(seg).sum()\n",
    "    den = w.groupby(seg).sum()\n",
    "    return (num / den).rename(values.name)\n",
    "\n",
    "def wprop_table(cat_series, seg, w):\n",
    "    \"\"\"Weighted proportion table: rows=segment, cols=category.\"\"\"\n",
    "    dummies = pd.get_dummies(cat_series.astype(str), dummy_na=False)\n",
    "    wmat = dummies.mul(w.values.reshape(-1, 1))\n",
    "    grp = wmat.groupby(seg).sum()\n",
    "    grp = grp.div(grp.sum(axis=1), axis=0)\n",
    "    grp.columns.name = cat_series.name\n",
    "    return grp\n",
    "\n",
    "# sizes & confidence\n",
    "seg_te = labels_test.reindex(X_test.index)\n",
    "w_te   = w_test.reindex(X_test.index)\n",
    "weighted_share_te = 100.0 * w_te.groupby(seg_te).sum() / w_te.sum()\n",
    "print(\"\\n[GMM] weighted_share TEST (%)\\n\", weighted_share_te.round(1))\n",
    "\n",
    "max_prob_te = proba_test.max(axis=1).rename(\"max_prob\")\n",
    "avg_conf_te = wmean_by_seg(max_prob_te, seg_te, w_te)\n",
    "print(\"\\n[GMM] avg max-membership prob by segment (TEST)\\n\", avg_conf_te.round(3))\n",
    "\n",
    "# numeric profile\n",
    "num_to_show = [c for c in [\"age\",\"weeks_worked_in_year\",\"log_wage_per_hour\",\n",
    "                           \"is_mover\",\"full_year_worker\",\"no_work\"]\n",
    "               if c in Xte_feat.columns]\n",
    "num_profiles_test = pd.concat([wmean_by_seg(Xte_feat[c], seg_te, w_te) for c in num_to_show], axis=1)\n",
    "print(\"\\n[GMM] Numeric profile (weighted means) — TEST\\n\", num_profiles_test.round(3))\n",
    "\n",
    "# categorical profile (top-3)\n",
    "cat_to_show = [c for c in [\"sex\",\"education\",\"class_of_worker\",\"citizenship\",\n",
    "                           \"region_of_previous_residence\",\"age_bucket\",\"weeks_bucket\",\n",
    "                           \"is_union\",\"is_self_employed\",\"has_any_capital\"]\n",
    "               if c in Xte_feat.columns]\n",
    "print(\"\\n[GMM] Categorical profile (top-3 per segment) — TEST\")\n",
    "for c in cat_to_show:\n",
    "    tab = wprop_table(Xte_feat[c], seg_te, w_te)\n",
    "    top3 = tab.apply(lambda r: r.sort_values(ascending=False).head(3), axis=1)\n",
    "    print(f\"\\n- {c} (weighted %):\")\n",
    "    print((100 * top3).round(1))\n",
    "\n",
    "# internal quality & supervised lift (if y_test is available)\n",
    "from sklearn.metrics import silhouette_score\n",
    "print(\"\\n[GMM] silhouette_test:\", silhouette_score(Z_test, seg_te))\n",
    "\n",
    "if 'y_test' in globals():\n",
    "    pos_rate = ((w_te * y_test).groupby(seg_te).sum() / w_te.groupby(seg_te).sum())\n",
    "    overall  = float((w_te * y_test).sum() / w_te.sum())\n",
    "    lift     = (pos_rate / overall).rename(\"lift\")\n",
    "    pr_table = pd.DataFrame({\n",
    "        \"weighted_share_%\": weighted_share_te.round(1),\n",
    "        \"pos_rate_%\": (100 * pos_rate).round(1),\n",
    "        \"lift\": lift.round(2),\n",
    "        \"avg_conf\": avg_conf_te.round(3),\n",
    "    }).sort_values(\"lift\", ascending=False)\n",
    "    print(\"\\n[GMM] Segment performance on TEST (weighted)\\n\", pr_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d221eb4",
   "metadata": {},
   "source": [
    "# Final result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24491153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sanity] KMeans Test weighted share %:\n",
      " segment\n",
      "0    17.6\n",
      "1    34.5\n",
      "2     0.3\n",
      "3     5.5\n",
      "4     3.5\n",
      "5     2.0\n",
      "6     8.9\n",
      "7    27.6\n",
      "Name: weight, dtype: float64\n",
      "[Sanity] HDBSCAN Test weighted share % (incl. -1 noise):\n",
      " segment\n",
      "-1     25.2\n",
      " 0      2.0\n",
      " 1      3.4\n",
      " 2      1.1\n",
      " 3      4.1\n",
      " 4      7.1\n",
      " 5      3.6\n",
      " 6      1.2\n",
      " 7      2.5\n",
      " 8      2.6\n",
      " 9      4.8\n",
      " 10    16.3\n",
      " 11     9.5\n",
      " 12    10.2\n",
      " 13     1.3\n",
      " 14     1.8\n",
      " 15     1.5\n",
      " 16     1.8\n",
      "Name: weight, dtype: float64\n",
      "[Sanity] GMM Test weighted share %:\n",
      " segment\n",
      "0     8.9\n",
      "1    22.4\n",
      "2    23.0\n",
      "3    14.4\n",
      "4     3.8\n",
      "5    12.8\n",
      "6     5.5\n",
      "7     0.2\n",
      "8     1.9\n",
      "9     7.0\n",
      "Name: weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Z_trainval / Z_test: 30-dim SVD features from your prep_pipe\n",
    "try:\n",
    "    Z_trainval, Z_test  # noqa\n",
    "except NameError:\n",
    "    Z_trainval = prep_pipe.fit_transform(X_trainval)\n",
    "    Z_test     = prep_pipe.transform(X_test)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# KMEANS LABELS\n",
    "# ------------------------------------------------------------------\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _safe_kmeans(k, seed=42):\n",
    "    try:\n",
    "        return KMeans(n_clusters=k, n_init=\"auto\", random_state=seed)\n",
    "    except TypeError:\n",
    "        return KMeans(n_clusters=k, n_init=10, random_state=seed)\n",
    "\n",
    "# If you already have a fitted kmeans, reuse it; otherwise fit quickly with your chosen best_k (e.g., 10)\n",
    "best_k_km = 10  # <- set this to the K you selected earlier\n",
    "if \"kmeans\" not in globals() or not hasattr(kmeans, \"labels_\") or len(getattr(kmeans, \"labels_\", [])) != len(Z_trainval):\n",
    "    kmeans = _safe_kmeans(best_k_km, seed=42)\n",
    "    kmeans.fit(Z_trainval, sample_weight=w_trainval)\n",
    "\n",
    "labels_trainval_km = pd.Series(kmeans.labels_, index=X_trainval.index, name=\"segment\").astype(int)\n",
    "labels_test_km     = pd.Series(kmeans.predict(Z_test), index=X_test.index, name=\"segment\").astype(int)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# HDBSCAN LABELS (fit on PPS subsample; assign full set via approximate_predict)\n",
    "# ------------------------------------------------------------------\n",
    "import hdbscan\n",
    "\n",
    "def _pps_subsample(Z, w, cap=60000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(Z)\n",
    "    n_sub = min(n, cap)\n",
    "    p = (w.values / w.values.sum())\n",
    "    idx = rng.choice(n, size=n_sub, replace=False, p=p)\n",
    "    return Z[idx], n_sub\n",
    "\n",
    "if \"clusterer\" not in globals() or not isinstance(clusterer, hdbscan.HDBSCAN):\n",
    "    Z_sub, n_sub = _pps_subsample(Z_trainval, w_trainval, cap=60000, seed=42)\n",
    "    min_cluster_size = max(200, int(0.01 * n_sub))  # tweak if you want fewer noise points\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=None,\n",
    "        metric=\"euclidean\",\n",
    "        cluster_selection_method=\"eom\",\n",
    "        approx_min_span_tree=True,\n",
    "        prediction_data=True\n",
    "    )\n",
    "    clusterer.fit(Z_sub)\n",
    "\n",
    "tr_labels_hdb, tr_strength_hdb = hdbscan.approximate_predict(clusterer, Z_trainval)\n",
    "te_labels_hdb, te_strength_hdb = hdbscan.approximate_predict(clusterer, Z_test)\n",
    "\n",
    "labels_trainval_hdb = pd.Series(tr_labels_hdb, index=X_trainval.index, name=\"segment\").astype(int)\n",
    "labels_test_hdb     = pd.Series(te_labels_hdb, index=X_test.index, name=\"segment\").astype(int)\n",
    "# strength arrays (float) aligned to the same indices:\n",
    "tr_strength_hdb = pd.Series(tr_strength_hdb, index=X_trainval.index, name=\"strength\")\n",
    "te_strength_hdb = pd.Series(te_strength_hdb, index=X_test.index, name=\"strength\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# GMM LABELS (quick refit if needed; use your best_k from weighted-BIC search)\n",
    "# ------------------------------------------------------------------\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "best_k_gmm = 10  # <- set this to the GMM best_k you found (your prints show 10 clusters)\n",
    "def _fit_gmm_quick(Z, n_comp=10, seed=42):\n",
    "    g = GaussianMixture(\n",
    "        n_components=n_comp,\n",
    "        covariance_type=\"full\",   # try \"diag\" if posteriors look too hard\n",
    "        reg_covar=1e-3,           # a bit larger to avoid overly hard assignments\n",
    "        max_iter=500,\n",
    "        random_state=seed,\n",
    "        init_params=\"kmeans\",\n",
    "    )\n",
    "    g.fit(Z)\n",
    "    return g\n",
    "\n",
    "if \"best_model\" in globals() and isinstance(best_model, GaussianMixture) and best_model.n_components == best_k_gmm:\n",
    "    gmm = best_model\n",
    "else:\n",
    "    gmm = _fit_gmm_quick(Z_trainval, n_comp=best_k_gmm, seed=42)\n",
    "\n",
    "labels_trainval_gmm = pd.Series(gmm.predict(Z_trainval), index=X_trainval.index, name=\"segment\").astype(int)\n",
    "labels_test_gmm     = pd.Series(gmm.predict(Z_test),     index=X_test.index,     name=\"segment\").astype(int)\n",
    "\n",
    "# also keep soft probabilities (can be useful for scorecards / calibration)\n",
    "proba_trainval_gmm  = pd.DataFrame(gmm.predict_proba(Z_trainval), index=X_trainval.index)\n",
    "proba_test_gmm      = pd.DataFrame(gmm.predict_proba(Z_test),     index=X_test.index)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# (Optional) quick sanity checks\n",
    "# ------------------------------------------------------------------\n",
    "def _wshare(lbls, w):\n",
    "    s = (w.groupby(lbls).sum() / w.sum()) * 100.0\n",
    "    return s.sort_index().round(1)\n",
    "\n",
    "print(\"[Sanity] KMeans Test weighted share %:\\n\", _wshare(labels_test_km, w_test))\n",
    "print(\"[Sanity] HDBSCAN Test weighted share % (incl. -1 noise):\\n\", _wshare(labels_test_hdb, w_test))\n",
    "print(\"[Sanity] GMM Test weighted share %:\\n\", _wshare(labels_test_gmm, w_test))# === Scorecard to compare KMeans / HDBSCAN / GMM on TEST ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ec56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         clusters_all  clusters_>=2%  noise_%  sil_train  sil_test  share_drift_pp  Top30_pop_%  Top30_pos_%  MI_seg_y\n",
      "model                                                                                                                 \n",
      "kmeans              8              7      0.0      0.323     0.320             0.7         49.2         94.9    0.0533\n",
      "gmm                10              8      0.0      0.322     0.319             1.0         37.8         90.3    0.0553\n",
      "hdbscan            18             11     25.2      0.276     0.270             1.6         32.4         74.4    0.0436\n"
     ]
    }
   ],
   "source": [
    "# === Scorecard to compare KMeans / HDBSCAN / GMM on TEST ===\n",
    "\n",
    "\n",
    "\n",
    "# --- Inputs you should provide ---\n",
    "# labels_trainval_km, labels_test_km\n",
    "# labels_trainval_hdb, labels_test_hdb\n",
    "# labels_trainval_gmm, labels_test_gmm\n",
    "# Z_trainval, Z_test  (the SVD embeddings)\n",
    "# w_trainval, w_test  (pd.Series aligned to indices)\n",
    "# y_trainval, y_test  (pd.Series; optional but recommended)\n",
    "\n",
    "methods = {\n",
    "    \"kmeans\":  {\"tr\": labels_trainval_km,  \"te\": labels_test_km},\n",
    "    \"hdbscan\": {\"tr\": labels_trainval_hdb, \"te\": labels_test_hdb},\n",
    "    \"gmm\":     {\"tr\": labels_trainval_gmm, \"te\": labels_test_gmm},\n",
    "}\n",
    "\n",
    "def weighted_share(labels, w):\n",
    "    return (w.groupby(labels).sum() / w.sum()).sort_index()\n",
    "\n",
    "def share_drift_L1(s_tv, s_te):\n",
    "    # Align indexes and compute L1 drift in percentage points\n",
    "    idx = s_tv.index.union(s_te.index)\n",
    "    a = s_tv.reindex(idx, fill_value=0.0)\n",
    "    b = s_te.reindex(idx, fill_value=0.0)\n",
    "    return float((a - b).abs().sum() * 100)\n",
    "\n",
    "def topX_capture(labels, y, w, top_share=0.30):\n",
    "    \"\"\"Sort clusters by weighted pos rate desc; take clusters until covering top_share of population.\n",
    "       Return (pop_covered%, positives_captured%) on TEST.\"\"\"\n",
    "    grp_w = w.groupby(labels).sum()\n",
    "    pos_w = (w * y).groupby(labels).sum()\n",
    "    pos_rate = (pos_w / grp_w).fillna(0.0)\n",
    "    order = pos_rate.sort_values(ascending=False).index\n",
    "\n",
    "    cum_pop = 0.0\n",
    "    cum_pos = 0.0\n",
    "    total_pop = float(w.sum())\n",
    "    total_pos = float((w * y).sum())\n",
    "    covered = []\n",
    "    for lab in order:\n",
    "        share = float(grp_w.loc[lab] / total_pop)\n",
    "        covered.append(lab)\n",
    "        cum_pop += share\n",
    "        cum_pos += float(pos_w.loc[lab] / total_pos if total_pos > 0 else 0.0)\n",
    "        if cum_pop >= top_share:\n",
    "            break\n",
    "    return 100*cum_pop, 100*cum_pos, pos_rate.loc[covered]\n",
    "\n",
    "def weighted_mutual_info(labels, y, w):\n",
    "    \"\"\"Approximate weighted MI by PPS upsampling (fast & simple).\"\"\"\n",
    "    # Draw n samples with probability proportional to w\n",
    "    n = len(w)\n",
    "    p = (w / w.sum()).values\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx = rng.choice(np.arange(n), size=min(n, 200_000), replace=True, p=p)\n",
    "    return mutual_info_score(labels.values[idx], y.values[idx])\n",
    "\n",
    "rows = []\n",
    "for name, d in methods.items():\n",
    "    lab_tr, lab_te = d[\"tr\"], d[\"te\"]\n",
    "\n",
    "    # shares\n",
    "    s_tr = weighted_share(lab_tr, w_trainval)\n",
    "    s_te = weighted_share(lab_te, w_test)\n",
    "    drift = share_drift_L1(s_tr, s_te)\n",
    "\n",
    "    # cluster counts (exclude noise = -1 for HDBSCAN when reporting)\n",
    "    n_all = lab_te.nunique()\n",
    "    n_eff = int((s_te[s_te >= 0.02]).count())  # clusters with >=2% weighted share\n",
    "    noise_share = float((s_te.loc[-1]*100) if (-1 in s_te.index) else 0.0)\n",
    "\n",
    "    # internal quality (silhouette on embeddings; sklearn has no weighted version)\n",
    "    try:\n",
    "        sil_tr = float(silhouette_score(Z_trainval, lab_tr))\n",
    "        sil_te = float(silhouette_score(Z_test, lab_te))\n",
    "    except Exception:\n",
    "        sil_tr = np.nan; sil_te = np.nan\n",
    "\n",
    "    # supervised utility (if y is available)\n",
    "    if 'y_test' in globals():\n",
    "        pop_cov, pos_cap, top_list = topX_capture(lab_te, y_test, w_test, top_share=0.30)\n",
    "        mi = weighted_mutual_info(lab_te, y_test, w_test)\n",
    "    else:\n",
    "        pop_cov = pos_cap = mi = np.nan\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"clusters_all\": n_all,\n",
    "        \"clusters_>=2%\": n_eff,\n",
    "        \"noise_%\": round(noise_share, 1),\n",
    "        \"sil_train\": round(sil_tr, 3) if not np.isnan(sil_tr) else np.nan,\n",
    "        \"sil_test\":  round(sil_te, 3) if not np.isnan(sil_te) else np.nan,\n",
    "        \"share_drift_pp\": round(drift, 1),\n",
    "        \"Top30_pop_%\": round(pop_cov, 1),\n",
    "        \"Top30_pos_%\": round(pos_cap, 1),\n",
    "        \"MI_seg_y\": round(mi, 4) if not np.isnan(mi) else np.nan,\n",
    "    })\n",
    "\n",
    "scorecard = pd.DataFrame(rows).set_index(\"model\").sort_values(\"Top30_pos_%\", ascending=False)\n",
    "print(scorecard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b069cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Segmentation - Finalized (KMeans fixed, HDBSCAN/GMM = tuned)\n",
    "# ===============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score, mutual_info_score\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Embeddings (prep_pipe SVD)\n",
    "# -----------------------------\n",
    "try:\n",
    "    Z_trainval, Z_test  # noqa\n",
    "except NameError:\n",
    "    Z_trainval = prep_pipe.fit_transform(X_trainval)\n",
    "    Z_test     = prep_pipe.transform(X_test)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 1) Tuned params (from your completed tuning results)\n",
    "# ------------------------------------------------------\n",
    "HDBSCAN_TUNED = dict(\n",
    "    min_cluster_size=600,\n",
    "    min_samples=None,                 # chosen in tuning\n",
    "    cluster_selection_method=\"eom\",\n",
    "    metric=\"euclidean\",\n",
    "    approx_min_span_tree=True,\n",
    "    prediction_data=True,\n",
    ")\n",
    "\n",
    "GMM_TUNED = dict(\n",
    "    n_components=13,                  # tuned result; set to 10 if you prefer earlier 10-cluster solution\n",
    "    covariance_type=\"full\",\n",
    "    reg_covar=1e-3,\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    init_params=\"kmeans\",\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2) KMEANS (keep as-is; already tuned on your side)\n",
    "# ------------------------------------------------------\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def _safe_kmeans(k, seed=42):\n",
    "    try:\n",
    "        return KMeans(n_clusters=k, n_init=\"auto\", random_state=seed)\n",
    "    except TypeError:\n",
    "        return KMeans(n_clusters=k, n_init=10, random_state=seed)\n",
    "\n",
    "best_k_km = 10  # your tuned K\n",
    "\n",
    "if \"kmeans\" not in globals() or not hasattr(kmeans, \"labels_\") or len(getattr(kmeans, \"labels_\", [])) != len(Z_trainval):\n",
    "    kmeans = _safe_kmeans(best_k_km, seed=42)\n",
    "   \n",
    "    try:\n",
    "        kmeans.fit(Z_trainval, sample_weight=w_trainval)\n",
    "    except TypeError:\n",
    "        kmeans.fit(Z_trainval)\n",
    "\n",
    "labels_trainval_km = pd.Series(kmeans.labels_, index=X_trainval.index, name=\"segment\").astype(int)\n",
    "labels_test_km     = pd.Series(kmeans.predict(Z_test), index=X_test.index, name=\"segment\").astype(int)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3) HDBSCAN (use PPS subsample + tuned params)\n",
    "# ------------------------------------------------------\n",
    "import hdbscan\n",
    "\n",
    "def _pps_subsample(Z, w, cap=60000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(Z)\n",
    "    n_sub = min(n, cap)\n",
    "    p = (w.values / w.values.sum())\n",
    "    idx = rng.choice(n, size=n_sub, replace=False, p=p)\n",
    "    return Z[idx]\n",
    "\n",
    "def _need_refit_hdbscan(clf, tuned):\n",
    "    if \"clusterer\" not in globals(): return True\n",
    "    if not isinstance(clf, hdbscan.HDBSCAN): return True\n",
    "    return not (\n",
    "        clf.min_cluster_size == tuned[\"min_cluster_size\"] and\n",
    "        clf.min_samples == tuned[\"min_samples\"] and\n",
    "        clf.cluster_selection_method == tuned[\"cluster_selection_method\"] and\n",
    "        clf.metric == tuned[\"metric\"]\n",
    "    )\n",
    "\n",
    "if _need_refit_hdbscan(globals().get(\"clusterer\", None), HDBSCAN_TUNED):\n",
    "    Z_sub = _pps_subsample(Z_trainval, w_trainval, cap=60000, seed=42)\n",
    "    clusterer = hdbscan.HDBSCAN(**HDBSCAN_TUNED)\n",
    "    clusterer.fit(Z_sub)\n",
    "\n",
    "tr_labels_hdb, tr_strength_arr = hdbscan.approximate_predict(clusterer, Z_trainval)\n",
    "te_labels_hdb, te_strength_arr = hdbscan.approximate_predict(clusterer, Z_test)\n",
    "\n",
    "labels_trainval_hdb = pd.Series(tr_labels_hdb, index=X_trainval.index, name=\"segment\").astype(int)\n",
    "labels_test_hdb     = pd.Series(te_labels_hdb, index=X_test.index,     name=\"segment\").astype(int)\n",
    "tr_strength_hdb     = pd.Series(tr_strength_arr, index=X_trainval.index, name=\"strength\")\n",
    "te_strength_hdb     = pd.Series(te_strength_arr, index=X_test.index,     name=\"strength\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4) GMM (use tuned params; refit if mismatch)\n",
    "# ------------------------------------------------------\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def _need_refit_gmm(g, tuned):\n",
    "    if \"gmm\" not in globals(): return True\n",
    "    if not isinstance(g, GaussianMixture): return True\n",
    "    if g.n_components != tuned[\"n_components\"]: return True\n",
    "    if g.covariance_type != tuned[\"covariance_type\"]: return True\n",
    "    if not np.isclose(getattr(g, \"reg_covar\", tuned[\"reg_covar\"]), tuned[\"reg_covar\"]): return True\n",
    "    return False\n",
    "\n",
    "if _need_refit_gmm(globals().get(\"gmm\", None), GMM_TUNED):\n",
    "    gmm = GaussianMixture(**GMM_TUNED).fit(Z_trainval)\n",
    "\n",
    "labels_trainval_gmm = pd.Series(gmm.predict(Z_trainval), index=X_trainval.index, name=\"segment\").astype(int)\n",
    "labels_test_gmm     = pd.Series(gmm.predict(Z_test),     index=X_test.index,     name=\"segment\").astype(int)\n",
    "proba_trainval_gmm  = pd.DataFrame(gmm.predict_proba(Z_trainval), index=X_trainval.index)\n",
    "proba_test_gmm      = pd.DataFrame(gmm.predict_proba(Z_test),     index=X_test.index)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 5) Sanity prints (weighted shares on TEST)\n",
    "# ------------------------------------------------------\n",
    "def _wshare(lbls, w):\n",
    "    return (w.groupby(lbls).sum() / w.sum() * 100.0).sort_index().round(1)\n",
    "\n",
    "print(\"[Sanity] KMeans | Test weighted share %\\n\", _wshare(labels_test_km,  w_test))\n",
    "print(\"[Sanity] HDBSCAN | Test weighted share % (incl. -1 noise)\\n\", _wshare(labels_test_hdb, w_test))\n",
    "print(\"[Sanity] GMM | Test weighted share %\\n\", _wshare(labels_test_gmm,  w_test))\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 6) Unified scorecard on TEST (and TRAINVAL for drift)\n",
    "# ------------------------------------------------------\n",
    "EXCLUDE_HDBSCAN_NOISE_IN_COUNTS = True  # set to False if you want to count noise as a cluster\n",
    "\n",
    "methods = {\n",
    "    \"kmeans\":  {\"tr\": labels_trainval_km,  \"te\": labels_test_km},\n",
    "    \"hdbscan\": {\"tr\": labels_trainval_hdb, \"te\": labels_test_hdb},\n",
    "    \"gmm\":     {\"tr\": labels_trainval_gmm, \"te\": labels_test_gmm},\n",
    "}\n",
    "\n",
    "def weighted_share(labels, w):\n",
    "    return (w.groupby(labels).sum() / w.sum()).sort_index()\n",
    "\n",
    "def share_drift_L1(s_tv, s_te):\n",
    "    idx = s_tv.index.union(s_te.index)\n",
    "    a = s_tv.reindex(idx, fill_value=0.0)\n",
    "    b = s_te.reindex(idx, fill_value=0.0)\n",
    "    return float((a - b).abs().sum() * 100)  # percentage points\n",
    "\n",
    "def topX_capture(labels, y, w, top_share=0.30):\n",
    "    grp_w = w.groupby(labels).sum()\n",
    "    pos_w = (w * y).groupby(labels).sum()\n",
    "    pos_rate = (pos_w / grp_w).fillna(0.0)\n",
    "    order = pos_rate.sort_values(ascending=False).index\n",
    "\n",
    "    cum_pop = 0.0\n",
    "    cum_pos = 0.0\n",
    "    total_pop = float(w.sum())\n",
    "    total_pos = float((w * y).sum())\n",
    "    covered = []\n",
    "    for lab in order:\n",
    "        share = float(grp_w.loc[lab] / total_pop)\n",
    "        covered.append(lab)\n",
    "        cum_pop += share\n",
    "        cum_pos += float(pos_w.loc[lab] / total_pos) if total_pos > 0 else 0.0\n",
    "        if cum_pop >= top_share:\n",
    "            break\n",
    "    return 100*cum_pop, 100*cum_pos, pos_rate.loc[covered]\n",
    "\n",
    "def weighted_mutual_info(labels, y, w):\n",
    "    n = len(w)\n",
    "    p = (w / w.sum()).values\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx = rng.choice(np.arange(n), size=min(n, 200_000), replace=True, p=p)\n",
    "    return float(mutual_info_score(labels.values[idx], y.values[idx]))\n",
    "\n",
    "rows = []\n",
    "for name, d in methods.items():\n",
    "    lab_tr, lab_te = d[\"tr\"], d[\"te\"]\n",
    "\n",
    "    # shares\n",
    "    s_tr = weighted_share(lab_tr, w_trainval)\n",
    "    s_te = weighted_share(lab_te, w_test)\n",
    "    drift = share_drift_L1(s_tr, s_te)\n",
    "\n",
    "    # cluster counts\n",
    "    if name == \"hdbscan\" and EXCLUDE_HDBSCAN_NOISE_IN_COUNTS:\n",
    "        s_te_eff = s_te.drop(index=-1, errors=\"ignore\")\n",
    "        n_all = s_te_eff.index.nunique()\n",
    "        n_eff = int((s_te_eff >= 0.02).sum())    # >=2% weighted share\n",
    "        noise_share = float((s_te.get(-1, 0.0)) * 100.0)\n",
    "    else:\n",
    "        n_all = lab_te.nunique()\n",
    "        n_eff = int((s_te >= 0.02).sum())\n",
    "        noise_share = float((s_te.get(-1, 0.0)) * 100.0)\n",
    "\n",
    "    # internal quality (unweighted silhouette on embeddings)\n",
    "    try:\n",
    "        sil_tr = float(silhouette_score(Z_trainval, lab_tr))\n",
    "        sil_te = float(silhouette_score(Z_test, lab_te))\n",
    "    except Exception:\n",
    "        sil_tr = np.nan; sil_te = np.nan\n",
    "\n",
    "    # supervised utility (optional)\n",
    "    if 'y_test' in globals():\n",
    "        pop_cov, pos_cap, _ = topX_capture(lab_te, y_test, w_test, top_share=0.30)\n",
    "        mi = weighted_mutual_info(lab_te, y_test, w_test)\n",
    "    else:\n",
    "        pop_cov = pos_cap = mi = np.nan\n",
    "\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"clusters_all\": n_all,\n",
    "        \"clusters_>=2%\": n_eff,\n",
    "        \"noise_%\": round(noise_share, 1),\n",
    "        \"sil_train\": round(sil_tr, 3) if not np.isnan(sil_tr) else np.nan,\n",
    "        \"sil_test\":  round(sil_te, 3) if not np.isnan(sil_te) else np.nan,\n",
    "        \"share_drift_pp\": round(drift, 1),\n",
    "        \"Top30_pop_%\": round(pop_cov, 1),\n",
    "        \"Top30_pos_%\": round(pos_cap, 1),\n",
    "        \"MI_seg_y\": round(mi, 4) if not np.isnan(mi) else np.nan,\n",
    "    })\n",
    "\n",
    "scorecard = pd.DataFrame(rows).set_index(\"model\").sort_values(\n",
    "    [\"Top30_pos_%\", \"sil_test\"], ascending=[False, False]\n",
    ")\n",
    "print(\"\\n=== Scorecard (TEST) ===\\n\", scorecard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bertopic_env)",
   "language": "python",
   "name": "bertopic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
